[{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"commands","dir":"","previous_headings":"","what":"Commands","title":"EndpointR Development Guide","text":"Run tests: testthat::test() Run single test: testthat::test_file(\"tests/testthat/test-tmp.R\") Run specific test: testthat::test_that(\"test description\", { test code }) Check package: devtools::check() R CMD check Build package: devtools::build() R CMD build . Generate documentation: devtools::document()","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"language-style-guidelines","dir":"","previous_headings":"","what":"Language Style Guidelines","title":"EndpointR Development Guide","text":"Use British English Use straightforward language jargon","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"code-style-guidelines","dir":"","previous_headings":"","what":"Code Style Guidelines","title":"EndpointR Development Guide","text":"Use roxygen2 markdown documentation Follow tidyverse style: snake_case function names, descriptive variables Chain operations native pipe (|>) Use tibble/data.frame data structures appropriate Include parameter types roxygen docs Use tidyeval data frame functions: capture variable names rlang::ensym() (e.g., text_sym <- rlang::ensym(text_var)) non-standard evaluation Error handling: use stopifnot() assertions cli::cli_abort() user-facing errors Format error messages cli syntax ({.val}, {.code}) Wrap examples requiring API request \\dontrun{} Use explicit return values document Keep functions small focused single responsibility API keys saved environment variables, input raw code","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"comment-guidelines","dir":"","previous_headings":"","what":"Comment Guidelines","title":"EndpointR Development Guide","text":"Write comments explain made new choice, stuff like ‘Check required parameters’ - Let obvious code speak Use lowercase comments British English (let AI use uppercase distinguish - helpful know whether ’s human AI comment)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"api-consistency","dir":"","previous_headings":"","what":"API Consistency","title":"EndpointR Development Guide","text":"Functions R/core.R intended ‘quite’ general - .e. form foundations rest package Generally want deal single text, batch text, df texts consistently, least predictably differently, across providers tasks","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"ai-usage","dir":"","previous_headings":"","what":"AI Usage","title":"EndpointR Development Guide","text":"Don’t commit code don’t understand AI writes code, better write tests. AI writes tests, better write code. Let AI help docs, make sure keep --date. able know ’s code, docs.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"git-branches","dir":"","previous_headings":"","what":"Git Branches","title":"EndpointR Development Guide","text":"Provide descriptive names possible feature/** bugfix/** documentation/** tests/**","code":""},{"path":"https://jpcompartir.github.io/EndpointR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 EndpointR authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"quick-start","dir":"Articles","previous_headings":"","what":"Quick start","title":"Managing API Keys Securely","text":"EndpointR two main functions managing API keys, get_api_key() set_api_key(), take ‘key_name’ input. security, set_api_key() uses askpass accept API keys rather code - means show .Rhistory, less likely leaked. First set API key: restart R session. Now get API key security reasons, print api_key console, pass function requires .","code":"set_api_key(\"TOY_API_KEY\") api_key <- get_api_key(\"TOY_API_KEY\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"what-is-an-api","dir":"Articles","previous_headings":"","what":"What is an API?","title":"Managing API Keys Securely","text":"API stands ‘Application Programming Interface’, mechanism two pieces software interact exchange data. APIs work requests - user sends request service, service sends response, fulfilling user’s request, providing error message. specific location within API allows user access service, function, called endpoint.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"what-is-an-api-key","dir":"Articles","previous_headings":"","what":"What is an API Key?","title":"Managing API Keys Securely","text":"many APIs provide access sensitive information, require credit card send requests, ’s important API providers able identify sending request, determine whether ’s safe respond, charge response. API key mechanism identifying sending request. See also: API keys","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"api-key-security","dir":"Articles","previous_headings":"","what":"API Key Security","title":"Managing API Keys Securely","text":"WARNING: EndpointR try help manage API keys safely, baseline level responsibility person using managed service needs take , set best practices need followed. paramount handle API keys securely. need avoid following things: Saving unencrypted API keys notes, emails, Google Docs etc. Uploading API keys web services (GitHub, etc.) Sharing API keys people Including unencrypted API keys code (scripts, R/Quarto/markdown files, .ipynb etc.) Unencypted API keys appearing .Rhistory file (especially file uploaded anywhere) suspect may done one - number - things, go directly got key, invalidate old one generate new one. key given , go directly person tell key compromised invalidate , provide new one. Instead , : Encrypt API keys Store environment variables using R/Rstudio/VScode Store managed secrets providers like GitHub use outside R/Rstudio/VScode","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"managing-multiple-keys","dir":"Articles","previous_headings":"","what":"Managing Multiple Keys","title":"Managing API Keys Securely","text":"endpoint EndpointR provides access , need correct environment variable stored .Renviron file API Key Lookup Table","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"what-are-text-embeddings","dir":"Articles","previous_headings":"","what":"What are Text Embeddings?","title":"Embeddings Providers","text":"Text embeddings numerical representations text capture semantic meaning. Think coordinates high-dimensional space similar texts closer together. ’re foundation : Semantic search Clustering similar documents Finding duplicates Building recommendation systems Powering RAG (Retrieval-Augmented Generation) applications EndpointR makes easy generate embeddings text data using either Hugging Face OpenAI APIs.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Embeddings Providers","text":"","code":"library(EndpointR) library(dplyr) library(tibble)  sample_texts <- tibble(   id = 1:3,   text = c(     \"Machine learning is transforming how we process information\",     \"I love building applications with embeddings\",      \"Natural language processing enables computers to understand text\"   ),   category = c(\"ML\", \"embeddings\", \"NLP\") )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"provider-comparison","dir":"Articles","previous_headings":"","what":"Provider Comparison","title":"Embeddings Providers","text":"diving code, let’s understand key differences:","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"setting-up","dir":"Articles","previous_headings":"Hugging Face Embeddings","what":"Setting Up","title":"Embeddings Providers","text":"First, get API key Hugging Face set . set endpoint’s URL. ’ve chosen endpoint accessing embeddings -mpnet-base-v2 model.","code":"set_api_key(\"HF_TEST_API_KEY\")  embed_url <-  \"https://router.huggingface.co/hf-inference/models/sentence-transformers/all-mpnet-base-v2/pipeline/feature-extraction\""},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"single-text","dir":"Articles","previous_headings":"Hugging Face Embeddings","what":"Single Text","title":"Embeddings Providers","text":"simplest case - embed one piece text:","code":"embedding <- hf_embed_text(   text = \"I want to understand the meaning of this sentence\",   endpoint_url = embed_url,   key_name = \"HF_TEST_API_KEY\" )  dim(embedding) # result: a tibble with 768 columns (V1 to V768)  embedding"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"batch-processing","dir":"Articles","previous_headings":"Hugging Face Embeddings","what":"Batch Processing","title":"Embeddings Providers","text":"multiple texts, use hf_embed_batch() handles batching automatically. feed vector inputs batch_size, function takes care batching vector many batches necessary. result includes: - text: original text - .error .error_message: error tracking - V1 V768: embedding dimensions","code":"texts_to_embed <- c(   \"First document about machine learning\",   \"Second document about deep learning\",   \"Third document about neural networks\",   \"Fourth document about data science\" )  batch_embeddings <- hf_embed_batch(   texts = texts_to_embed,   endpoint_url = embed_url,   key_name = \"HF_TEST_API_KEY\",   batch_size = 2,  # process 2 texts per API call   concurrent_requests = 2  # run 2 requests in parallel )  # Check results glimpse(batch_embeddings[1,1:10 ]) # truncated for ease"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"data-frame-integration","dir":"Articles","previous_headings":"Hugging Face Embeddings","what":"Data Frame Integration","title":"Embeddings Providers","text":"commonly, ’ll want embed column data frame:","code":"embedded_df <- hf_embed_df(   df = sample_texts,   text_var = text,      # column containing text   id_var = id,          # unique identifier column   endpoint_url = embed_url,   key_name = \"HF_TEST_API_KEY\",   batch_size = 3,   concurrent_requests = 1 )  # Original data + embeddings names(embedded_df)[1:10]  # shows: id, text, category, .error, .error_message, V1, V2...  embedded_df"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"setting-up-1","dir":"Articles","previous_headings":"OpenAI Embeddings","what":"Setting Up","title":"Embeddings Providers","text":"Get API key OpenAI website set :","code":"set_api_key(\"OPENAI_API_KEY\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"single-text-1","dir":"Articles","previous_headings":"OpenAI Embeddings","what":"Single Text","title":"Embeddings Providers","text":"OpenAI offers configurable embedding dimensions:","code":"# Default dimensions (1536 for text-embedding-3-small) embedding <- oai_embed_text(   text = \"I want to understand the meaning of this sentence\" )  # Custom dimensions for smaller embeddings small_embedding <- oai_embed_text(   text = \"I want to understand the meaning of this sentence\",   model = \"text-embedding-3-small\",   dimensions = 512  # reduce size by ~67% )  dim(small_embedding)  # 1 row, 512 embedding columns + index"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"batch-processing-1","dir":"Articles","previous_headings":"OpenAI Embeddings","what":"Batch Processing","title":"Embeddings Providers","text":"OpenAI allows multiple texts single API call, oai_embed_batch() leverages.","code":"texts_to_embed <- c(   \"First document about machine learning\",   \"Second document about deep learning\",   \"Third document about neural networks\",   \"Fourth document about data science\" )  batch_embeddings <- oai_embed_batch(   texts = texts_to_embed,   model = \"text-embedding-3-small\",   dimensions = 1536,  # default for this model   batch_size = 10,    # texts per API request   concurrent_requests = 3  # parallel requests )   batch_embeddings |>   reframe(     total = n(),     succeeded = sum(!.error),     failed = sum(.error)   )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"data-frame-integration-1","dir":"Articles","previous_headings":"OpenAI Embeddings","what":"Data Frame Integration","title":"Embeddings Providers","text":"","code":"embedded_df <- oai_embed_df(   df = sample_texts,   text_var = text,   id_var = id,   model = \"text-embedding-3-large\",  # higher quality embeddings   dimensions = 3072,  # maximum dimensions for this model   batch_size = 20,   concurrent_requests = 5 )  # Extract just the embeddings for downstream use embedded_df |>    select(starts_with(\"V\"))"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"openai-limits","dir":"Articles","previous_headings":"Handling Sequence Length","what":"OpenAI Limits","title":"Embeddings Providers","text":"OpenAI token limit 8,192 per request. Since 1 token ≈\\approx 4 characters: can truncate texts substr() function practice course want use intelligent splitting procedure.","code":"long_texts <- tibble(   id = 1:3,   text = c(     paste(rep(\"word\", 100), collapse = \" \"),    # ~400 chars, safe     paste(rep(\"word\", 8000), collapse = \" \"),   # ~32k chars, near limit       paste(rep(\"word\", 10000), collapse = \" \")   # ~40k chars, too long!   ) )  long_texts |>   mutate(     char_count = nchar(text),     approx_tokens = char_count / 4,     will_fail = approx_tokens > 8192   ) # truncation (data / information loss happens!) long_texts |>   mutate(     text = ifelse(nchar(text) > 32000,                    substr(text, 1, 32000),                    text)   ) |>   mutate(     char_count = nchar(text),     approx_tokens = char_count / 4,     will_fail = approx_tokens > 8192   )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"hugging-face-limits","dir":"Articles","previous_headings":"Handling Sequence Length","what":"Hugging Face Limits","title":"Embeddings Providers","text":"using Inference API, limits vary model. Check model’s documentation. models handle ~512 tokens well. modern models can handle (check model card). Dedicated Inference Endpoints receive many requests assigned hardware able handle. code chunk shows chunk texts ’re finding errors due payload size:","code":"chunk_text <- function(text, max_chars = 2000) {   if (nchar(text) <= max_chars) return(list(text))      # v. simple chunking (consider sentence boundaries/ more intelligent chunking in production)   chunks <- substring(text,                       seq(1, nchar(text), max_chars),                       seq(max_chars, nchar(text) + max_chars - 1, max_chars))   as.list(chunks[nchar(chunks) > 0]) }"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"error-handling","dir":"Articles","previous_headings":"Best Practices","what":"Error Handling","title":"Embeddings Providers","text":"Always check errors results. chunk shows send failures another batch request, beware ’ll need handle resulting data frames.","code":"results <- oai_embed_batch(texts = texts_to_embed)  # Check overall success if (any(results$.error)) {   failed <- results |>      filter(.error) |>     select(text, .error_message)      print(failed)      # Retry failed texts with adjusted parameters   retry_texts <- failed$text   retry_results <- oai_embed_batch(     texts = retry_texts,     batch_size = 1,  # one at a time     timeout = 30     # longer timeout   ) }"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"performance-tips","dir":"Articles","previous_headings":"Best Practices","what":"Performance Tips","title":"Embeddings Providers","text":"Start small: Begin batch_size = 5 concurrent_requests = 1. Scale gradually: Increase parameters whilst monitoring errors Hugging Face: -MiniLM-L6-v2 speed (384 dims) OpenAI: text-embedding-3-small custom dimensions flexibility Consider dedicated endpoints production Hugging Face deployments TIP: Check organisation’s tier OpenAI, tier 5 organisatons can send many requests tier 1. OpenAI Rate Limits","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"cost-optimisation","dir":"Articles","previous_headings":"Best Practices","what":"Cost Optimisation","title":"Embeddings Providers","text":"OpenAI: Reduce dimensions save storage computation WARNING: ’ll need compare performance vs size trade-particular use-case.","code":"compact_embeddings <- oai_embed_batch(   texts = texts_to_embed,   model = \"text-embedding-3-small\",   dimensions = 360 )"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"semantic-search","dir":"Articles","previous_headings":"Common Use Cases","what":"Semantic Search","title":"Embeddings Providers","text":"basic implementation semantic search (AKA dense embedding/vector search AKA neural search ) Embed document corpus Embed search query Find similar documents (cosine similarity) Extract top 5 similar TODO: add code ? NOTE: use-cases hybrid approach comprising full-text search semantic search yield best result","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"clustering","dir":"Articles","previous_headings":"Common Use Cases","what":"Clustering","title":"Embeddings Providers","text":"Generate embeddings Extract embeddings matrix clustering algorithm Run clustering algorithm Add clusters data frame TIP: practice need inspect outputs clustering model tune . code run model small part modelling process","code":"embeddings_for_clustering <- hf_embed_df(   df = sample_texts,   text_var = text,   id_var = id,   endpoint_url = embed_url,   key_name = \"HF_TEST_API_KEY\" )  embedding_matrix <- embeddings_for_clustering |>   select(starts_with(\"V\")) |>   as.matrix()  kmeans_result <- kmeans(embedding_matrix, centers = 2)  clustered_texts <- sample_texts |>   mutate(cluster = kmeans_result$cluster)"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"rate-limits","dir":"Articles","previous_headings":"Troubleshooting","what":"Rate Limits","title":"Embeddings Providers","text":"Use fewer concurrent_requests ’re running rate limit issues. rate limits request, others tokens. ’re running token limits, solution wait longer requests. ’re running request limits, increase batch_size, embed data fewer requests.","code":"results <- hf_embed_batch(   texts = large_text_collection,   batch_size = 3,   concurrent_requests = 1  # sequential processing )  results <- oai_embed_batch(   texts = large_text_collection,   batch_size = 5, # fewer requests with larger batch size   concurrent_requests = 2 )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"timeouts","dir":"Articles","previous_headings":"Troubleshooting","what":"Timeouts","title":"Embeddings Providers","text":"Increase value timeout parameter sending many requests, responses begin timing .","code":"results <- oai_embed_batch(   texts = texts_to_embed,   timeout = 60,    max_retries = 5 )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"memory-issues","dir":"Articles","previous_headings":"Troubleshooting","what":"Memory Issues","title":"Embeddings Providers","text":"Process chunks large datasets TIP: also write splits individual files, iterate files avoid reading data memory .","code":"library(purrr)  text_chunks <- split(large_text_vector,                       ceiling(seq_along(large_text_vector) / 1000))  all_embeddings <- map(text_chunks, ~{   oai_embed_batch(.x, batch_size = 10) }) |>   list_rbind()"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Embeddings Providers","text":"See Improving Performance vignette optimisation tips Check Hugging Face Inference classification tasks Explore different embedding models specific use case Remember: embeddings foundation many NLP applications. Choose provider based needs quality, speed, cost, flexibility.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Using Hugging Face Inference Endpoints","text":"Follow Hugging Face’s docs generate Hugging Face token, register EndpointR:","code":"library(EndpointR) library(dplyr) library(httr2) library(tibble)  my_data <- tibble(   id = 1:3,   text = c(     \"Machine learning is fascinating\",     \"I love working with embeddings\",      \"Natural language processing is powerful\"   ),   category = c(\"ML\", \"embeddings\", \"NLP\") ) set_api_key(\"HF_TEST_API_KEY\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"choosing-your-service","dir":"Articles","previous_headings":"","what":"Choosing Your Service","title":"Using Hugging Face Inference Endpoints","text":"Hugging Face offers two inference options: Inference API: Free, good testing Dedicated Endpoints: Paid, reliable, fast vignette, ’ll use Inference API. switch dedicated endpoints, just change URL.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting Started","title":"Using Hugging Face Inference Endpoints","text":"Go Hugging Face’s models hub fetch Inference API’s URL model want embed data . models available via Hugging Face Inference API, need use model available may need deploy Dedicated Inference Endpoint.","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"single-text","dir":"Articles","previous_headings":"Embeddings","what":"Single Text","title":"Using Hugging Face Inference Endpoints","text":"Embed one piece text: result tibble one row 384 columns (V1 V384). column embedding dimension. Note: number columns depends model. Check model’s Hugging Face page embedding size.","code":"# inference api url for embeddings embed_url <- \"https://router.huggingface.co/hf-inference/models/sentence-transformers/all-mpnet-base-v2/pipeline/feature-extraction\"  result <- hf_embed_text(   text = \"This is a sample text to embed\",   endpoint_url = embed_url,   key_name = \"HF_API_KEY\" )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"list-of-texts","dir":"Articles","previous_headings":"Embeddings","what":"List of Texts","title":"Using Hugging Face Inference Endpoints","text":"Embed multiple texts using batching: result includes: text: original text .error: TRUE something went wrong .error_message: went wrong (anything) V1 V384: embedding values","code":"texts <- c(   \"First text to embed\",   \"Second text to embed\",   \"Third text to embed\" )  batch_result <- hf_embed_batch(   texts,   endpoint_url = embed_url,   key_name = \"HF_API_KEY\",   batch_size = 3  # process 3 texts per request )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"data-frame","dir":"Articles","previous_headings":"Embeddings","what":"Data Frame","title":"Using Hugging Face Inference Endpoints","text":"commonly, ’ll want embed column data frame: Check errors: Extract just embeddings:","code":"embedding_result <- hf_embed_df(   df = my_data,   text_var = text,      # column with your text   id_var = id,          # column with unique ids   endpoint_url = embed_url,   key_name = \"HF_API_KEY\" ) embedding_result |> count(.error) embeddings_only <- embedding_result |> select(V1:V384)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"classification","dir":"Articles","previous_headings":"","what":"Classification","title":"Using Hugging Face Inference Endpoints","text":"Classification works way embeddings, just different URL output format. neceessary, can also provide custom function tidying output.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"single-text-1","dir":"Articles","previous_headings":"Classification","what":"Single Text","title":"Using Hugging Face Inference Endpoints","text":"","code":"classify_url <- \"https://router.huggingface.co/hf-inference/models/distilbert/distilbert-base-uncased-finetuned-sst-2-english\"  sentiment <- hf_classify_text(   text = \"I love this package!\",   endpoint_url = classify_url,   key_name = \"HF_API_KEY\" )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"data-frame-1","dir":"Articles","previous_headings":"Classification","what":"Data Frame","title":"Using Hugging Face Inference Endpoints","text":"result includes: original id column Classification labels (e.g., POSITIVE, NEGATIVE) Confidence scores Error tracking columns. NOTE: Classification labels model task specific.","code":"classification_result <- hf_classify_df(   df = my_data,   text_var = text,   id_var = id,   endpoint_url = classify_url,   key_name = \"HF_API_KEY\" )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"using-dedicated-endpoints","dir":"Articles","previous_headings":"","what":"Using Dedicated Endpoints","title":"Using Hugging Face Inference Endpoints","text":"use dedicated endpoints instead Inference API: Deploy model dedicated endpoint (see Hugging Face docs) Get endpoint URL Replace URL function: Note: Dedicated endpoints take 20-30 seconds start ’re idle. Set max_retries = 5 give time wake .","code":"# just change this line dedicated_url <- \"https://your-endpoint-name.endpoints.huggingface.cloud\"  # everything else stays the same result <- hf_embed_text(   text = \"Sample text\",   endpoint_url = dedicated_url,  # <- only change   key_name = \"HF_API_KEY\" )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"tips","dir":"Articles","previous_headings":"","what":"Tips","title":"Using Hugging Face Inference Endpoints","text":"Start small batch sizes (3-5) increase gradually Inference API rate limits - dedicated endpoints hardware constraints, increase hardware higher limits production use, choose dedicated endpoints Check Improving Performance vignette speed tips","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"common-issues","dir":"Articles","previous_headings":"","what":"Common Issues","title":"Using Hugging Face Inference Endpoints","text":"Rate limits: Reduce batch size add delays requests Model available: models work Inference API. Check model page use dedicated endpoints. Timeouts: Increase max_retries reduce batch size","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"improving-performance","dir":"Articles","previous_headings":"","what":"Improving Performance","title":"Using Hugging Face Inference Endpoints","text":"EndpointR’s functions come knobs dials can turn improve throughput performance. Visit Improving Performance vignette information.","code":""},{"path":[]},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"understanding-the-time-problem","dir":"Articles","previous_headings":"","what":"Understanding the Time Problem","title":"Improving Performance","text":"row data frame, need create request perform. perform request sending information internet endpoint. endpoint accepts request, performs computation, sends successful response error. step associated cost time, cost step equal - steps costly others. Creating request almost instant, larger data frame - e.g. 100,000 rows: creating 100,000 requests simultaneously take ≈\\approx 30s-90s. create , ’ll wait full amount time begin sending requests. Clearly wasteful - created first request sent , save least 90 seconds. Sending request endpoint waiting process usually take lot longer preparing request. Therefore, can find ways reduce time spend waiting , able reduce overall time takes prepare send requests significantly. vignette focus mainly hf_embed_df() function options improving throughput requests. ideas apply hf_classify_df() function, ideas transfer providers functions, e.g. oai_*().","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"set-up","dir":"Articles","previous_headings":"","what":"Set Up","title":"Improving Performance","text":"TODO: Copied hugging_face_inference.Rmd, need re-order edit ’ll need data, ’ll grab sentences data frame introductory vignette Hugging Face Inference: WARNING: follow along need provide endpoint_url set API key. using encrypted URL security purposes. ENDPOINTR_KEY person-specific (.e. access key) environment variable can used encrypt information, stored securely offline, committed GitHub online services.","code":"library(tibble) library(tidyr) library(EndpointR) library(ggplot2) library(httr2) library(dplyr) embedding_sentences <- c(   \"Text embedding models compress your rambling prose into compact vectors, much like how British commuters squeeze themselves into tube carriages during rush hour.\",   \"Setting up an inference endpoint without proper documentation is akin to attempting to navigate London using a map of Birmingham.\",   \"When your embedding model starts hallucinating, it's rather like watching Parliament during Question Time—entertaining but utterly unpredictable.\",   \"Optimising your inference endpoint is essential unless you fancy your users growing old whilst waiting for a response, perhaps even ageing enough to collect their pension.\",   \"The distance between word embeddings reveals semantic relationships, though sadly not the distance between what your client requested and what they actually wanted.\",   \"Creating multilingual embeddings is a bit like attempting to order tea properly across Europe—technically possible but fraught with cultural misunderstandings.\",   \"Batch processing through inference endpoints saves computing resources, much like how the British save conversation topics by discussing the weather exclusively.\",   \"Token limits on embedding APIs are the digital equivalent of a queue at a British post office—inevitably, you'll reach the front just as they close for lunch.\",   \"Fine-tuning embedding models on domain-specific corpora is rather like training a British child to apologise—it requires repetition, patience, and considerable examples.\",   \"When your inference endpoint crashes under load, it maintains that quintessentially British trait of breaking down precisely when most inconvenient.\" )  embedding_ids <- 1:10  embedding_df <-   tibble(     id = embedding_ids,     sentence = embedding_sentences   ) endpoint_url <- httr2::secret_decrypt(\"kcZCsc92Ty7PuAk7_FdCcdOU_dlDpvWdDyfpwCWg-wW80eJxJPdQ68nz4V_0922SzSwM5_dYfpTOqsZ-GodUpLN4PQbwE73wlZkBCWIaIXc15g\", \"ENDPOINTR_KEY\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"improving-performance---time","dir":"Articles","previous_headings":"","what":"Improving Performance - Time","title":"Improving Performance","text":"hf_embed_df function, main options increase number requests send simultaneously, number texts send within request. concurrent_requests lets EndpointR know many requests send time. number 1 ~20-50 (extreme cases may ok 100, endpoints accept ) batch_size lets EndpointR know whether send requests individual texts, number texts ‘batched ’ together.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"solution-1-concurrent-requests","dir":"Articles","previous_headings":"Improving Performance - Time","what":"Solution 1: Concurrent Requests","title":"Improving Performance","text":"Hugging Face Inference Endpoints can handle multiple requests arriving time. goal send just maximum number requests endpoint can handle. example, endpoint powerful GPU may able handle hundreds documents time, whereas endpoint small CPU able handle handful. guide, start ~5 concurrent requests, work ~20. start hitting rate limits go back ~10 find sweetspot. endpoint handling 20 requests without returning rate limit errors, experiment > 20 requests. **TIP*:: Endpoints can configured ‘autoscale’ - meaning provide additional hardware number queued requests configured threshold configured duration time. send 5 concurrent requests - iterate data frame 5 rows time, send new requests responses returned, run data embed. Let’s benchmark performance (see appendix code) - overhead associated generating parallel requests, ’ll need bigger data frame understand type speed can get. Recording results: can see ~40% reduction processing time going 5-> 10 concurrent requests ~15% reduction going 10 -> 20 concurrent requests. *Exact times fluctuate, take approximates.","code":"hf_embed_df(   df = embedding_df,   text_var = sentence,   endpoint_url = endpoint_url,   key_name = \"HF_TEST_API_KEY\",   id_var= id,   concurrent_requests = 5,   batch_size = 1,   progress = TRUE ) id_1000 <- 1:1000 sentences_1000 <- rep(embedding_df$sentence, 100) embedding_df_1000 <- tibble(id = id_1000, sentence = sentences_1000)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"solution-2-batching-requests","dir":"Articles","previous_headings":"Improving Performance - Time","what":"Solution 2: Batching Requests","title":"Improving Performance","text":"Another method reducing processing time send data within request. mean can send fewer requests overall, reduce time spent sending information network. Sending multiple rows data within request creates batch request. 1,000 rows data, batch_size = 10 result 100 requests sent, instead 1,000 batch_size = 1. TIP: experiment batch sizes find sweetspot - usually starting around 8-16, capping ~64. ’ll know ’ve gone high ’ll start seeing retry bar, /responses contain errors.","code":"hf_embed_df(   df = embedding_df_1000,   text_var = sentence,   endpoint_url = endpoint_url,   key_name = \"HF_TEST_API_KEY\",   id_var= id,   concurrent_requests = 1,   batch_size = 20,   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"solution-3-combining-concurrency-and-batching","dir":"Articles","previous_headings":"Improving Performance - Time","what":"Solution 3: Combining Concurrency and Batching","title":"Improving Performance","text":"maximum speed , can also send batches multiple concurrent requests. endpoint able handle , 10 concurrent requests batch_size = 10 faster 10 concurrent requests batch_size = 5.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"benchmarking-solutions","dir":"Articles","previous_headings":"","what":"Benchmarking Solutions","title":"Improving Performance","text":"separate session using dataset available package, explored relationship batch size, concurrent requests, throughput ; throughput number rows processed per second. looked combinations : batch_size = c(1, 4, 8, 16, 32) concurrent_requests = c(1, 5, 10, 15, 20) embed ≈\\approx 2,000 documents sending 1 text 1 request time, get throughput ≈\\approx 2.16 texts per second, takes 15 minutes! end, 20 concurrent requests batch size 8, 20 concurrent requests batch size 32 throughput $$195, close 200x quicke! get results back 10 seconds. NOTE: needed , re-run benchmarking code multiple times general trend clear.   parameter important speed, batch_size = concurrent_requests =? eye looks like concurrent_requests bigger effect, ’s clear parameters positive effect, within boundaries parameters, approximately linear relationship throughput. Putting together quick linear model confirms everything else equal, increasing concurrent_requests increases throughput batch_size QUESTION: might happen keep increasing batch_size concurrent_requests? relationship hold ?","code":"data(batch_concurrent_benchmark, package = \"EndpointR\")  knitr::kable(batch_concurrent_benchmark |> mutate_if(is.numeric, ~ round(.x, 2))) batch_concurrent_benchmark |>    mutate(batch_size = factor(batch_size), concurrent_requests= factor(concurrent_requests)) |>   ggplot(aes(x= batch_size, y = throughput, group = concurrent_requests)) +   geom_point(aes(colour = concurrent_requests)) +   geom_line(aes(colour = concurrent_requests)) +   labs(y = \"Throughput\", x = \"Batch Size\", title = \"Increasing `batch_size` and `concurrent_requests` increases throughput\") +   scale_colour_viridis_d() +   theme_minimal() +   theme(legend.position = \"bottom\") batch_concurrent_benchmark |>      mutate(batch_size = factor(batch_size),             concurrent_requests = factor(concurrent_requests)) |>       ggplot(aes(x = batch_size, y = concurrent_requests, fill = throughput)) +      geom_tile() +     geom_text(aes(label = round(throughput, 1)), colour = \"white\") +     scale_fill_viridis_c(name = \"Throughput\") +     theme_minimal() +     labs(x = \"Batch Size\", y = \"Concurrent Requests\",          title = \"Throughput by Batch Size and Concurrent Requests\") +     theme(legend.position = \"none\") model <- lm(throughput ~ batch_size + concurrent_requests, data = batch_concurrent_benchmark)  broom::tidy(model) |>     mutate(across(c(estimate, std.error, statistic), ~round(.x, 2))) #> # A tibble: 3 × 5 #>   term                estimate std.error statistic   p.value #>   <chr>                  <dbl>     <dbl>     <dbl>     <dbl> #> 1 (Intercept)             3.34     15.5       0.22 0.832     #> 2 batch_size              2.4       0.71      3.37 0.00366   #> 3 concurrent_requests     6.45      1.12      5.77 0.0000224"},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"understanding-the-memory-problem","dir":"Articles","previous_headings":"","what":"Understanding the Memory Problem","title":"Improving Performance","text":"TODO:","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"appendix---embeddings","dir":"Articles","previous_headings":"Understanding the Memory Problem","what":"Appendix - Embeddings","title":"Improving Performance","text":"TODO: date post-refactoring","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"benchmarking-concurrent-requests","dir":"Articles","previous_headings":"Understanding the Memory Problem > Appendix - Embeddings","what":"Benchmarking Concurrent Requests","title":"Improving Performance","text":"","code":"run_benchmark <- function(num_concurrent, data, endpoint, key) {   start_time <- Sys.time()   res_df <- try(hf_embed_df(                   df = data,                   text_var = sentence,                    id_var= id,                            endpoint_url = endpoint,                   key_name = key,                   include_errors = FALSE,                    concurrent_requests = num_concurrent,                   progress = FALSE                  ), silent = TRUE)    processing_time <- Sys.time() - start_time   success <- !inherits(res_df, \"try-error\") && nrow(res_df) == nrow(data)   return(data.frame(     concurrent_requests = num_concurrent,     processing_time_secs = as.numeric(processing_time, units = \"secs\"),     success = success   )) }  num_requests_vec <- c(5, 10, 20) results_list <- lapply(num_requests_vec, function(n) {   run_benchmark(     num_concurrent = n,     data = embedding_df_1000,     endpoint = endpoint_url,     key = \"HF_TEST_API_KEY\"    ) })  (summary_df <- do.call(rbind, results_list))"},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"benchmarking-batch-and-concurrent-requests","dir":"Articles","previous_headings":"Understanding the Memory Problem > Appendix - Embeddings","what":"Benchmarking Batch and Concurrent Requests","title":"Improving Performance","text":"won’t able re-run exactly -, data frame provided package. bring data wanted .","code":"trust <- readr::read_csv(\"~/data/trust/trust_slice_spam_classification.csv\") |>    select(text) |>   mutate(id = row_number()) |>   filter(!is.na(text), text != \"\") # stop NAs and empty vals crashing anything  chunk_size <- 2000 total_chunks <- 20  # this chunking logic is actually rubbish. trust_chunks <- trust |>   mutate(chunk_id = ceiling(id / chunk_size)) |>   group_split(chunk_id) |>   head(total_chunks)  benchmark_params <- crossing(   batch_size = c(1, 4, 8, 16, 32),   concurrent_requests = c(1, 5, 10, 20) ) |>   mutate(chunk_index = row_number() %% total_chunks + 1)  benchmark_results <- benchmark_params |>   mutate(result = pmap(list(batch_size, concurrent_requests, chunk_index), function(bs, cr, ci) {     current_chunk <- trust_chunks[[ci]]      start_time <- Sys.time()      res <- try(hf_embed_df(       df = current_chunk,       text_var = text,       id_var = id,       endpoint_url = endpoint_url,       key_name = \"HF_TEST_API_KEY\",       batch_size = bs,       concurrent_requests = cr,       progress = TRUE     ), silent = TRUE)      elapsed_time <- as.numeric(Sys.time() - start_time, units = \"secs\")     success <- !inherits(res, \"try-error\")     rows_processed <- if(success) nrow(current_chunk) else 0      list(       elapsed_time = elapsed_time,       success = success,       rows_processed = rows_processed,       throughput = if(success) rows_processed / elapsed_time else 0     )   })) |>   nest_wider(result)"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"openai---quick-start---chat-completions-api---single-text","dir":"Articles","previous_headings":"","what":"OpenAI - Quick Start - Chat Completions API - Single Text","title":"Connecting to Major Model Providers","text":"get completion single text can use oai_complete_text() function: Output: [1] “sentiment text positive. speaker expresses happiness admiration fantastic weather, along hopeful wish experience similar conditions elsewhere.”","code":"set_api_key(\"OPENAI_API_KEY\")  sentiment_system_prompt = \"Analyse the sentiment of the given text.\" text = \"The weather has been absolutely fantastic this summer. I wish it could be like this every year, maybe I'll move to the South of France where they get 300 days of sunshine a year. Oh to dream.\"   oai_complete_text(   text = text,   system_prompt = sentiment_system_prompt,   model = \"gpt-4.1-nano\" )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"openai---quick-start---chat-completions-api---data-frame-of-texts","dir":"Articles","previous_headings":"OpenAI - Quick Start - Chat Completions API - Single Text","what":"OpenAI - Quick Start - Chat Completions API - Data Frame of Texts","title":"Connecting to Major Model Providers","text":"can input data frame directly oai_complete_df() function:","code":"review_df <- data.frame(   id = 1:5,   text = c(     \"Absolutely fantastic service! The staff were incredibly helpful and friendly.\",     \"Terrible experience. Food was cold and the waiter was rude.\",     \"Pretty good overall, but nothing special. Average food and service.\",     \"Outstanding meal! Best restaurant I've been to in years. Highly recommend!\",     \"Disappointed with the long wait times. Food was okay when it finally arrived.\"   ) )  oai_complete_df(   review_df,   text_var = text,   id_var = id,   output_file = NULL, # leave this to 'auto' to have your results written to a file in your current working directory   system_prompt = sentiment_system_prompt,   concurrent_requests = 2,    chunk_size = 5 ) ℹ Processing 5 texts in 1 chunk of up to 5 each ℹ Performing 5 requests in parallel (with 2 concurrent requests)... ✔ Batch 1: 5 successful, 0 failed                                                                  ✔ Completed processing: 5 successful, 0 failed # A tibble: 5 × 5                                                                                         id content                                                             .error .error_msg .batch     1     1 \"The sentiment of the text is highly positive.\"                     FALSE  NA              1 2     2 \"The sentiment of the text is negative.\"                            FALSE  NA              1 3     3 \"The sentiment of the text is generally neutral with a slight lean… FALSE  NA              1 4     4 \"The sentiment of the text is highly positive.\"                     FALSE  NA              1 5     5 \"The sentiment of the text is negative.\"                            FALSE  NA              1 >"},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"openai---quick-start---structured-outputs","dir":"Articles","previous_headings":"","what":"OpenAI - Quick Start - Structured Outputs","title":"Connecting to Major Model Providers","text":"issue responses far, follow specific format. means need parse results . However, can use schema determine format response: Now need extract sentiment value sentiment column, one per row: WARNING: lot data - .e. 100,000s, 1,000,000s rows, may want split data last step chunks via parallel processing reduce peak memory usage.","code":"sentiment_schema <- create_json_schema(   name = \"simple_sentiment_schema\",   schema = schema_object(     sentiment = schema_string(description = \"Sentiment classification\",                               enum = c(\"positive\", \"negative\", \"neutral\")),     required = list(\"sentiment\")   ) )   structured_df <- oai_complete_df(   review_df,   text_var = text,   id_var = id,   schema = sentiment_schema,   output_file = NULL,   system_prompt = sentiment_system_prompt,   concurrent_requests = 2,    chunk_size = 5 ) Processing 5 texts in 1 chunk of up to 5 each ℹ Performing 5 requests in parallel (with 2 concurrent requests)... ✔ Batch 1: 5 successful, 0 failed                                                                  ✔ Completed processing: 5 successful, 0 failed # A tibble: 5 × 5                                                                                         id content                        .error .error_msg .batch     1     1 \"{\\\"sentiment\\\":\\\"positive\\\"}\" FALSE  NA              1 2     2 \"{\\\"sentiment\\\":\\\"negative\\\"}\" FALSE  NA              1 3     3 \"{\\\"sentiment\\\":\\\"neutral\\\"}\"  FALSE  NA              1 4     4 \"{\\\"sentiment\\\":\\\"positive\\\"}\" FALSE  NA              1 5     5 \"{\\\"sentiment\\\":\\\"negative\\\"}\" FALSE  NA              1 > structured_df |>   dplyr::mutate(content = purrr::map(content, ~safely_from_json(.x))) |>    tidyr::unnest_wider(content) # A tibble: 5 × 5      id sentiment .error .error_msg .batch    1     1 positive  FALSE  NA              1 2     2 negative  FALSE  NA              1 3     3 neutral   FALSE  NA              1 4     4 positive  FALSE  NA              1 5     5 negative  FALSE  NA              1"},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"openai---under-the-hood","dir":"Articles","previous_headings":"","what":"OpenAI - Under the Hood","title":"Connecting to Major Model Providers","text":"looked oai_complete_text() oai_complete_df() high-level convenience functions, ’s good know ’s happening hood.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"chat-completions-api","dir":"Articles","previous_headings":"OpenAI - Under the Hood","what":"Chat Completions API","title":"Connecting to Major Model Providers","text":"“Chat Completions API endpoint generate model response list messages comprising conversation.” use-cases EndpointR covers, conversation single interaction user model. Usually goal achieve narrow task repeatedly. TIP: looking persistent, open-ended chats LLMs, ’s best head Claude, ChatGPT, Gemini - provider choice. type tasks might use Completions API ? Document classification, e.g. Sentiment Analysis Translation Structured data extraction","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"sentiment-analysis","dir":"Articles","previous_headings":"OpenAI - Under the Hood > Chat Completions API","what":"Sentiment Analysis","title":"Connecting to Major Model Providers","text":"Whilst generally recommend using OpenAI’s models sentiment analysis, 1 task people familiar . set basic 2 system prompt tell LLM want text send : can create request inspect {httr2}’s req_dry_run() function - ’s good hygiene check request formatted expect , particularly using first time. example, intend system prompt, {“role”: “system”} key:value pair inside messages list? value “model”: valid model ID? Etc. TIP:may need familiarise OpenAI API Documentation proceeding. generated HTTP request look something like : EndpointR (via {httr2}) handles HTTP mechanics: authentication, request headers, endpoint configuration. constructs JSON payload specified model, system prompt, user message, whilst setting sensible defaults temperature 3 max_tokens 4. demonstrative purposes, ran prompt data three times (see ) First run: “sentiment text frustrated exasperated.” Second run: “sentiment text quite negative. speaker appears frustrated exasperated, expressing dissatisfaction situation.” Third run: ““sentiment text quite negative. speaker appears frustrated exasperated, expressing dissatisfaction impatience situation.” Whilst responses directionally/approximately accurate, inconsistent output can unpleasant work . scaled hundreds thousands requests, inconsistency can extremely draining, productivity reducing. wanted response conform usual sentiment categories, need write custom parser extract ‘negative’, ‘neutral’, ‘positive’ output. Looking first output, ’s clear parser need reasonably sophisticated. send another request, asking model please output ‘positive’, ‘negative’, ‘neutral’ 5. Clearly work willing . ’ll look techniques deal systematically, achieve predictable outputs Structured Outputs section. model hand text difficult traditional, three-category 6, document-level sentiment analysis? [1] “sentiment text mixed, positive feelings expressed interface (”brilliant”) negative feelings performance (“absolutely dreadful”).” model smart enough recognise sentiment fit neatly ‘positive’, ‘negative’, ‘neutral’ output formatted nice way downstream use.","code":"system_prompt <- \"Analyse the text's sentiment: \"  text <- \"Oh man, I'm getting to the end of my limit with this thing. WHY DOESN'T IT JUST WORK?!?\" sentiment_request <- oai_build_completions_request(   text,   system_prompt = system_prompt )   sentiment_request |>   req_dry_run() POST /v1/chat/completions HTTP/1.1 accept: */* accept-encoding: deflate, gzip authorization:  content-length: 247 content-type: application/json host: api.openai.com user-agent: EndpointR  {   \"model\": \"gpt-4.1-nano\",   \"messages\": [     {       \"role\": \"system\",       \"content\": \"Analyse the text's sentiment: \"     },     {       \"role\": \"user\",       \"content\": \"Oh man, I'm getting to the end of my limit with this thing. WHY DOESN'T IT JUST WORK?!?\"     }   ],   \"temperature\": 0,   \"max_tokens\": 500 } sentiment_response <- sentiment_request |>    perform_request_or_return_error() sentiment_response |>   resp_check_status() |>    resp_body_json() |>    pluck(\"choices\", 1, \"message\", \"content\") ambiguous_text <- \"The interface is brilliant but the performance is absolutely dreadful\"  ambiguous_sentiment <- oai_build_completions_request(   ambiguous_text,   system_prompt = system_prompt ) |>    perform_request_or_return_error() |>    resp_body_json() |>    pluck(\"choices\", 1, \"message\", \"content\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"multiple-texts","dir":"Articles","previous_headings":"OpenAI - Under the Hood > Chat Completions API","what":"Multiple Texts","title":"Connecting to Major Model Providers","text":"Individual requests useful applications EndpointR built handle , whilst taking care implementation details like concurrent requests, retries, failing gracefully. Let’s experiment better system prompt list texts: running testing, took 8.5 seconds 10 requests send one time. took 1.8 seconds send 10 requests parallel - ~4.7x speed , showing overhead cost sending requests parallel - .e. see 10x speed increase 10x requests. Now extract content response, can see 7 response classification belonging classes - making prompt slightly better helped us get better final result. However, repeat many texts - hundreds/thousands, ’s unlikely every single response conform categories. Without instruction, models tendency slightly change output unpredictable intervals.","code":"updated_sentiment_prompt <- \"Classify the text into sentiment categories. The accepted categories are 'positive', 'negative', 'neutral', and 'mixed'. A 'mixed' text contains elements of positive and negative. \" classical_texts <- c(  \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",  \"All happy families are alike; each unhappy family is unhappy in its own way.\",  \"It was the best of times, it was the worst of times.\",  \"Call me Ishmael.\",  \"The sun shone, having no alternative, on the nothing new.\",  \"All animals are equal, but some animals are more equal than others.\",  \"So we beat on, boats against the current, borne back ceaselessly into the past.\",  \"The heart was made to be broken.\",  \"Tomorrow, and tomorrow, and tomorrow, creeps in this petty pace from day to day.\",  \"I have always depended on the kindness of strangers.\" ) classical_requests <- oai_build_completions_request_list(   classical_texts,   system_prompt = updated_sentiment_prompt ) start_seq <- Sys.time() classical_responses  <- classical_requests |>    perform_requests_with_strategy() end_seq <- Sys.time() - start_seq start_par <- Sys.time() classical_responses  <- classical_requests |>    perform_requests_with_strategy(concurrent_requests = 10) end_par <- Sys.time() - start_par classical_responses |>    map(~ resp_body_json(.x) |>          pluck(\"choices\", 1, \"message\", \"content\") |>          as_tibble()   ) |>    list_rbind() |>    mutate(text = classical_texts, .before = 1)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"data-frame-of-texts","dir":"Articles","previous_headings":"OpenAI - Under the Hood > Chat Completions API","what":"Data Frame of Texts","title":"Connecting to Major Model Providers","text":"function oai_complete_df() takes data frame, id variable, text variable mandatory inputs, returns data frame columns: id_var, text+var, .error_msg, .error, .batch. oai_complete_df() function knobs can turn improve throughput, reduce memory usage, increase likelihood successful response. default split data frame chunk_size = chunks. chunk processed order, file created store results previous batches. end chunks results retrieved file. help reduce likelihood : Lose API responses long-running calculations fail part-way Run memory responses list grows size data frame. function reports progress happens, letting know chunk processed, e.g. 1/5, progress bar requests within chunk. function provides summary failures successes directly console chunk, end chunks.","code":"df_classical_texts <- tibble(   id = 1:10,   text = classical_texts ) oai_complete_df(df_classical_texts,                  text_var = text,                  id_var = id,                 concurrent_requests = 5,                 output_file = NULL # set this to write to a temporary file, useful for documentation and testing.                 )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"openai-structured-outputs","dir":"Articles","previous_headings":"OpenAI - Under the Hood","what":"OpenAI Structured Outputs","title":"Connecting to Major Model Providers","text":"textual responses get LLM providers difficult deal programmatically, make guarantees form response. example, ask LLM classify documents, sometimes give just classification - ‘positive’, sometimes add pre-amble ‘document positive’, sometimes something else entirely. detailed information creating JSON schemas structured outputs, see vignette(\"structured_outputs_json_schema\").","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"anthropic","dir":"Articles","previous_headings":"","what":"Anthropic","title":"Connecting to Major Model Providers","text":"TBC","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"google","dir":"Articles","previous_headings":"","what":"Google","title":"Connecting to Major Model Providers","text":"TBC","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"introduction-to-structured-outputs","dir":"Articles","previous_headings":"","what":"Introduction to Structured Outputs","title":"Structured Outputs - JSON Schema","text":"Structured outputs ensure LLMs return data exactly format need. Instead parsing messy text, get validated JSON matching schema. use structured outputs? - eliminates parsing errors inconsistent formats - guarantees data types (numbers vs strings, booleans vs text) - enables reliable data extraction pipelines - reduces prompt engineering overhead Basic workflow: 1. define schema using helper functions 2. send requests OpenAI schema passed 3. validate response validate_response()","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"quick-start","dir":"Articles","previous_headings":"","what":"Quick Start","title":"Structured Outputs - JSON Schema","text":"WARNING: using schema structured outputs OpenAI, fields MUST required: NOTE: first time send request schema, take longer usual. “Typical schemas take 10 seconds process first request, complex schemas may take minute.” 1","code":"contact_schema <- create_json_schema(  name = \"contact_info\",  schema = schema_object(    name = schema_string(\"person's full name\"),    email = schema_string(\"email address\"),    phone = schema_string(\"phone number\"),    required = list(\"name\", \"email\", \"phone\"),    additional_properties = FALSE  ) )  req <- oai_build_completions_request(   input = \"Am I speaking with Margaret Phillips? Yes, ok, and your email is mphil@hotmail.co.uk. Ok perfect, and your phone number? Was that 07564789789? Ok great. Just a second please Margaret, you're verified\",   schema = contact_schema )  resp <- req_perform(req)  resp |>   resp_body_json() |>    pluck(\"choices\", 1, \"message\", \"content\") |>    validate_response(schema = contact_schema) |>    tibble::as_tibble()"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"schema-types","dir":"Articles","previous_headings":"","what":"Schema Types","title":"Structured Outputs - JSON Schema","text":"EndpointR provides several schema types match different data extraction needs. type enforces specific constraints validations: schema_string() - text data schema_number() / schema_integer() - numeric values optional min/max schema_boolean() - true/false values schema_enum() - predefined choices schema_array() - lists items schema_object() - nested structures Let’s explore type practical examples: can inspect schema print human-readable form json_dump() jsonlite::toJSON() Another, complicated, schema product review extraction, introduce schema_array:","code":"# text classification with enums sentiment_schema <- create_json_schema(   name = \"sentiment_analysis\",   schema = schema_object(     sentiment = schema_enum(       c(\"positive\", \"negative\", \"neutral\"),       \"overall sentiment of text\"     ),     confidence = schema_number(       \"confidence score\",        minimum = 0,        maximum = 1     ),     is_spam = schema_boolean(\"contains spam content\"),     required = c(\"sentiment\", \"confidence\", \"is_spam\")   ) ) json_dump(sentiment_schema) |>    jsonlite::toJSON(pretty = TRUE, auto_unbox = TRUE) rating_schema <- create_json_schema(   name = \"product_review\",   schema = schema_object(     rating = schema_integer(\"star rating\", minimum = 1, maximum = 5),     title = schema_string(\"review title\"),     pros = schema_array(       schema_string(),       \"positive aspects mentioned\"     ),     cons = schema_array(       schema_string(),       \"negative aspects mentioned\"     ),     would_recommend = schema_boolean(\"recommends product\"),     required = c(\"rating\",\"title\", \"pros\", \"cons\", \"would_recommend\")   ) )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"complex-nested-structures","dir":"Articles","previous_headings":"","what":"Complex Nested Structures","title":"Structured Outputs - JSON Schema","text":"Finally fairly complex schema - supplier schema_object within schema_object, line_items sits within schema_object, schema_array, schema_object, multiple schema_* objects.","code":"# invoice parsing with line items invoice_schema <- create_json_schema(   name = \"invoice_data\",   schema = schema_object(     # header information     invoice_number = schema_string(\"invoice reference number\"),     issue_date = schema_string(\"date issued (YYYY-MM-DD format)\"),     due_date = schema_string(\"payment due date (YYYY-MM-DD format)\"),     # billing details     supplier = schema_object(       name = schema_string(\"supplier company name\"),       address = schema_string(\"supplier address\"),       vat_number = schema_string(\"VAT registration number\"),       required = c(\"name\")     ),     customer = schema_object(       name = schema_string(\"customer name\"),       address = schema_string(\"customer address\"),       required = c(\"name\")     ),     # line items array     line_items = schema_array(       schema_object(         description = schema_string(\"item description\"),         quantity = schema_integer(\"quantity ordered\", minimum = 1),         unit_price = schema_number(\"price per unit\", minimum = 0),         line_total = schema_number(\"total for this line\", minimum = 0),         required = c(\"description\", \"quantity\", \"unit_price\", \"line_total\")       ),       \"invoice line items\",       min_items = 1     ),     # totals     subtotal = schema_number(\"subtotal before tax\", minimum = 0),     vat_amount = schema_number(\"VAT amount\", minimum = 0),     total_amount = schema_number(\"final total amount\", minimum = 0)   ) )  invoice_schema@schema$required <-  names(invoice_schema@schema$properties)  # This helper line ensures ALL properties are marked as required, which is   mandatory for OpenAI's structured outputs. Without this, the API will reject  the schema. Use this pattern when you want all fields to be required rather  than listing them individually."},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"validation","dir":"Articles","previous_headings":"","what":"Validation","title":"Structured Outputs - JSON Schema","text":"schema type enforces specific constraints. method validating whether specific responses meet schema’s constraints.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"how-validation-works","dir":"Articles","previous_headings":"Validation","what":"How Validation Works","title":"Structured Outputs - JSON Schema","text":"using structured outputs LLM providers: API-side enforcement: provider ensures generated responses match schema Local validation: - validate_response() double-checks data integrity locally dual approach catches generation errors data transmission issues. ’s comprehensive example: QUESTION: ’s wrong schema want use OpenAI API? properties set required fields, meaning use schema OpenAI’s structured outputs. Now let’s see happens try validate mocked response object conforms schema: try validate mocked response object conform schema:","code":"user_profile_schema <- create_json_schema(  name = \"user_profile\",  schema = schema_object(    # string fields    name = schema_string(\"full name\"),    bio = schema_string(\"user biography\"),        # numeric fields    age = schema_integer(\"age in years\", minimum = 13, maximum = 120),    account_balance = schema_number(\"balance in pounds\", minimum = 0),    is_verified = schema_boolean(\"account verified status\"),    newsletter_opt_in = schema_boolean(\"subscribed to newsletter\"),    subscription_tier = schema_enum(      c(\"free\", \"premium\", \"enterprise\"),      \"subscription level\"),    priority = schema_enum(      c(1, 2, 3),      \"support priority level\",      type = \"integer\"    ),    interests = schema_array(      schema_string(),      \"user interests\",      min_items = 1,      max_items = 10    ),        required = c(\"name\", \"age\", \"is_verified\", \"subscription_tier\")  ) ) valid_user <- '{   \"name\": \"Alice Smith\",   \"age\": 28,   \"account_balance\": 156.75,   \"is_verified\": true,   \"newsletter_opt_in\": false,   \"subscription_tier\": \"premium\",   \"priority\": 2,   \"interests\": [\"data science\", \"functional programming\", \"statistics\"] }'  validated_data <- validate_response(user_profile_schema, valid_user) str(validated_data) invalid_age <- '{   \"name\": \"Young User\",   \"age\": 10,   \"is_verified\": true,   \"subscription_tier\": \"free\" }'  validate_response(user_profile_schema, invalid_age)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"working-with-s7-objects","dir":"Articles","previous_headings":"","what":"Working with S7 Objects","title":"Structured Outputs - JSON Schema","text":"EndpointR uses S7 objects schema system. provides better type safety validation, means familiar S3 methods won’t work expected. Understanding work objects help debug issues customise schemas. example, call: jsonlite::toJSON(contact_schema). try, ’ll get error: Error: method asJSON S3 class: S7_object Instead, use S7 method json_dump defined json_schema class. converts schema R list ready converted JSON. won’t print list -long ugly, instead can check structure: Alongside json_dump EndpointR::json_schema given validate_response method saw used earlier quickstart.","code":"contact_json_dump <- json_dump(contact_schema) str(contact_json_dump)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"converting-schema-objects-to-json","dir":"Articles","previous_headings":"Working with S7 Objects","what":"Converting Schema Objects to JSON","title":"Structured Outputs - JSON Schema","text":"can convert dumped schema JSON object using {jsonlite}‘s toJSON function. object now class ’json’. can convert schema back regular R list {jsonlite}’s fromJSON function:","code":"contact_json_schema <-    toJSON(contact_json_dump,                   pretty = TRUE,                   auto_unbox = TRUE)  class(contact_json_schema) {   \"type\": \"json_schema\",   \"json_schema\": {     \"name\": \"contact_info\",     \"schema\": {       \"type\": \"object\",       \"properties\": {         \"name\": {           \"type\": \"string\",           \"description\": \"person's full name\"         },         \"email\": {           \"type\": \"string\",           \"description\": \"email address\"         },         \"phone\": {           \"type\": \"string\",           \"description\": \"phone number\"         }       },       \"additionalProperties\": false,       \"required\": [\"name\", \"email\", \"phone\"]     },     \"strict\": true   } } from_contact_json_schema <- fromJSON(contact_json_schema)  class(from_contact_json_schema)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best Practices","title":"Structured Outputs - JSON Schema","text":"Schema design principles: Use descriptive field names descriptions Set appropriate constraints (min/max values, required fields) Prefer enums free text categories Nest objects logically complex data Validate mock responses advance","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"troubleshooting-tips","dir":"Articles","previous_headings":"","what":"Troubleshooting tips:","title":"Structured Outputs - JSON Schema","text":"Use json_dump() inspect final schema structure use jsonlite::toJSON(x, pretty = TRUE) view schema human-readable form Test schemas mock data using validate_response() Start simple add complexity incrementally Check enum values match expected model outputs Validate required fields cover essential data Make sure properties required using OpenAI API structured outputs","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jack Penzer. Author, maintainer.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Penzer J (2025). EndpointR: Connects various Machine Learning inference providers. R package version 0.1.1, https://jpcompartir.github.io/EndpointR/.","code":"@Manual{,   title = {EndpointR: Connects to various Machine Learning inference providers},   author = {Jack Penzer},   year = {2025},   note = {R package version 0.1.1},   url = {https://jpcompartir.github.io/EndpointR/}, }"},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"endpointr","dir":"","previous_headings":"","what":"EndpointR","title":"EndpointR","text":"EndpointR ‘batteries included’, open-source R package connecting various Application Programming Interfaces (APIs) Machine Learning model predictions. TIP: experienced programmer, experience hitting APIs, consider going directly httr2","code":""},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"EndpointR","text":"EndpointR put CRAN, can download install latest development version following code:","code":"library(EndpointR) remotes::install_github(\"jpcompartir/EndpointR\")"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"hugging-face---embeddings","dir":"","previous_headings":"","what":"Hugging Face - embeddings","title":"EndpointR","text":"Securely set API key Point endpoint - ‘-mpnet-base-v2’ model feature extraction (embeddings) Embed single text: Embed list texts batches: Embed data frame texts:","code":"set_api_key(\"HF_API_KEY\") endpoint_url <- \"https://router.huggingface.co/hf-inference/models/sentence-transformers/all-mpnet-base-v2/pipeline/feature-extraction\" hf_embed_text(   text = \"Convert this text to embeddings\",   endpoint_url = endpoint_url,   key_name = \"HF_API_KEY\" ) review_texts <-c(     \"Absolutely fantastic service! The staff were incredibly helpful and friendly.\",     \"Terrible experience. Food was cold and the waiter was rude.\",     \"Pretty good overall, but nothing special. Average food and service.\",     \"Outstanding meal! Best restaurant I've been to in years. Highly recommend!\",     \"Disappointed with the long wait times. Food was okay when it finally arrived.\"   )  hf_embed_batch(   texts = review_texts,   endpoint_url = endpoint_url,   key_name = \"HF_API_KEY\",   batch_size = 3,   concurrent_requests = 2 ) review_data <- tibble::tibble(   review_id = 1:5,   review_text = review_texts ) hf_embed_df(   df = review_data,   text_var = review_text,   id_var = review_id,   endpoint_url = endpoint_url,   key_name = \"HF_API_KEY\",   concurrent_requests = 2,   batch_size = 3 )"},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"hugging-face---classification","dir":"","previous_headings":"","what":"Hugging Face - Classification","title":"EndpointR","text":"Select Classification Endpoint URL Classify single text: ’ll need grab label2id mapping model’s card: Cardiff NLP model info Classify data frame: Read Hugging Face Inference Vignette infromation embedding classifying using Dedicated Inference Endpoints Inference API Hugging Face.","code":"sentiment_endpoint <- \"https://router.huggingface.co/hf-inference/models/cardiffnlp/twitter-roberta-base-sentiment\" labelid_2class <- function() {   return(list(negative = \"LABEL_0\",               neutral = \"LABEL_1\",               positive = \"LABEL_2\")) }   hf_classify_text(   text = review_texts[[1]],   endpoint_url = sentiment_endpoint,   key_name = \"HF_API_KEY\" ) |>     dplyr::rename(!!!labelid_2class()) hf_classify_df(   df = review_data,   text_var = review_text,   id_var = review_id,   endpoint_url = sentiment_endpoint,   key_name = \"HF_API_KEY\",   batch_size = 8,   concurrent_requests = 3 ) |>   dplyr::rename(!!!labelid_2class())"},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"openai---chat-completions-api","dir":"","previous_headings":"","what":"OpenAI - Chat Completions API","title":"EndpointR","text":"Make sure ’ve set API key: Complete single text: Complete single text schema tidy: Complete Data Frame texts: Complete Data Frame texts schema: Read LLM Providers Vignette, Structured Outputs Vignette information common workflows OpenAI Chat Completions API 1","code":"set_api_key(\"OPENAI_API_KEY\") oai_complete_text(   text = review_texts[[2]],   system_prompt = \"Classify the sentiment of the following text: \" ) sentiment_schema <- create_json_schema(   name = \"sentiment_analysis\",   schema = schema_object(     sentiment = schema_string(\"positive, negative, or neutral\"),     confidence = schema_number(\"confidence score between 0 and 1\"), # we don't necessarily recommend asking a model for its confidence score, this is mainly a schema-construction demo!     required = list(\"sentiment\", \"confidence\")   ) )  oai_complete_text(   text = review_texts[[2]],   system_prompt = \"Classify the sentiment of the following text: \",   schema = sentiment_schema,   tidy = TRUE ) |>    tibble::as_tibble() oai_complete_df(   df = review_data,   text_var = review_text,   id_var = review_id,   system_prompt = \"Classify the following review:\",   key_name = \"OPENAI_API_KEY\",   concurrent_requests = 5 # send 5 rows of data simultaneously ) oai_complete_df(   df = review_data,   text_var = review_text,   id_var = review_id,   system_prompt = \"Classify the following review:\",   schema = sentiment_schema,   key_name = \"OPENAI_API_KEY\",   concurrent_requests = 5 # send 5 rows of data simultaneously )"},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"api-key-security","dir":"","previous_headings":"","what":"API Key Security","title":"EndpointR","text":"Read httr2 vignette managing API keys securely encrypting . Read EndpointR API Keys vignette information API keys need wach endpoint support, securely import API keys .Renvironfile.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/EndpointR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"EndpointR: Connects to various Machine Learning inference providers — EndpointR-package","title":"EndpointR: Connects to various Machine Learning inference providers — EndpointR-package","text":"EndpointR 'batteries included', open-source R package connecting various APIs Machine Learning model predictions. EndpointR built company-specific use cases, may useful wide audience.","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/reference/EndpointR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"EndpointR: Connects to various Machine Learning inference providers — EndpointR-package","text":"Maintainer: Jack Penzer Jack.penzer@sharecreative.com","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/base_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a base HTTP POST request for API endpoints — base_request","title":"Create a base HTTP POST request for API endpoints — base_request","text":"Constructs base httr2 POST request object common headers authentication. function sets foundation API requests standard configuration.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/base_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a base HTTP POST request for API endpoints — base_request","text":"","code":"base_request(endpoint_url, api_key)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/base_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a base HTTP POST request for API endpoints — base_request","text":"endpoint_url Character string containing API endpoint URL api_key Character string containing API authentication key","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/base_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a base HTTP POST request for API endpoints — base_request","text":"httr2_request object configured POST method, JSON content type, bearer token authentication","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/base_request.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a base HTTP POST request for API endpoints — base_request","text":"","code":"if (FALSE) { # \\dontrun{   # Create a base POST request for an API endpoint   req <- base_request(     endpoint_url = \"https://api.example.com/v1/endpoint\",     api_key = \"your-api-key-here\"   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.html","id":null,"dir":"Reference","previous_headings":"","what":"Batch concurrent benchmark results — batch_concurrent_benchmark","title":"Batch concurrent benchmark results — batch_concurrent_benchmark","text":"Benchmark data comparing different batch sizes concurrent request configurations Hugging Face API calls.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Batch concurrent benchmark results — batch_concurrent_benchmark","text":"","code":"batch_concurrent_benchmark"},{"path":"https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Batch concurrent benchmark results — batch_concurrent_benchmark","text":"data frame 20 rows 7 variables: batch_size Integer; size batch processed concurrent_requests Integer; number concurrent requests chunk_index Integer; index processed chunk elapsed_time Numeric; time taken seconds success Logical; whether operation succeeded rows_processed Integer; number rows successfully processed throughput Numeric; requests processed per second","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Batch concurrent benchmark results — batch_concurrent_benchmark","text":"Internal benchmarking EndpointR functions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.html","id":null,"dir":"Reference","previous_headings":"","what":"Split a data frame into chunks for batch processing — chunk_dataframe","title":"Split a data frame into chunks for batch processing — chunk_dataframe","text":"Splits data frame chunks specified size.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split a data frame into chunks for batch processing — chunk_dataframe","text":"","code":"chunk_dataframe(df, chunk_size)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split a data frame into chunks for batch processing — chunk_dataframe","text":"df data frame split batches chunk_size Number rows per batch","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split a data frame into chunks for batch processing — chunk_dataframe","text":"list data frames, chunk_size rows","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/create_json_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a JSON Schema object — create_json_schema","title":"Create a JSON Schema object — create_json_schema","text":"Create JSON Schema object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/create_json_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a JSON Schema object — create_json_schema","text":"","code":"create_json_schema(name, schema, strict = TRUE, description = \"\")"},{"path":"https://jpcompartir.github.io/EndpointR/reference/create_json_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a JSON Schema object — create_json_schema","text":"name Name schema schema JSON schema definition list strict Whether enforce strict mode (default TRUE) description Optional description schema","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/create_json_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a JSON Schema object — create_json_schema","text":"json_schema S7 object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_embeddings_hf.html","id":null,"dir":"Reference","previous_headings":"","what":"Example embedding results from Hugging Face API — df_embeddings_hf","title":"Example embedding results from Hugging Face API — df_embeddings_hf","text":"sample dataset containing text embeddings generated using Hugging Face's embedding API. dataset demonstrates structure results returned hf_embed_batch() hf_embed_df() functions.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_embeddings_hf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example embedding results from Hugging Face API — df_embeddings_hf","text":"","code":"df_embeddings_hf"},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_embeddings_hf.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example embedding results from Hugging Face API — df_embeddings_hf","text":"data frame 3 rows 773 variables: id Integer; unique identifier text text Character; original text embedded category Character; category classification text .error Logical; whether embedding process failed .error_message Character; error message embedding failed (NA successful) V1 Numeric; embedding vector dimensions V2 Numeric; embedding vector dimensions V3 Numeric; embedding vector dimensions V4 Numeric; embedding vector dimensions V5 Numeric; embedding vector dimensions V6 Numeric; embedding vector dimensions V7 Numeric; embedding vector dimensions V8 Numeric; embedding vector dimensions V9 Numeric; embedding vector dimensions V10 Numeric; embedding vector dimensions V11 Numeric; embedding vector dimensions V12 Numeric; embedding vector dimensions V13 Numeric; embedding vector dimensions V14 Numeric; embedding vector dimensions V15 Numeric; embedding vector dimensions V16 Numeric; embedding vector dimensions V17 Numeric; embedding vector dimensions V18 Numeric; embedding vector dimensions V19 Numeric; embedding vector dimensions V20 Numeric; embedding vector dimensions V21 Numeric; embedding vector dimensions V22 Numeric; embedding vector dimensions V23 Numeric; embedding vector dimensions V24 Numeric; embedding vector dimensions V25 Numeric; embedding vector dimensions V26 Numeric; embedding vector dimensions V27 Numeric; embedding vector dimensions V28 Numeric; embedding vector dimensions V29 Numeric; embedding vector dimensions V30 Numeric; embedding vector dimensions V31 Numeric; embedding vector dimensions V32 Numeric; embedding vector dimensions V33 Numeric; embedding vector dimensions V34 Numeric; embedding vector dimensions V35 Numeric; embedding vector dimensions V36 Numeric; embedding vector dimensions V37 Numeric; embedding vector dimensions V38 Numeric; embedding vector dimensions V39 Numeric; embedding vector dimensions V40 Numeric; embedding vector dimensions V41 Numeric; embedding vector dimensions V42 Numeric; embedding vector dimensions V43 Numeric; embedding vector dimensions V44 Numeric; embedding vector dimensions V45 Numeric; embedding vector dimensions V46 Numeric; embedding vector dimensions V47 Numeric; embedding vector dimensions V48 Numeric; embedding vector dimensions V49 Numeric; embedding vector dimensions V50 Numeric; embedding vector dimensions V51 Numeric; embedding vector dimensions V52 Numeric; embedding vector dimensions V53 Numeric; embedding vector dimensions V54 Numeric; embedding vector dimensions V55 Numeric; embedding vector dimensions V56 Numeric; embedding vector dimensions V57 Numeric; embedding vector dimensions V58 Numeric; embedding vector dimensions V59 Numeric; embedding vector dimensions V60 Numeric; embedding vector dimensions V61 Numeric; embedding vector dimensions V62 Numeric; embedding vector dimensions V63 Numeric; embedding vector dimensions V64 Numeric; embedding vector dimensions V65 Numeric; embedding vector dimensions V66 Numeric; embedding vector dimensions V67 Numeric; embedding vector dimensions V68 Numeric; embedding vector dimensions V69 Numeric; embedding vector dimensions V70 Numeric; embedding vector dimensions V71 Numeric; embedding vector dimensions V72 Numeric; embedding vector dimensions V73 Numeric; embedding vector dimensions V74 Numeric; embedding vector dimensions V75 Numeric; embedding vector dimensions V76 Numeric; embedding vector dimensions V77 Numeric; embedding vector dimensions V78 Numeric; embedding vector dimensions V79 Numeric; embedding vector dimensions V80 Numeric; embedding vector dimensions V81 Numeric; embedding vector dimensions V82 Numeric; embedding vector dimensions V83 Numeric; embedding vector dimensions V84 Numeric; embedding vector dimensions V85 Numeric; embedding vector dimensions V86 Numeric; embedding vector dimensions V87 Numeric; embedding vector dimensions V88 Numeric; embedding vector dimensions V89 Numeric; embedding vector dimensions V90 Numeric; embedding vector dimensions V91 Numeric; embedding vector dimensions V92 Numeric; embedding vector dimensions V93 Numeric; embedding vector dimensions V94 Numeric; embedding vector dimensions V95 Numeric; embedding vector dimensions V96 Numeric; embedding vector dimensions V97 Numeric; embedding vector dimensions V98 Numeric; embedding vector dimensions V99 Numeric; embedding vector dimensions V100 Numeric; embedding vector dimensions V101 Numeric; embedding vector dimensions V102 Numeric; embedding vector dimensions V103 Numeric; embedding vector dimensions V104 Numeric; embedding vector dimensions V105 Numeric; embedding vector dimensions V106 Numeric; embedding vector dimensions V107 Numeric; embedding vector dimensions V108 Numeric; embedding vector dimensions V109 Numeric; embedding vector dimensions V110 Numeric; embedding vector dimensions V111 Numeric; embedding vector dimensions V112 Numeric; embedding vector dimensions V113 Numeric; embedding vector dimensions V114 Numeric; embedding vector dimensions V115 Numeric; embedding vector dimensions V116 Numeric; embedding vector dimensions V117 Numeric; embedding vector dimensions V118 Numeric; embedding vector dimensions V119 Numeric; embedding vector dimensions V120 Numeric; embedding vector dimensions V121 Numeric; embedding vector dimensions V122 Numeric; embedding vector dimensions V123 Numeric; embedding vector dimensions V124 Numeric; embedding vector dimensions V125 Numeric; embedding vector dimensions V126 Numeric; embedding vector dimensions V127 Numeric; embedding vector dimensions V128 Numeric; embedding vector dimensions V129 Numeric; embedding vector dimensions V130 Numeric; embedding vector dimensions V131 Numeric; embedding vector dimensions V132 Numeric; embedding vector dimensions V133 Numeric; embedding vector dimensions V134 Numeric; embedding vector dimensions V135 Numeric; embedding vector dimensions V136 Numeric; embedding vector dimensions V137 Numeric; embedding vector dimensions V138 Numeric; embedding vector dimensions V139 Numeric; embedding vector dimensions V140 Numeric; embedding vector dimensions V141 Numeric; embedding vector dimensions V142 Numeric; embedding vector dimensions V143 Numeric; embedding vector dimensions V144 Numeric; embedding vector dimensions V145 Numeric; embedding vector dimensions V146 Numeric; embedding vector dimensions V147 Numeric; embedding vector dimensions V148 Numeric; embedding vector dimensions V149 Numeric; embedding vector dimensions V150 Numeric; embedding vector dimensions V151 Numeric; embedding vector dimensions V152 Numeric; embedding vector dimensions V153 Numeric; embedding vector dimensions V154 Numeric; embedding vector dimensions V155 Numeric; embedding vector dimensions V156 Numeric; embedding vector dimensions V157 Numeric; embedding vector dimensions V158 Numeric; embedding vector dimensions V159 Numeric; embedding vector dimensions V160 Numeric; embedding vector dimensions V161 Numeric; embedding vector dimensions V162 Numeric; embedding vector dimensions V163 Numeric; embedding vector dimensions V164 Numeric; embedding vector dimensions V165 Numeric; embedding vector dimensions V166 Numeric; embedding vector dimensions V167 Numeric; embedding vector dimensions V168 Numeric; embedding vector dimensions V169 Numeric; embedding vector dimensions V170 Numeric; embedding vector dimensions V171 Numeric; embedding vector dimensions V172 Numeric; embedding vector dimensions V173 Numeric; embedding vector dimensions V174 Numeric; embedding vector dimensions V175 Numeric; embedding vector dimensions V176 Numeric; embedding vector dimensions V177 Numeric; embedding vector dimensions V178 Numeric; embedding vector dimensions V179 Numeric; embedding vector dimensions V180 Numeric; embedding vector dimensions V181 Numeric; embedding vector dimensions V182 Numeric; embedding vector dimensions V183 Numeric; embedding vector dimensions V184 Numeric; embedding vector dimensions V185 Numeric; embedding vector dimensions V186 Numeric; embedding vector dimensions V187 Numeric; embedding vector dimensions V188 Numeric; embedding vector dimensions V189 Numeric; embedding vector dimensions V190 Numeric; embedding vector dimensions V191 Numeric; embedding vector dimensions V192 Numeric; embedding vector dimensions V193 Numeric; embedding vector dimensions V194 Numeric; embedding vector dimensions V195 Numeric; embedding vector dimensions V196 Numeric; embedding vector dimensions V197 Numeric; embedding vector dimensions V198 Numeric; embedding vector dimensions V199 Numeric; embedding vector dimensions V200 Numeric; embedding vector dimensions V201 Numeric; embedding vector dimensions V202 Numeric; embedding vector dimensions V203 Numeric; embedding vector dimensions V204 Numeric; embedding vector dimensions V205 Numeric; embedding vector dimensions V206 Numeric; embedding vector dimensions V207 Numeric; embedding vector dimensions V208 Numeric; embedding vector dimensions V209 Numeric; embedding vector dimensions V210 Numeric; embedding vector dimensions V211 Numeric; embedding vector dimensions V212 Numeric; embedding vector dimensions V213 Numeric; embedding vector dimensions V214 Numeric; embedding vector dimensions V215 Numeric; embedding vector dimensions V216 Numeric; embedding vector dimensions V217 Numeric; embedding vector dimensions V218 Numeric; embedding vector dimensions V219 Numeric; embedding vector dimensions V220 Numeric; embedding vector dimensions V221 Numeric; embedding vector dimensions V222 Numeric; embedding vector dimensions V223 Numeric; embedding vector dimensions V224 Numeric; embedding vector dimensions V225 Numeric; embedding vector dimensions V226 Numeric; embedding vector dimensions V227 Numeric; embedding vector dimensions V228 Numeric; embedding vector dimensions V229 Numeric; embedding vector dimensions V230 Numeric; embedding vector dimensions V231 Numeric; embedding vector dimensions V232 Numeric; embedding vector dimensions V233 Numeric; embedding vector dimensions V234 Numeric; embedding vector dimensions V235 Numeric; embedding vector dimensions V236 Numeric; embedding vector dimensions V237 Numeric; embedding vector dimensions V238 Numeric; embedding vector dimensions V239 Numeric; embedding vector dimensions V240 Numeric; embedding vector dimensions V241 Numeric; embedding vector dimensions V242 Numeric; embedding vector dimensions V243 Numeric; embedding vector dimensions V244 Numeric; embedding vector dimensions V245 Numeric; embedding vector dimensions V246 Numeric; embedding vector dimensions V247 Numeric; embedding vector dimensions V248 Numeric; embedding vector dimensions V249 Numeric; embedding vector dimensions V250 Numeric; embedding vector dimensions V251 Numeric; embedding vector dimensions V252 Numeric; embedding vector dimensions V253 Numeric; embedding vector dimensions V254 Numeric; embedding vector dimensions V255 Numeric; embedding vector dimensions V256 Numeric; embedding vector dimensions V257 Numeric; embedding vector dimensions V258 Numeric; embedding vector dimensions V259 Numeric; embedding vector dimensions V260 Numeric; embedding vector dimensions V261 Numeric; embedding vector dimensions V262 Numeric; embedding vector dimensions V263 Numeric; embedding vector dimensions V264 Numeric; embedding vector dimensions V265 Numeric; embedding vector dimensions V266 Numeric; embedding vector dimensions V267 Numeric; embedding vector dimensions V268 Numeric; embedding vector dimensions V269 Numeric; embedding vector dimensions V270 Numeric; embedding vector dimensions V271 Numeric; embedding vector dimensions V272 Numeric; embedding vector dimensions V273 Numeric; embedding vector dimensions V274 Numeric; embedding vector dimensions V275 Numeric; embedding vector dimensions V276 Numeric; embedding vector dimensions V277 Numeric; embedding vector dimensions V278 Numeric; embedding vector dimensions V279 Numeric; embedding vector dimensions V280 Numeric; embedding vector dimensions V281 Numeric; embedding vector dimensions V282 Numeric; embedding vector dimensions V283 Numeric; embedding vector dimensions V284 Numeric; embedding vector dimensions V285 Numeric; embedding vector dimensions V286 Numeric; embedding vector dimensions V287 Numeric; embedding vector dimensions V288 Numeric; embedding vector dimensions V289 Numeric; embedding vector dimensions V290 Numeric; embedding vector dimensions V291 Numeric; embedding vector dimensions V292 Numeric; embedding vector dimensions V293 Numeric; embedding vector dimensions V294 Numeric; embedding vector dimensions V295 Numeric; embedding vector dimensions V296 Numeric; embedding vector dimensions V297 Numeric; embedding vector dimensions V298 Numeric; embedding vector dimensions V299 Numeric; embedding vector dimensions V300 Numeric; embedding vector dimensions V301 Numeric; embedding vector dimensions V302 Numeric; embedding vector dimensions V303 Numeric; embedding vector dimensions V304 Numeric; embedding vector dimensions V305 Numeric; embedding vector dimensions V306 Numeric; embedding vector dimensions V307 Numeric; embedding vector dimensions V308 Numeric; embedding vector dimensions V309 Numeric; embedding vector dimensions V310 Numeric; embedding vector dimensions V311 Numeric; embedding vector dimensions V312 Numeric; embedding vector dimensions V313 Numeric; embedding vector dimensions V314 Numeric; embedding vector dimensions V315 Numeric; embedding vector dimensions V316 Numeric; embedding vector dimensions V317 Numeric; embedding vector dimensions V318 Numeric; embedding vector dimensions V319 Numeric; embedding vector dimensions V320 Numeric; embedding vector dimensions V321 Numeric; embedding vector dimensions V322 Numeric; embedding vector dimensions V323 Numeric; embedding vector dimensions V324 Numeric; embedding vector dimensions V325 Numeric; embedding vector dimensions V326 Numeric; embedding vector dimensions V327 Numeric; embedding vector dimensions V328 Numeric; embedding vector dimensions V329 Numeric; embedding vector dimensions V330 Numeric; embedding vector dimensions V331 Numeric; embedding vector dimensions V332 Numeric; embedding vector dimensions V333 Numeric; embedding vector dimensions V334 Numeric; embedding vector dimensions V335 Numeric; embedding vector dimensions V336 Numeric; embedding vector dimensions V337 Numeric; embedding vector dimensions V338 Numeric; embedding vector dimensions V339 Numeric; embedding vector dimensions V340 Numeric; embedding vector dimensions V341 Numeric; embedding vector dimensions V342 Numeric; embedding vector dimensions V343 Numeric; embedding vector dimensions V344 Numeric; embedding vector dimensions V345 Numeric; embedding vector dimensions V346 Numeric; embedding vector dimensions V347 Numeric; embedding vector dimensions V348 Numeric; embedding vector dimensions V349 Numeric; embedding vector dimensions V350 Numeric; embedding vector dimensions V351 Numeric; embedding vector dimensions V352 Numeric; embedding vector dimensions V353 Numeric; embedding vector dimensions V354 Numeric; embedding vector dimensions V355 Numeric; embedding vector dimensions V356 Numeric; embedding vector dimensions V357 Numeric; embedding vector dimensions V358 Numeric; embedding vector dimensions V359 Numeric; embedding vector dimensions V360 Numeric; embedding vector dimensions V361 Numeric; embedding vector dimensions V362 Numeric; embedding vector dimensions V363 Numeric; embedding vector dimensions V364 Numeric; embedding vector dimensions V365 Numeric; embedding vector dimensions V366 Numeric; embedding vector dimensions V367 Numeric; embedding vector dimensions V368 Numeric; embedding vector dimensions V369 Numeric; embedding vector dimensions V370 Numeric; embedding vector dimensions V371 Numeric; embedding vector dimensions V372 Numeric; embedding vector dimensions V373 Numeric; embedding vector dimensions V374 Numeric; embedding vector dimensions V375 Numeric; embedding vector dimensions V376 Numeric; embedding vector dimensions V377 Numeric; embedding vector dimensions V378 Numeric; embedding vector dimensions V379 Numeric; embedding vector dimensions V380 Numeric; embedding vector dimensions V381 Numeric; embedding vector dimensions V382 Numeric; embedding vector dimensions V383 Numeric; embedding vector dimensions V384 Numeric; embedding vector dimensions V385 Numeric; embedding vector dimensions V386 Numeric; embedding vector dimensions V387 Numeric; embedding vector dimensions V388 Numeric; embedding vector dimensions V389 Numeric; embedding vector dimensions V390 Numeric; embedding vector dimensions V391 Numeric; embedding vector dimensions V392 Numeric; embedding vector dimensions V393 Numeric; embedding vector dimensions V394 Numeric; embedding vector dimensions V395 Numeric; embedding vector dimensions V396 Numeric; embedding vector dimensions V397 Numeric; embedding vector dimensions V398 Numeric; embedding vector dimensions V399 Numeric; embedding vector dimensions V400 Numeric; embedding vector dimensions V401 Numeric; embedding vector dimensions V402 Numeric; embedding vector dimensions V403 Numeric; embedding vector dimensions V404 Numeric; embedding vector dimensions V405 Numeric; embedding vector dimensions V406 Numeric; embedding vector dimensions V407 Numeric; embedding vector dimensions V408 Numeric; embedding vector dimensions V409 Numeric; embedding vector dimensions V410 Numeric; embedding vector dimensions V411 Numeric; embedding vector dimensions V412 Numeric; embedding vector dimensions V413 Numeric; embedding vector dimensions V414 Numeric; embedding vector dimensions V415 Numeric; embedding vector dimensions V416 Numeric; embedding vector dimensions V417 Numeric; embedding vector dimensions V418 Numeric; embedding vector dimensions V419 Numeric; embedding vector dimensions V420 Numeric; embedding vector dimensions V421 Numeric; embedding vector dimensions V422 Numeric; embedding vector dimensions V423 Numeric; embedding vector dimensions V424 Numeric; embedding vector dimensions V425 Numeric; embedding vector dimensions V426 Numeric; embedding vector dimensions V427 Numeric; embedding vector dimensions V428 Numeric; embedding vector dimensions V429 Numeric; embedding vector dimensions V430 Numeric; embedding vector dimensions V431 Numeric; embedding vector dimensions V432 Numeric; embedding vector dimensions V433 Numeric; embedding vector dimensions V434 Numeric; embedding vector dimensions V435 Numeric; embedding vector dimensions V436 Numeric; embedding vector dimensions V437 Numeric; embedding vector dimensions V438 Numeric; embedding vector dimensions V439 Numeric; embedding vector dimensions V440 Numeric; embedding vector dimensions V441 Numeric; embedding vector dimensions V442 Numeric; embedding vector dimensions V443 Numeric; embedding vector dimensions V444 Numeric; embedding vector dimensions V445 Numeric; embedding vector dimensions V446 Numeric; embedding vector dimensions V447 Numeric; embedding vector dimensions V448 Numeric; embedding vector dimensions V449 Numeric; embedding vector dimensions V450 Numeric; embedding vector dimensions V451 Numeric; embedding vector dimensions V452 Numeric; embedding vector dimensions V453 Numeric; embedding vector dimensions V454 Numeric; embedding vector dimensions V455 Numeric; embedding vector dimensions V456 Numeric; embedding vector dimensions V457 Numeric; embedding vector dimensions V458 Numeric; embedding vector dimensions V459 Numeric; embedding vector dimensions V460 Numeric; embedding vector dimensions V461 Numeric; embedding vector dimensions V462 Numeric; embedding vector dimensions V463 Numeric; embedding vector dimensions V464 Numeric; embedding vector dimensions V465 Numeric; embedding vector dimensions V466 Numeric; embedding vector dimensions V467 Numeric; embedding vector dimensions V468 Numeric; embedding vector dimensions V469 Numeric; embedding vector dimensions V470 Numeric; embedding vector dimensions V471 Numeric; embedding vector dimensions V472 Numeric; embedding vector dimensions V473 Numeric; embedding vector dimensions V474 Numeric; embedding vector dimensions V475 Numeric; embedding vector dimensions V476 Numeric; embedding vector dimensions V477 Numeric; embedding vector dimensions V478 Numeric; embedding vector dimensions V479 Numeric; embedding vector dimensions V480 Numeric; embedding vector dimensions V481 Numeric; embedding vector dimensions V482 Numeric; embedding vector dimensions V483 Numeric; embedding vector dimensions V484 Numeric; embedding vector dimensions V485 Numeric; embedding vector dimensions V486 Numeric; embedding vector dimensions V487 Numeric; embedding vector dimensions V488 Numeric; embedding vector dimensions V489 Numeric; embedding vector dimensions V490 Numeric; embedding vector dimensions V491 Numeric; embedding vector dimensions V492 Numeric; embedding vector dimensions V493 Numeric; embedding vector dimensions V494 Numeric; embedding vector dimensions V495 Numeric; embedding vector dimensions V496 Numeric; embedding vector dimensions V497 Numeric; embedding vector dimensions V498 Numeric; embedding vector dimensions V499 Numeric; embedding vector dimensions V500 Numeric; embedding vector dimensions V501 Numeric; embedding vector dimensions V502 Numeric; embedding vector dimensions V503 Numeric; embedding vector dimensions V504 Numeric; embedding vector dimensions V505 Numeric; embedding vector dimensions V506 Numeric; embedding vector dimensions V507 Numeric; embedding vector dimensions V508 Numeric; embedding vector dimensions V509 Numeric; embedding vector dimensions V510 Numeric; embedding vector dimensions V511 Numeric; embedding vector dimensions V512 Numeric; embedding vector dimensions V513 Numeric; embedding vector dimensions V514 Numeric; embedding vector dimensions V515 Numeric; embedding vector dimensions V516 Numeric; embedding vector dimensions V517 Numeric; embedding vector dimensions V518 Numeric; embedding vector dimensions V519 Numeric; embedding vector dimensions V520 Numeric; embedding vector dimensions V521 Numeric; embedding vector dimensions V522 Numeric; embedding vector dimensions V523 Numeric; embedding vector dimensions V524 Numeric; embedding vector dimensions V525 Numeric; embedding vector dimensions V526 Numeric; embedding vector dimensions V527 Numeric; embedding vector dimensions V528 Numeric; embedding vector dimensions V529 Numeric; embedding vector dimensions V530 Numeric; embedding vector dimensions V531 Numeric; embedding vector dimensions V532 Numeric; embedding vector dimensions V533 Numeric; embedding vector dimensions V534 Numeric; embedding vector dimensions V535 Numeric; embedding vector dimensions V536 Numeric; embedding vector dimensions V537 Numeric; embedding vector dimensions V538 Numeric; embedding vector dimensions V539 Numeric; embedding vector dimensions V540 Numeric; embedding vector dimensions V541 Numeric; embedding vector dimensions V542 Numeric; embedding vector dimensions V543 Numeric; embedding vector dimensions V544 Numeric; embedding vector dimensions V545 Numeric; embedding vector dimensions V546 Numeric; embedding vector dimensions V547 Numeric; embedding vector dimensions V548 Numeric; embedding vector dimensions V549 Numeric; embedding vector dimensions V550 Numeric; embedding vector dimensions V551 Numeric; embedding vector dimensions V552 Numeric; embedding vector dimensions V553 Numeric; embedding vector dimensions V554 Numeric; embedding vector dimensions V555 Numeric; embedding vector dimensions V556 Numeric; embedding vector dimensions V557 Numeric; embedding vector dimensions V558 Numeric; embedding vector dimensions V559 Numeric; embedding vector dimensions V560 Numeric; embedding vector dimensions V561 Numeric; embedding vector dimensions V562 Numeric; embedding vector dimensions V563 Numeric; embedding vector dimensions V564 Numeric; embedding vector dimensions V565 Numeric; embedding vector dimensions V566 Numeric; embedding vector dimensions V567 Numeric; embedding vector dimensions V568 Numeric; embedding vector dimensions V569 Numeric; embedding vector dimensions V570 Numeric; embedding vector dimensions V571 Numeric; embedding vector dimensions V572 Numeric; embedding vector dimensions V573 Numeric; embedding vector dimensions V574 Numeric; embedding vector dimensions V575 Numeric; embedding vector dimensions V576 Numeric; embedding vector dimensions V577 Numeric; embedding vector dimensions V578 Numeric; embedding vector dimensions V579 Numeric; embedding vector dimensions V580 Numeric; embedding vector dimensions V581 Numeric; embedding vector dimensions V582 Numeric; embedding vector dimensions V583 Numeric; embedding vector dimensions V584 Numeric; embedding vector dimensions V585 Numeric; embedding vector dimensions V586 Numeric; embedding vector dimensions V587 Numeric; embedding vector dimensions V588 Numeric; embedding vector dimensions V589 Numeric; embedding vector dimensions V590 Numeric; embedding vector dimensions V591 Numeric; embedding vector dimensions V592 Numeric; embedding vector dimensions V593 Numeric; embedding vector dimensions V594 Numeric; embedding vector dimensions V595 Numeric; embedding vector dimensions V596 Numeric; embedding vector dimensions V597 Numeric; embedding vector dimensions V598 Numeric; embedding vector dimensions V599 Numeric; embedding vector dimensions V600 Numeric; embedding vector dimensions V601 Numeric; embedding vector dimensions V602 Numeric; embedding vector dimensions V603 Numeric; embedding vector dimensions V604 Numeric; embedding vector dimensions V605 Numeric; embedding vector dimensions V606 Numeric; embedding vector dimensions V607 Numeric; embedding vector dimensions V608 Numeric; embedding vector dimensions V609 Numeric; embedding vector dimensions V610 Numeric; embedding vector dimensions V611 Numeric; embedding vector dimensions V612 Numeric; embedding vector dimensions V613 Numeric; embedding vector dimensions V614 Numeric; embedding vector dimensions V615 Numeric; embedding vector dimensions V616 Numeric; embedding vector dimensions V617 Numeric; embedding vector dimensions V618 Numeric; embedding vector dimensions V619 Numeric; embedding vector dimensions V620 Numeric; embedding vector dimensions V621 Numeric; embedding vector dimensions V622 Numeric; embedding vector dimensions V623 Numeric; embedding vector dimensions V624 Numeric; embedding vector dimensions V625 Numeric; embedding vector dimensions V626 Numeric; embedding vector dimensions V627 Numeric; embedding vector dimensions V628 Numeric; embedding vector dimensions V629 Numeric; embedding vector dimensions V630 Numeric; embedding vector dimensions V631 Numeric; embedding vector dimensions V632 Numeric; embedding vector dimensions V633 Numeric; embedding vector dimensions V634 Numeric; embedding vector dimensions V635 Numeric; embedding vector dimensions V636 Numeric; embedding vector dimensions V637 Numeric; embedding vector dimensions V638 Numeric; embedding vector dimensions V639 Numeric; embedding vector dimensions V640 Numeric; embedding vector dimensions V641 Numeric; embedding vector dimensions V642 Numeric; embedding vector dimensions V643 Numeric; embedding vector dimensions V644 Numeric; embedding vector dimensions V645 Numeric; embedding vector dimensions V646 Numeric; embedding vector dimensions V647 Numeric; embedding vector dimensions V648 Numeric; embedding vector dimensions V649 Numeric; embedding vector dimensions V650 Numeric; embedding vector dimensions V651 Numeric; embedding vector dimensions V652 Numeric; embedding vector dimensions V653 Numeric; embedding vector dimensions V654 Numeric; embedding vector dimensions V655 Numeric; embedding vector dimensions V656 Numeric; embedding vector dimensions V657 Numeric; embedding vector dimensions V658 Numeric; embedding vector dimensions V659 Numeric; embedding vector dimensions V660 Numeric; embedding vector dimensions V661 Numeric; embedding vector dimensions V662 Numeric; embedding vector dimensions V663 Numeric; embedding vector dimensions V664 Numeric; embedding vector dimensions V665 Numeric; embedding vector dimensions V666 Numeric; embedding vector dimensions V667 Numeric; embedding vector dimensions V668 Numeric; embedding vector dimensions V669 Numeric; embedding vector dimensions V670 Numeric; embedding vector dimensions V671 Numeric; embedding vector dimensions V672 Numeric; embedding vector dimensions V673 Numeric; embedding vector dimensions V674 Numeric; embedding vector dimensions V675 Numeric; embedding vector dimensions V676 Numeric; embedding vector dimensions V677 Numeric; embedding vector dimensions V678 Numeric; embedding vector dimensions V679 Numeric; embedding vector dimensions V680 Numeric; embedding vector dimensions V681 Numeric; embedding vector dimensions V682 Numeric; embedding vector dimensions V683 Numeric; embedding vector dimensions V684 Numeric; embedding vector dimensions V685 Numeric; embedding vector dimensions V686 Numeric; embedding vector dimensions V687 Numeric; embedding vector dimensions V688 Numeric; embedding vector dimensions V689 Numeric; embedding vector dimensions V690 Numeric; embedding vector dimensions V691 Numeric; embedding vector dimensions V692 Numeric; embedding vector dimensions V693 Numeric; embedding vector dimensions V694 Numeric; embedding vector dimensions V695 Numeric; embedding vector dimensions V696 Numeric; embedding vector dimensions V697 Numeric; embedding vector dimensions V698 Numeric; embedding vector dimensions V699 Numeric; embedding vector dimensions V700 Numeric; embedding vector dimensions V701 Numeric; embedding vector dimensions V702 Numeric; embedding vector dimensions V703 Numeric; embedding vector dimensions V704 Numeric; embedding vector dimensions V705 Numeric; embedding vector dimensions V706 Numeric; embedding vector dimensions V707 Numeric; embedding vector dimensions V708 Numeric; embedding vector dimensions V709 Numeric; embedding vector dimensions V710 Numeric; embedding vector dimensions V711 Numeric; embedding vector dimensions V712 Numeric; embedding vector dimensions V713 Numeric; embedding vector dimensions V714 Numeric; embedding vector dimensions V715 Numeric; embedding vector dimensions V716 Numeric; embedding vector dimensions V717 Numeric; embedding vector dimensions V718 Numeric; embedding vector dimensions V719 Numeric; embedding vector dimensions V720 Numeric; embedding vector dimensions V721 Numeric; embedding vector dimensions V722 Numeric; embedding vector dimensions V723 Numeric; embedding vector dimensions V724 Numeric; embedding vector dimensions V725 Numeric; embedding vector dimensions V726 Numeric; embedding vector dimensions V727 Numeric; embedding vector dimensions V728 Numeric; embedding vector dimensions V729 Numeric; embedding vector dimensions V730 Numeric; embedding vector dimensions V731 Numeric; embedding vector dimensions V732 Numeric; embedding vector dimensions V733 Numeric; embedding vector dimensions V734 Numeric; embedding vector dimensions V735 Numeric; embedding vector dimensions V736 Numeric; embedding vector dimensions V737 Numeric; embedding vector dimensions V738 Numeric; embedding vector dimensions V739 Numeric; embedding vector dimensions V740 Numeric; embedding vector dimensions V741 Numeric; embedding vector dimensions V742 Numeric; embedding vector dimensions V743 Numeric; embedding vector dimensions V744 Numeric; embedding vector dimensions V745 Numeric; embedding vector dimensions V746 Numeric; embedding vector dimensions V747 Numeric; embedding vector dimensions V748 Numeric; embedding vector dimensions V749 Numeric; embedding vector dimensions V750 Numeric; embedding vector dimensions V751 Numeric; embedding vector dimensions V752 Numeric; embedding vector dimensions V753 Numeric; embedding vector dimensions V754 Numeric; embedding vector dimensions V755 Numeric; embedding vector dimensions V756 Numeric; embedding vector dimensions V757 Numeric; embedding vector dimensions V758 Numeric; embedding vector dimensions V759 Numeric; embedding vector dimensions V760 Numeric; embedding vector dimensions V761 Numeric; embedding vector dimensions V762 Numeric; embedding vector dimensions V763 Numeric; embedding vector dimensions V764 Numeric; embedding vector dimensions V765 Numeric; embedding vector dimensions V766 Numeric; embedding vector dimensions V767 Numeric; embedding vector dimensions V768 Numeric; embedding vector dimensions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_embeddings_hf.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Example embedding results from Hugging Face API — df_embeddings_hf","text":"Generated using Hugging Face embedding model via EndpointR functions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_sentiment_classification_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Example sentiment classification results from Hugging Face API — df_sentiment_classification_example","title":"Example sentiment classification results from Hugging Face API — df_sentiment_classification_example","text":"sample dataset containing sentiment classification results generated using Hugging Face's classification API. dataset demonstrates structure results returned hf_classify_batch() hf_classify_df() functions.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_sentiment_classification_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example sentiment classification results from Hugging Face API — df_sentiment_classification_example","text":"","code":"df_sentiment_classification_example"},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_sentiment_classification_example.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example sentiment classification results from Hugging Face API — df_sentiment_classification_example","text":"data frame 3 rows 7 variables: id Integer; unique identifier text text Character; original text classified category Character; category classification text NEGATIVE Numeric; probability score negative sentiment (0-1) POSITIVE Numeric; probability score positive sentiment (0-1) .error Logical; whether classification process failed .error_message Character; error message classification failed (NA successful)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_sentiment_classification_example.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Example sentiment classification results from Hugging Face API — df_sentiment_classification_example","text":"Generated using Hugging Face sentiment classification model via EndpointR functions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/dot-create_error_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Create standardised error tibble for failed requests — .create_error_tibble","title":"Create standardised error tibble for failed requests — .create_error_tibble","text":"Internal function create consistent error tibble structure. Ensures uniform error reporting across different failure modes.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/dot-create_error_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create standardised error tibble for failed requests — .create_error_tibble","text":"","code":".create_error_tibble(indices, error_message)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/dot-create_error_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create standardised error tibble for failed requests — .create_error_tibble","text":"indices Vector indices indicating original request positions error_message Character string condition object describing error","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/dot-create_error_tibble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create standardised error tibble for failed requests — .create_error_tibble","text":"tibble columns: original_index: Position original request batch .error: Always TRUE error tibbles .error_message: Character description error","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/dot-extract_successful_completion_content.html","id":null,"dir":"Reference","previous_headings":"","what":"should only pass in successes here, as we're not going to validate types here or try to catch errors and this is marginally quicker/memory efficient than pipe + pluck — .extract_successful_completion_content","title":"should only pass in successes here, as we're not going to validate types here or try to catch errors and this is marginally quicker/memory efficient than pipe + pluck — .extract_successful_completion_content","text":"pass successes , going validate types try catch errors marginally quicker/memory efficient pipe + pluck","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/dot-extract_successful_completion_content.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"should only pass in successes here, as we're not going to validate types here or try to catch errors and this is marginally quicker/memory efficient than pipe + pluck — .extract_successful_completion_content","text":"","code":".extract_successful_completion_content(resp)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"Retrieve API key .Renviron. API key set via set_api_key()manually placed .Renviron file.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"","code":"get_api_key(key_name)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"key_name name API format \"ENDPOINT_API_KEY\" -> \"ANTHROPIC_API_KEY\"","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"character","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"","code":"if (FALSE) { # \\dontrun{   # retrieve an Anthropic API key   anthropic_key <- get_api_key(\"ANTHROPIC_API_KEY\")    # use the key in a function call   my_function(api_key = anthropic_key) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare a single text embedding request — hf_build_request","title":"Prepare a single text embedding request — hf_build_request","text":"Creates httr2 request object obtaining response Hugging Face Inference endpoint single text input. function can used multiple tasks, .e. embedding input, classifying input","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare a single text embedding request — hf_build_request","text":"","code":"hf_build_request(   input,   endpoint_url,   key_name,   parameters = list(),   max_retries = 5,   timeout = 10,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare a single text embedding request — hf_build_request","text":"input Character string get response endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key parameters Advanced usage: parameters pass API endpoint max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating request","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare a single text embedding request — hf_build_request","text":"httr2 request object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare a single text embedding request — hf_build_request","text":"developers, function can form basis single requests, mapped list requests.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare a single text embedding request — hf_build_request","text":"","code":"if (FALSE) { # \\dontrun{   # Create request using API key from environment   req <- hf_build_request(     input = \"This is a sample text to embed\",     endpoint_url = \"https://my-endpoint.huggingface.cloud/embedding_api\",     key_name = \"HF_API_KEY\"   )    # Using default key name   req <- hf_build_request(     input = \"This is a sample text to classify\",     endpoint_url = \"https://my-endpoint.huggingface.cloud/classification_api\"   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare a batch request for multiple texts — hf_build_request_batch","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"Creates httr2 request object obtaining response Hugging Face Inference endpoint multiple text inputs single batch. function can used various tasks, embedding classifying multiple inputs simultaneously.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"","code":"hf_build_request_batch(   inputs,   parameters = list(),   endpoint_url,   key_name,   max_retries = 5,   timeout = 10,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"inputs Vector list character strings process batch parameters Parameters send inputs endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating request","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"httr2 request object configured batch processing","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"developers, function forms basis batch requests, enabling efficient processing multiple inputs single API call.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"","code":"if (FALSE) { # \\dontrun{   # Create batch request using API key from environment   batch_req <- hf_build_request_batch(     inputs = c(\"First text to embed\", \"Second text to embed\"),     endpoint_url = \"https://my-endpoint.huggingface.cloud/embedding_api\",     key_name = \"HF_API_KEY\"   )    # Using custom timeout and retry settings   batch_req <- hf_build_request_batch(     inputs = c(\"Text one\", \"Text two\", \"Text three\"),     endpoint_url = \"https://my-endpoint.huggingface.cloud/embedding_api\",     key_name = \"HF_API_KEY\",     max_retries = 3,     timeout = 15   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare embedding requests for texts in a data frame — hf_build_request_df","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"Creates httr2 request objects text data frame column. Thus function handles request creation, handle performing request, tidying response. perform request, select appropriate *_perform_* function.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"","code":"hf_build_request_df(   df,   text_var,   id_var,   endpoint_url,   key_name,   parameters = list(),   max_retries = 3,   timeout = 10,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"df data frame containing texts embed text_var Name column containing text send endpoint id_var Name column use ID (optional) endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key parameters Parameters send inputs max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating requests","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"data frame original data plus request objects","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"","code":"if (FALSE) { # \\dontrun{   # Prepare requests for a data frame   df <- data.frame(     id = 1:3,     text = c(\"First example\", \"Second example\", \"Third example\")   )    requests_df <- hf_build_request_df(     df = df,     text_var = text,     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     id_var = id   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"Classifies batch texts using Hugging Face classification endpoint returns classification scores tidy format. Handles batching, concurrent requests, error recovery automatically.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"","code":"hf_classify_batch(   texts,   endpoint_url,   key_name,   ...,   tidy_func = tidy_batch_classification_response,   parameters = list(return_all_scores = TRUE),   batch_size = 8,   progress = TRUE,   concurrent_requests = 5,   max_retries = 5,   timeout = 30,   include_texts = TRUE,   relocate_col = 2 )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"texts Character vector texts classify endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... Additional arguments passed request functions tidy_func Function process API responses, defaults tidy_batch_classification_response parameters List parameters API endpoint, defaults list(return_all_scores = TRUE) batch_size Integer; number texts per batch (default: 8) progress Logical; whether show progress bar (default: TRUE) concurrent_requests Integer; number concurrent requests (default: 5) max_retries Integer; maximum retry attempts (default: 5) timeout Numeric; request timeout seconds (default: 20) include_texts Logical; whether include original texts output (default: TRUE) relocate_col Integer; column position text column (default: 2)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"Data frame classification scores text, plus columns original text (include_texts=TRUE), error status, error messages","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"function processes multiple texts efficiently splitting batches optionally sending concurrent requests. includes robust error handling progress reporting large batches. function automatically handles request failures retries includes error information output requests fail. Original text order preserved results. function currently handle list(return_all_scores = FALSE).","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"","code":"if (FALSE) { # \\dontrun{   texts <- c(     \"This product is brilliant!\",     \"Terrible quality, waste of money\",     \"Average product, nothing special\"   )    results <- hf_classify_batch(     texts = texts,     endpoint_url = \"redacted\",     key_name = \"API_KEY\",     batch_size = 3   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"Classifies texts data frame column using Hugging Face classification endpoint joins results back original data frame.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"","code":"hf_classify_df(   df,   text_var,   id_var,   endpoint_url,   key_name,   ...,   tidy_func = tidy_batch_classification_response,   parameters = list(return_all_scores = TRUE),   batch_size = 4,   concurrent_requests = 1,   max_retries = 5,   timeout = 30,   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"df Data frame containing texts classify text_var Column name containing texts classify (unquoted) id_var Column name use identifier joining (unquoted) endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... Additional arguments passed request functions tidy_func Function process API responses, defaults tidy_batch_classification_response parameters List parameters API endpoint, defaults list(return_all_scores = TRUE) batch_size Integer; number texts per batch (default: 4) concurrent_requests Integer; number concurrent requests (default: 1) max_retries Integer; maximum retry attempts (default: 5) timeout Numeric; request timeout seconds (default: 30) progress Logical; whether show progress bar (default: TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"Original data frame additional columns classification scores, classification results table row counts match","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"function extracts texts specified column, classifies using hf_classify_batch(), joins classification results back original data frame using specified ID column. function preserves original data frame structure adds new columns classification scores. number rows match processing (due errors), returns classification results separately warning. function currently handle list(return_all_scores = FALSE).","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"","code":"if (FALSE) { # \\dontrun{   df <- data.frame(     id = 1:3,     review = c(\"Excellent service\", \"Poor quality\", \"Average experience\")   )    classified_df <- hf_classify_df(     df = df,     text_var = review,     id_var = id,     endpoint_url = \"redacted\",     key_name = \"API_KEY\"   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"Sends text Hugging Face classification endpoint returns classification scores. default, returns tidied data frame one row columns classification label.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"","code":"hf_classify_text(   text,   endpoint_url,   key_name,   ...,   parameters = list(return_all_scores = TRUE),   tidy = TRUE,   max_retries = 5,   timeout = 20,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"text Character string classify endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... Additional arguments passed hf_perform_request ultimately httr2::req_perform parameters Advanced usage: parameters pass API endpoint, defaults list(return_all_scores = TRUE). tidy Logical; TRUE (default), returns tidied data frame max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Logical; whether validate endpoint creating request","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"tidied data frame classification scores (tidy=TRUE) raw API response","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"function handles entire process creating request Hugging Face Inference API endpoint text classification, sending request, processing response. function automatically retry failed requests according max_retries parameter. tidy=TRUE (default), transforms nested JSON response tidy data frame one row columns classification label. tidying fails, function returns raw response informative message.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"","code":"if (FALSE) { # \\dontrun{   # Basic classification with default parameters   result <- hf_classify_text(     text = \"This product is excellent!\",     endpoint_url = \"redacted\",     key_name = \"API_KEY\"   )    # Classification with custom parameters for a spam detection model   spam_result <- hf_classify_text(     text = \"URGENT: You've won a free holiday! Call now to claim.\",     endpoint_url = \"redacted\",     parameters = list(return_all_scores = TRUE)   )    # Get raw response without tidying   raw_result <- hf_classify_text(     text = \"I love this movie\",     endpoint_url = \"redacted\",     key_name = \"API_KEY\",     tidy = FALSE   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate batches of embeddings for a list of texts — hf_embed_batch","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"High-level function generate embeddings multiple text strings. function handles batching parallel processing embedding requests, attempts handle errors gracefully.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"","code":"hf_embed_batch(   texts,   endpoint_url,   key_name,   ...,   tidy_func = tidy_embedding_response,   parameters = list(),   batch_size = 8,   include_texts = TRUE,   concurrent_requests = 5,   max_retries = 5,   timeout = 10,   validate = FALSE,   relocate_col = 2 )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"texts Vector list character strings get embeddings endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... ellipsis sent hf_perform_request TODO (reserved ATM) tidy_func Function process/tidy raw API response (default: tidy_embedding_response) parameters Advanced usage: parameters pass API endpoint. batch_size Number texts process one batch include_texts Whether return original texts return tibble concurrent_requests Number requests send simultaneously max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating request relocate_col position data frame relocate results .","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"tibble containing embedding vectors","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"","code":"if (FALSE) { # \\dontrun{   # Generate embeddings for multiple texts using default batch size   embeddings <- hf_embed_batch(     texts = c(\"First example\", \"Second example\", \"Third example\"),     endpoint_url = \"https://my-endpoint.huggingface.cloud\"   )    # With custom batch size and concurrent requests   embeddings <- hf_embed_batch(     texts = c(\"First example\", \"Second example\", \"Third example\"),     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     batch_size = 10,     concurrent_requests = 5   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate embeddings for texts in a data frame — hf_embed_df","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"High-level function generate embeddings texts data frame. function handles entire process request creation response processing, options batching & parallel execution. Setting number retries","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"","code":"hf_embed_df(   df,   text_var,   id_var,   endpoint_url,   key_name,   batch_size = 8,   concurrent_requests = 1,   max_retries = 5,   timeout = 15,   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"df data frame containing texts embed text_var Name column containing text embed id_var Name column use ID endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key batch_size Number texts process one batch (NULL batching) concurrent_requests Number requests send . APIs allow multiple requests. max_retries Maximum number retry attempts failed requests. timeout Request timeout seconds progress Whether display progress bar","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"data frame original data plus embedding columns","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"","code":"if (FALSE) { # \\dontrun{   # Generate embeddings for a data frame   df <- data.frame(     id = 1:3,     text = c(\"First example\", \"Second example\", \"Third example\")   )    # Use parallel processing without batching   embeddings_df <- hf_embed_df(     df = df,     text_var = text,     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     id_var = id,     parallel = TRUE,     batch_size = NULL   )    # Use batching without parallel processing   embeddings_df <- hf_embed_df(     df = df,     text_var = text,     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     id_var = id,     parallel = FALSE,     batch_size = 10   )    # Use both batching and parallel processing   embeddings_df <- hf_embed_df(     df = df,     text_var = text,     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     id_var = id,     parallel = TRUE,     batch_size = 10   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate embeddings for a single text — hf_embed_text","title":"Generate embeddings for a single text — hf_embed_text","text":"High-level function generate embeddings single text string. function handles entire process request creation response processing.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate embeddings for a single text — hf_embed_text","text":"","code":"hf_embed_text(   text,   endpoint_url,   key_name,   ...,   parameters = list(),   tidy = TRUE,   max_retries = 3,   timeout = 10,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate embeddings for a single text — hf_embed_text","text":"text Character string get embeddings endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... ellipsis sent hf_perform_request, forwards httr2::req_perform parameters Advanced usage: parameters pass API endpoint tidy Whether attempt tidy response max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating request","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate embeddings for a single text — hf_embed_text","text":"tibble containing embedding vectors","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate embeddings for a single text — hf_embed_text","text":"","code":"if (FALSE) { # \\dontrun{   # Generate embeddings using API key from environment   embeddings <- hf_embed_text(     text = \"This is a sample text to embed\",     endpoint_url = \"https://my-endpoint.huggingface.cloud\"   )    # With custom API key environment variable name   embeddings <- hf_embed_text(     text = \"This is a sample text to embed\",     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     key_name = \"MY_CUSTOM_API_KEY\"   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute a single embedding request and process the response — hf_perform_request","title":"Execute a single embedding request and process the response — hf_perform_request","text":"Performs prepared request returns response","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute a single embedding request and process the response — hf_perform_request","text":"","code":"hf_perform_request(request, ...)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute a single embedding request and process the response — hf_perform_request","text":"request httr2 request object created hf_build_request ... ellipsis sent httr2::req_perform, e.g. path verbosityarguments.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute a single embedding request and process the response — hf_perform_request","text":"httr2 response object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Execute a single embedding request and process the response — hf_perform_request","text":"","code":"if (FALSE) { # \\dontrun{   # Create and perform request   req <- hf_build_request(     input = \"This is a sample text to embed\",     endpoint_url = \"https://my-endpoint.huggingface.cloud\"   )   embeddings <- hf_perform_request(req)  } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/json_dump.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert json_schema to API format — json_dump","title":"Convert json_schema to API format — json_dump","text":"Convert json_schema API format","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/json_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create JSON Schema S7 class for structured outputs — json_schema","title":"Create JSON Schema S7 class for structured outputs — json_schema","text":"S7 class JSON Schema definitions used , e.g. OpenAI structured outputs","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/json_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create JSON Schema S7 class for structured outputs — json_schema","text":"","code":"json_schema(   name = character(0),   schema = list(),   strict = TRUE,   description = \"\" )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/json_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create JSON Schema S7 class for structured outputs — json_schema","text":"name Name schema schema JSON schema definition list strict Whether enforce strict mode (default TRUE) description Optional description schema","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Build an OpenAI API Chat Completions request — oai_build_completions_request","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"function constructs httr2 request object specifically tailored interacting OpenAI's Chat Completions API. handles formatting messages, model parameters, optional JSON schema structured responses.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"","code":"oai_build_completions_request(   input,   endpointr_id = NULL,   model = \"gpt-4.1-nano\",   temperature = 0,   max_tokens = 500L,   schema = NULL,   system_prompt = NULL,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\",   timeout = 20,   max_retries = 5 )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"input Text input send model endpointr_id id persist response model OpenAI model use (default: \"gpt-4.1-nano\") temperature Sampling temperature (0-2), higher values = randomness max_tokens Maximum tokens response schema Optional JSON schema structured output (json_schema object list) system_prompt Optional system prompt key_name Environment variable name API key endpoint_url OpenAI API endpoint URL timeout Request timeout seconds max_retries Maximum number retry attempts failed requests","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"httr2 request object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"function simplifies process making calls OpenAI Chat Completions API assembling request body according API's specifications. input system_prompt (provided) automatically structured required 'messages' array format API. structured outputs (JSON mode), need provide valid JSON schema via schema parameter. can pre-formatted list object class \"json_schema\". schema provided, function automatically set schema$additionalProperties <- FALSE ensure schema$strict <- TRUE (strict already defined schema) encourage predictable reliable structured outputs API. also good idea set temperature 0 extracting structured outputs.","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Build OpenAI requests for batch processing — oai_build_completions_request_list","title":"Build OpenAI requests for batch processing — oai_build_completions_request_list","text":"Build OpenAI requests batch processing","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build OpenAI requests for batch processing — oai_build_completions_request_list","text":"","code":"oai_build_completions_request_list(   inputs,   endpointr_ids = NULL,   model = \"gpt-4.1-nano\",   temperature = 0,   max_tokens = 500L,   schema = NULL,   system_prompt = NULL,   max_retries = 5L,   timeout = 30,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\" )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build OpenAI requests for batch processing — oai_build_completions_request_list","text":"inputs Character vector text inputs endpointr_ids vector IDs persist responses model OpenAI model use temperature Sampling temperature max_tokens Maximum tokens per response schema Optional JSON schema structured output system_prompt Optional system prompt max_retries Integer; maximum retry attempts (default: 5) timeout Numeric; request timeout seconds (default: 30) key_name Environment variable name API key endpoint_url OpenAI API endpoint URL","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build OpenAI requests for batch processing — oai_build_completions_request_list","text":"List httr2 request objects","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Build OpenAI embedding API request — oai_build_embedding_request","title":"Build OpenAI embedding API request — oai_build_embedding_request","text":"Creates httr2 request object configured OpenAI's embedding API. lower-level function handles request configuration including authentication, retries, timeouts.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build OpenAI embedding API request — oai_build_embedding_request","text":"","code":"oai_build_embedding_request(   input,   model = \"text-embedding-3-small\",   dimensions = NULL,   max_retries = 5,   timeout = 20,   endpoint_url = \"https://api.openai.com/v1/embeddings\",   key_name = \"OPENAI_API_KEY\",   verbose = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build OpenAI embedding API request — oai_build_embedding_request","text":"input Character vector text(s) embed model OpenAI embedding model use (default: \"text-embedding-3-small\") dimensions Number embedding dimensions (NULL uses model default) max_retries Maximum retry attempts failed requests (default: 5) timeout Request timeout seconds (default: 20) endpoint_url OpenAI API endpoint URL (default: OpenAI's embedding endpoint) key_name Name environment variable containing API key (default: \"OPENAI_API_KEY\") verbose Whether enable verbose request logging (default: FALSE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build OpenAI embedding API request — oai_build_embedding_request","text":"httr2 request object configured OpenAI embedding API. request object includes total_chars attribute containing total character count input texts.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build OpenAI embedding API request — oai_build_embedding_request","text":"function builds HTTP request execute . request can performed using httr2::req_perform() package's hf_perform_request() function. Note OpenAI limits input length - individual inputs exceed model's token limit (typically 8192 tokens embedding models). Empty strings allowed input.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build OpenAI embedding API request — oai_build_embedding_request","text":"","code":"if (FALSE) { # \\dontrun{   # Build a simple request   req <- oai_build_embedding_request(\"Hello world\")    # Build request with custom dimensions   req <- oai_build_embedding_request(     input = \"Hello world\",     dimensions = 512,     model = \"text-embedding-3-large\"   )    # Perform the request   response <- httr2::req_perform(req) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.html","id":null,"dir":"Reference","previous_headings":"","what":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","title":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","text":"function processes large volumes text OpenAI's Chat Completions API configurable chunks, writing results progressively CSV file. handles concurrent requests, automatic retries, structured outputs managing memory efficiently large-scale processing.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","text":"","code":"oai_complete_chunks(   texts,   ids,   chunk_size = 5000L,   model = \"gpt-4.1-nano\",   system_prompt = NULL,   output_file = \"auto\",   schema = NULL,   concurrent_requests = 5L,   temperature = 0L,   max_tokens = 500L,   max_retries = 5L,   timeout = 30L,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\" )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","text":"texts Character vector texts process ids Vector unique identifiers corresponding text (length texts) chunk_size Number texts process batch (default: 5000) model OpenAI model use (default: \"gpt-4.1-nano\") system_prompt Optional system prompt applied requests output_file Path .CSV file results. \"auto\" generates filename, location persistent across sessions. NULL, generates timestamped filename. schema Optional JSON schema structured output (json_schema object list) concurrent_requests Integer; number concurrent requests (default: 5) temperature Sampling temperature (0-2), lower = deterministic (default: 0) max_tokens Maximum tokens per response (default: 500) max_retries Maximum retry attempts per failed request (default: 5) timeout Request timeout seconds (default: 30) key_name Name environment variable containing API key (default: OPENAI_API_KEY) endpoint_url OpenAI API endpoint URL","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","text":"tibble containing results columns: id: Original identifier input content: API response content (text JSON string schema used) .error: Logical indicating request failed .error_msg: Error message failed, NA otherwise .batch: Batch number tracking","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","text":"function designed processing large text datasets may fit comfortably memory. divides input chunks, processes chunk concurrent API requests, writes results immediately disk minimise memory usage. function preserves data integrity matching results source texts ids parameter. chunk processed independently results appended output file, allowing resumable processing interrupted. using structured outputs schema, responses validated JSON schema stored raw JSON strings output file. allows flexible post-processing without memory constraints API calls. chunking strategy balances API efficiency memory management. Larger chunk_size values reduce overhead increase memory usage. Adjust based system resources text sizes.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","text":"","code":"if (FALSE) { # \\dontrun{ # basic usage with automatic file naming:  # large-scale processing with custom output file: #structured extraction with schema:   # post-process structured results: xx <- xx |>   dplyr::filter(!.error) |>   dplyr::mutate(parsed = map(content, ~jsonlite::fromJSON(.x))) |>   unnest_wider(parsed) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","title":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","text":"function takes data frame text inputs processes row OpenAI's Chat Completions API using efficient chunked processing. handles concurrent requests, automatic retries, structured output validation writing results progressively disk.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","text":"","code":"oai_complete_df(   df,   text_var,   id_var,   model = \"gpt-4.1-nano\",   output_file = \"auto\",   system_prompt = NULL,   schema = NULL,   chunk_size = 1000,   concurrent_requests = 1L,   max_retries = 5L,   timeout = 30,   temperature = 0,   max_tokens = 500L,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\" )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","text":"df Data frame containing text process text_var Column name (unquoted) containing text inputs id_var Column name (unquoted) unique row identifiers model OpenAI model use (default: \"gpt-4.1-nano\") output_file Path .CSV file results. \"auto\" generates filename, location persistent across sessions. NULL, generates timestamped filename. system_prompt Optional system prompt applied requests schema Optional JSON schema structured output (json_schema object list) chunk_size Number texts process batch (default: 5000) concurrent_requests Integer; number concurrent requests (default: 5) max_retries Maximum retry attempts per failed request (default: 5) timeout Request timeout seconds (default: 30) temperature Sampling temperature (0-2), lower = deterministic (default: 0) max_tokens Maximum tokens per response (default: 500) key_name Name environment variable containing API key (default: OPENAI_API_KEY) endpoint_url OpenAI API endpoint URL","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","text":"tibble original id column additional columns: content: API response content (text JSON string schema used) .error: Logical indicating request failed .error_msg: Error message failed, NA otherwise .batch: Batch number tracking","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","text":"function provides data frame interface chunked processing capabilities oai_complete_chunks(). extracts specified text column, processes texts configurable chunks concurrent API requests, returns results matched original data id_var parameter. chunking approach enables processing large data frames without memory constraints. Results written progressively CSV file (either specified auto-generated) read back return value. using structured outputs schema, responses validated JSON schema stored JSON strings. Post-processing may needed unnest separate columns. Failed requests marked .error = TRUE include error messages, allowing easy filtering retry logic failures.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","text":"","code":"if (FALSE) { # \\dontrun{  } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a completion for a single text using OpenAI's Chat Completions API — oai_complete_text","title":"Generate a completion for a single text using OpenAI's Chat Completions API — oai_complete_text","text":"High-level function generate completion single text string. function handles entire process request creation response processing, optional structured output support.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a completion for a single text using OpenAI's Chat Completions API — oai_complete_text","text":"","code":"oai_complete_text(   text,   model = \"gpt-4.1-nano\",   system_prompt = NULL,   schema = NULL,   temperature = 0,   max_tokens = 500L,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\",   max_retries = 5L,   timeout = 30,   tidy = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a completion for a single text using OpenAI's Chat Completions API — oai_complete_text","text":"text Character string send model model OpenAI model use (default: \"gpt-4.1-nano\") system_prompt Optional system prompt guide model's behaviour schema Optional JSON schema structured output (json_schema object list) temperature Sampling temperature (0-2), lower = deterministic (default: 0) max_tokens Maximum tokens response (default: 500) key_name Environment variable name API key (default: \"OPENAI_API_KEY\") endpoint_url OpenAI API endpoint URL max_retries Maximum retry attempts failed requests (default: 5) timeout Request timeout seconds (default: 30) tidy Whether attempt parse structured output (default: TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a completion for a single text using OpenAI's Chat Completions API — oai_complete_text","text":"Character string containing model's response, parsed JSON schema provided","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a completion for a single text using OpenAI's Chat Completions API — oai_complete_text","text":"","code":"if (FALSE) { # \\dontrun{   # Simple completion   response <- oai_complete_text(     text = \"Explain quantum computing in simple terms\",     temperature = 0.7   )    # With system prompt   response <- oai_complete_text(     text = \"What are the main benefits?\",     system_prompt = \"You are an expert in renewable energy\",     max_tokens = 200   )    # Structured output with schema   sentiment_schema <- create_json_schema(     name = \"sentiment_analysis\",     schema = schema_object(       sentiment = schema_string(\"positive, negative, or neutral\"),       confidence = schema_number(\"confidence score between 0 and 1\"),       required = list(\"sentiment\", \"confidence\")     )   )    result <- oai_complete_text(     text = \"I absolutely loved this product!\",     schema = sentiment_schema,     temperature = 0   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","title":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","text":"High-level function generate embeddings multiple text strings using OpenAI's embedding API. function handles batching, concurrent requests, error handling, provides progress reporting large collections texts.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","text":"","code":"oai_embed_batch(   texts,   model = \"text-embedding-3-small\",   dimensions = 1536,   batch_size = 10,   concurrent_requests = 1,   max_retries = 5,   timeout = 20,   endpoint_url = \"https://api.openai.com/v1/embeddings\",   key_name = \"OPENAI_API_KEY\",   include_texts = TRUE,   relocate_col = 2,   verbose = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","text":"texts Vector list character strings generate embeddings model OpenAI embedding model use (default: \"text-embedding-3-small\") dimensions Number embedding dimensions (default: 1536 text-embedding-3-small) batch_size Number texts process one API request (default: 10) concurrent_requests Number requests send simultaneously (default: 1) max_retries Maximum retry attempts failed requests (default: 5) timeout Request timeout seconds (default: 20) endpoint_url OpenAI API endpoint URL (default: OpenAI's embedding endpoint) key_name Name environment variable containing API key (default: \"OPENAI_API_KEY\") include_texts Whether include original texts result (default: TRUE) relocate_col Column position place error columns (default: 2) verbose Whether enable verbose request logging (default: FALSE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","text":"tibble containing: Embedding vectors columns (V1, V2, ..., Vn) .error: Logical column indicating embedding failed .error_message: Character column error details text: Original texts (include_texts = TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","text":"function efficiently processes multiple texts : Splitting texts batches specified size Creating concurrent requests (configured) faster processing Handling individual batch failures gracefully Pre-allocating memory embeddings improve performance Providing detailed success/failure reporting batch fails, documents specific batch marked failed, documents across batches. Failed embeddings filled NA values marked error information. function returns tibble embedding columns (V1, V2, ..., Vn), error tracking columns (.error, .error_message), optionally original texts.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","text":"","code":"if (FALSE) { # \\dontrun{   # Basic batch embedding   texts <- c(\"First text\", \"Second text\", \"Third text\")   embeddings <- oai_embed_batch(texts)    # Large-scale processing with concurrent requests   large_texts <- rep(\"Sample text\", 100)   embeddings <- oai_embed_batch(     texts = large_texts,     batch_size = 20,     concurrent_requests = 5,     dimensions = 512   )    # Custom model and settings   embeddings <- oai_embed_batch(     texts = texts,     model = \"text-embedding-3-large\",     dimensions = 1024,     include_texts = FALSE,     timeout = 30   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","title":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","text":"High-level function generate embeddings texts data frame using OpenAI's embedding API. function handles entire process request creation response processing, options batching & concurrent requests.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","text":"","code":"oai_embed_df(   df,   text_var,   id_var,   model = \"text-embedding-3-small\",   dimensions = 1536,   key_name = \"OPENAI_API_KEY\",   batch_size = 10,   concurrent_requests = 1,   max_retries = 5,   timeout = 20,   endpoint_url = \"https://api.openai.com/v1/embeddings\",   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","text":"df Data frame containing texts embed text_var Column name (unquoted) containing texts embed id_var Column name (unquoted) unique row identifiers model OpenAI embedding model use (default: \"text-embedding-3-small\") dimensions Number embedding dimensions (NULL uses model default) key_name Name environment variable containing API key batch_size Number texts process one batch (default: 10) concurrent_requests Number concurrent requests (default: 1) max_retries Maximum retry attempts per request (default: 5) timeout Request timeout seconds (default: 20) endpoint_url OpenAI API endpoint URL progress Whether display progress bar (default: TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","text":"Original data frame additional columns embeddings (V1, V2, etc.), plus .error .error_message columns indicating failures","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","text":"function extracts texts specified column, generates embeddings using oai_embed_batch(), joins results back original data frame using specified ID column. function preserves original data frame structure adds new columns embedding dimensions (V1, V2, ..., Vn). number rows match processing (due errors), returns results warning. OpenAI's embedding API allows specify number dimensions output embeddings, can useful reducing memory usage, storage cost,s matching specific downstream requirements. default model-specific (1536 text-embedding-3-small). OpenAI Embedding Updates","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","text":"","code":"if (FALSE) { # \\dontrun{   df <- data.frame(     id = 1:3,     text = c(\"First example\", \"Second example\", \"Third example\")   )    # Generate embeddings with default dimensions   embeddings_df <- oai_embed_df(     df = df,     text_var = text,     id_var = id   )    # Generate embeddings with custom dimensions   embeddings_df <- oai_embed_df(     df = df,     text_var = text,     id_var = id,     dimensions = 360,  # smaller embeddings     batch_size = 5   )    # Use with concurrent requests for faster processing   embeddings_df <- oai_embed_df(     df = df,     text_var = text,     id_var = id,     model = \"text-embedding-3-large\",     concurrent_requests = 3   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate embeddings for a single text using OpenAI — oai_embed_text","title":"Generate embeddings for a single text using OpenAI — oai_embed_text","text":"High-level function generate embeddings single text string using OpenAI's embedding API. function handles entire process request creation response processing.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate embeddings for a single text using OpenAI — oai_embed_text","text":"","code":"oai_embed_text(   text,   model = \"text-embedding-3-small\",   dimensions = NULL,   max_retries = 5,   timeout = 20,   endpoint_url = \"https://api.openai.com/v1/embeddings\",   key_name = \"OPENAI_API_KEY\",   tidy = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate embeddings for a single text using OpenAI — oai_embed_text","text":"text Character string generate embeddings (must non-empty) model OpenAI embedding model use (default: \"text-embedding-3-small\") dimensions Number embedding dimensions (NULL uses model default) max_retries Maximum retry attempts failed requests (default: 5) timeout Request timeout seconds (default: 20) endpoint_url OpenAI API endpoint URL (default: OpenAI's embedding endpoint) key_name Name environment variable containing API key (default: \"OPENAI_API_KEY\") tidy Whether return tidy tibble format (default: TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate embeddings for a single text using OpenAI — oai_embed_text","text":"tidy = TRUE, returns tibble embedding vectors columns (V1, V2, etc.). tidy = FALSE, returns raw httr2 response object.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate embeddings for a single text using OpenAI — oai_embed_text","text":"function designed single text inputs. processing multiple texts, use oai_embed_batch() efficient batch operations. function automatically handles API authentication, request retries, error handling. default, returns tidy tibble embedding vectors columns, can get raw response setting tidy = FALSE.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate embeddings for a single text using OpenAI — oai_embed_text","text":"","code":"if (FALSE) { # \\dontrun{   # Generate embeddings for a single text   embeddings <- oai_embed_text(\"Hello world\")    # Use a different model with custom dimensions   embeddings <- oai_embed_text(     text = \"Hello world\",     model = \"text-embedding-3-large\",     dimensions = 1024   )    # Get raw response instead of tidy format   raw_response <- oai_embed_text(     text = \"Hello world\",     tidy = FALSE   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","title":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","text":"Executes list HTTP requests either sequentially parallel. Automatically chooses sequential processing concurrent_requests = 1 one request.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","text":"","code":"perform_requests_with_strategy(   requests,   concurrent_requests = 1,   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","text":"requests List httr2_request objects perform concurrent_requests Integer specifying maximum number simultaneous requests (default: 1) progress Logical indicating whether show progress bar (default: TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","text":"List httr2_response objects error objects failed requests","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","text":"returns responses order requests sent, returns errors predictable format.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","text":"","code":"if (FALSE) { # \\dontrun{   # Sequential processing   responses <- perform_requests_with_strategy(     requests = my_requests,     concurrent_requests = 1   )    # Parallel processing with 5 concurrent requests   responses <- perform_requests_with_strategy(     requests = my_requests,     concurrent_requests = 5,     progress = TRUE   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/process_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Process API response with error handling — process_response","title":"Process API response with error handling — process_response","text":"Higher-order function applies tidying function API response. Handles successful responses errors, returning consistent tibble structure. tidy_func parameter allows provide necessary function particular workflow.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/process_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process API response with error handling — process_response","text":"","code":"process_response(resp, indices, tidy_func)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/process_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process API response with error handling — process_response","text":"resp httr2_response object error object failed request indices Vector indices track original position requests tidy_func Function process/tidy successful API responses","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/process_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process API response with error handling — process_response","text":"tibble processed results error information, including: original_index: Position original request batch .error: Logical indicating error occurred .error_message: Character description error Additional columns tidy_func output","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/process_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process API response with error handling — process_response","text":"","code":"if (FALSE) { # \\dontrun{   # Process a response with custom tidying function   result <- process_response(     resp = api_response,     indices = c(1, 2, 3),     tidy_func = function(r) { tibble::tibble(data = httr2::resp_body_json(r)) }   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_from_json.html","id":null,"dir":"Reference","previous_headings":"","what":"Safely extract JSON — safely_from_json","title":"Safely extract JSON — safely_from_json","text":"wrapper around jsonlite::fromJSON returns list instead throwing error JSON parsing fails. Uses purrr::possibly provide graceful error handling. explicit: receive error message JSON failed parse, just empty list. property allows handle NULL/NA results way like .","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_from_json.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Safely extract JSON — safely_from_json","text":"","code":"safely_from_json(...)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_from_json.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Safely extract JSON — safely_from_json","text":"... Arguments passed jsonlite::fromJSON. first argument txt (JSON string, URL file parse). Additional arguments include: simplifyVector (coerce JSON arrays containing primitives atomic vectors, default TRUE), simplifyDataFrame (coerce JSON arrays containing objects data frames, default TRUE), simplifyMatrix (coerce JSON arrays containing equal-length sub-arrays matrices, default TRUE), flatten (automatically flatten nested data frames, default FALSE).","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_from_json.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Safely extract JSON — safely_from_json","text":"Parsed JSON object success, empty list failure","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_from_json.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Safely extract JSON — safely_from_json","text":"","code":"# Valid JSON safely_from_json('{\"name\": \"John\", \"age\": 30}') #> $name #> [1] \"John\" #>  #> $age #> [1] 30 #>   # Invalid JSON returns empty list instead of error safely_from_json('{\"invalid\": json}') #> list()  # Works with URLs and files too safely_from_json(\"https://api.example.com/data.json\") #> Warning: URL 'https://api.example.com/data.json': status was 'Couldn't resolve host name' #> list()"},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Safely perform an embedding request with error handling — safely_perform_request","title":"Safely perform an embedding request with error handling — safely_perform_request","text":"Wrapper around httr2::req_perform handles errors gracefully.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Safely perform an embedding request with error handling — safely_perform_request","text":"","code":"safely_perform_request(request)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Safely perform an embedding request with error handling — safely_perform_request","text":"request httr2 request object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Safely perform an embedding request with error handling — safely_perform_request","text":"list components $result $error","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Create array property schema — schema_array","title":"Create array property schema — schema_array","text":"Defines list/array fields containing multiple items type. Use tags, categories, collection data. Can constrain array length.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create array property schema — schema_array","text":"","code":"schema_array(items, description = NULL, min_items = NULL, max_items = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create array property schema — schema_array","text":"items Schema definition array elements (use schema_* helpers) description Human-readable field description min_items Minimum number array elements max_items Maximum number array elements","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_array.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create array property schema — schema_array","text":"","code":"# List of tags schema_array(schema_string(), \"Product tags\", max_items = 10) #> $type #> [1] \"array\" #>  #> $items #> $items$type #> [1] \"string\" #>  #>  #> $description #> [1] \"Product tags\" #>  #> $maxItems #> [1] 10 #>   # Array of objects schema_array(   schema_object(     name = schema_string(\"Ingredient name\"),     amount = schema_number(\"Amount needed\")   ),   \"Recipe ingredients\" ) #> $type #> [1] \"array\" #>  #> $items #> $items$type #> [1] \"object\" #>  #> $items$properties #> $items$properties$name #> $items$properties$name$type #> [1] \"string\" #>  #> $items$properties$name$description #> [1] \"Ingredient name\" #>  #>  #> $items$properties$amount #> $items$properties$amount$type #> [1] \"number\" #>  #> $items$properties$amount$description #> [1] \"Amount needed\" #>  #>  #>  #> $items$additionalProperties #> [1] FALSE #>  #>  #> $description #> [1] \"Recipe ingredients\" #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_boolean.html","id":null,"dir":"Reference","previous_headings":"","what":"Create boolean property schema — schema_boolean","title":"Create boolean property schema — schema_boolean","text":"Defines true/false fields. Use flags, yes/questions, binary states. Ensures LLM returns exactly true false, \"yes\"/\"\" strings.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_boolean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create boolean property schema — schema_boolean","text":"","code":"schema_boolean(description = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_boolean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create boolean property schema — schema_boolean","text":"description Human-readable field description","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_boolean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create boolean property schema — schema_boolean","text":"","code":"# Feature flag schema_boolean(\"Has premium subscription\") #> $type #> [1] \"boolean\" #>  #> $description #> [1] \"Has premium subscription\" #>   # Classification result schema_boolean(\"Contains sensitive information\") #> $type #> [1] \"boolean\" #>  #> $description #> [1] \"Contains sensitive information\" #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_enum.html","id":null,"dir":"Reference","previous_headings":"","what":"Create enumerated property schema — schema_enum","title":"Create enumerated property schema — schema_enum","text":"Defines fields constrained specific allowed values. flexible schema_string enum parameter - supports numeric enums mixed types. Use categories, status codes, multi-choice field.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_enum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create enumerated property schema — schema_enum","text":"","code":"schema_enum(values, description = NULL, type = \"string\")"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_enum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create enumerated property schema — schema_enum","text":"values Vector allowed values description Human-readable field description type Data type enum values (\"string\", \"integer\", \"number\")","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_enum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create enumerated property schema — schema_enum","text":"","code":"# Status categories schema_enum(c(\"draft\", \"published\", \"archived\"), \"Document status\") #> $type #> [1] \"string\" #>  #> $enum #> [1] \"draft\"     \"published\" \"archived\"  #>  #> $description #> [1] \"Document status\" #>   # Priority levels as numbers schema_enum(c(1, 2, 3, 4, 5), \"Priority level\", type = \"integer\") #> $type #> [1] \"integer\" #>  #> $enum #> [1] 1 2 3 4 5 #>  #> $description #> [1] \"Priority level\" #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_integer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create integer property schema — schema_integer","title":"Create integer property schema — schema_integer","text":"Defines whole number fields optional range constraints. Use counts, IDs, quantities, discrete numeric values. restrictive schema_number.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_integer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create integer property schema — schema_integer","text":"","code":"schema_integer(description = NULL, minimum = NULL, maximum = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_integer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create integer property schema — schema_integer","text":"description Human-readable field description minimum Minimum allowed value (inclusive) maximum Maximum allowed value (inclusive)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_integer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create integer property schema — schema_integer","text":"","code":"# Item quantity schema_integer(\"Quantity ordered\", minimum = 1) #> $type #> [1] \"integer\" #>  #> $description #> [1] \"Quantity ordered\" #>  #> $minimum #> [1] 1 #>   # Rating scale schema_integer(\"Star rating\", minimum = 1, maximum = 5) #> $type #> [1] \"integer\" #>  #> $description #> [1] \"Star rating\" #>  #> $minimum #> [1] 1 #>  #> $maximum #> [1] 5 #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_number.html","id":null,"dir":"Reference","previous_headings":"","what":"Create numeric property schema — schema_number","title":"Create numeric property schema — schema_number","text":"Defines decimal number fields optional range constraints. Use prices, percentages, ratings, continuous numeric data.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_number.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create numeric property schema — schema_number","text":"","code":"schema_number(description = NULL, minimum = NULL, maximum = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_number.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create numeric property schema — schema_number","text":"description Human-readable field description minimum Minimum allowed value (inclusive) maximum Maximum allowed value (inclusive)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_number.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create numeric property schema — schema_number","text":"","code":"# Product price schema_number(\"Price in USD\", minimum = 0) #> $type #> [1] \"number\" #>  #> $description #> [1] \"Price in USD\" #>  #> $minimum #> [1] 0 #>   # Percentage score schema_number(\"Confidence score\", minimum = 0, maximum = 100) #> $type #> [1] \"number\" #>  #> $description #> [1] \"Confidence score\" #>  #> $minimum #> [1] 0 #>  #> $maximum #> [1] 100 #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_object.html","id":null,"dir":"Reference","previous_headings":"","what":"Create JSON Schema object definitions — schema_object","title":"Create JSON Schema object definitions — schema_object","text":"helper functions simplify creating JSON Schema definitions structured outputs (e.g. OpenAI). provide type-safe, validated schemas ensure consistent LLM responses matching expected data structure. JSON Schema constrains LLM outputs specific formats, preventing parsing errors ensuring reliable data extraction unstructured text. Create object schema nested properties Defines JSON object typed properties. Use structured data like user profiles, API responses, nested data structure. LLM return JSON matching exactly schema.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_object.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create JSON Schema object definitions — schema_object","text":"","code":"schema_object(..., required = NULL, additional_properties = FALSE)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_object.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create JSON Schema object definitions — schema_object","text":"... Named arguments defining object properties (use schema_* helpers) required Character vector required property names additional_properties Whether allow extra properties beyond defined","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_object.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create JSON Schema object definitions — schema_object","text":"","code":"# User profile with required fields schema_object(   name = schema_string(\"Full name\"),   age = schema_integer(\"Age in years\"),   required = c(\"name\", \"age\") ) #> $type #> [1] \"object\" #>  #> $properties #> $properties$name #> $properties$name$type #> [1] \"string\" #>  #> $properties$name$description #> [1] \"Full name\" #>  #>  #> $properties$age #> $properties$age$type #> [1] \"integer\" #>  #> $properties$age$description #> [1] \"Age in years\" #>  #>  #>  #> $additionalProperties #> [1] FALSE #>  #> $required #> [1] \"name\" \"age\"  #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_string.html","id":null,"dir":"Reference","previous_headings":"","what":"Create string property schema — schema_string","title":"Create string property schema — schema_string","text":"Defines text fields optional constraints. Use names, descriptions, textual data. Can restrict specific values via enum parameter.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_string.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create string property schema — schema_string","text":"","code":"schema_string(description = NULL, enum = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_string.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create string property schema — schema_string","text":"description Human-readable field description (helps LLM understand context) enum Character vector allowed values (creates dropdown-like constraint)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_string.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create string property schema — schema_string","text":"","code":"# Simple text field schema_string(\"User's email address\") #> $type #> [1] \"string\" #>  #> $description #> [1] \"User's email address\" #>   # Constrained to specific values schema_string(\"Sentiment\", enum = c(\"positive\", \"negative\", \"neutral\")) #> $type #> [1] \"string\" #>  #> $description #> [1] \"Sentiment\" #>  #> $enum #> [1] \"positive\" \"negative\" \"neutral\"  #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/sentiment_classification_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Single sentiment classification result example — sentiment_classification_example","title":"Single sentiment classification result example — sentiment_classification_example","text":"sample result sentiment classification single text using Hugging Face's classification API. demonstrates structure returned hf_classify_text() function.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/sentiment_classification_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Single sentiment classification result example — sentiment_classification_example","text":"","code":"sentiment_classification_example"},{"path":"https://jpcompartir.github.io/EndpointR/reference/sentiment_classification_example.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Single sentiment classification result example — sentiment_classification_example","text":"data frame 1 row 2 variables: NEGATIVE Numeric; probability score negative sentiment (0-1) POSITIVE Numeric; probability score positive sentiment (0-1)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/sentiment_classification_example.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Single sentiment classification result example — sentiment_classification_example","text":"Generated using Hugging Face sentiment classification model via EndpointR functions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Set your API keys so they can be accessed by EndpointR — set_api_key","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"Set API key endpoint - endpoint Anthropic, OpenAI, specific Hugging Face Inference endpoint, another supported provider. Add overwrite=TRUE need update existing key. able retrieve key get_api_key() function.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"","code":"set_api_key(key_name, overwrite = FALSE)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"key_name name API format \"ENDPOINT_API_KEY\" -> \"ANTHROPIC_API_KEY\" overwrite Whether overwrite existing value API key.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"Nothing","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"","code":"if (FALSE) { # \\dontrun{   # set an Anthropic API key   set_api_key(\"ANTHROPIC_API_KEY\")    # update an existing OpenAI key   set_api_key(\"OPENAI_API_KEY\", overwrite = TRUE) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/single_embedding_hf.html","id":null,"dir":"Reference","previous_headings":"","what":"Single embedding result example from Hugging Face API — single_embedding_hf","title":"Single embedding result example from Hugging Face API — single_embedding_hf","text":"sample embedding vector result processing single text using Hugging Face's embedding API. demonstrates structure returned hf_embed_text() function.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/single_embedding_hf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Single embedding result example from Hugging Face API — single_embedding_hf","text":"","code":"single_embedding_hf"},{"path":"https://jpcompartir.github.io/EndpointR/reference/single_embedding_hf.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Single embedding result example from Hugging Face API — single_embedding_hf","text":"data frame 1 row 768 variables: V1 Numeric; embedding vector dimensions V2 Numeric; embedding vector dimensions V3 Numeric; embedding vector dimensions V4 Numeric; embedding vector dimensions V5 Numeric; embedding vector dimensions V6 Numeric; embedding vector dimensions V7 Numeric; embedding vector dimensions V8 Numeric; embedding vector dimensions V9 Numeric; embedding vector dimensions V10 Numeric; embedding vector dimensions V11 Numeric; embedding vector dimensions V12 Numeric; embedding vector dimensions V13 Numeric; embedding vector dimensions V14 Numeric; embedding vector dimensions V15 Numeric; embedding vector dimensions V16 Numeric; embedding vector dimensions V17 Numeric; embedding vector dimensions V18 Numeric; embedding vector dimensions V19 Numeric; embedding vector dimensions V20 Numeric; embedding vector dimensions V21 Numeric; embedding vector dimensions V22 Numeric; embedding vector dimensions V23 Numeric; embedding vector dimensions V24 Numeric; embedding vector dimensions V25 Numeric; embedding vector dimensions V26 Numeric; embedding vector dimensions V27 Numeric; embedding vector dimensions V28 Numeric; embedding vector dimensions V29 Numeric; embedding vector dimensions V30 Numeric; embedding vector dimensions V31 Numeric; embedding vector dimensions V32 Numeric; embedding vector dimensions V33 Numeric; embedding vector dimensions V34 Numeric; embedding vector dimensions V35 Numeric; embedding vector dimensions V36 Numeric; embedding vector dimensions V37 Numeric; embedding vector dimensions V38 Numeric; embedding vector dimensions V39 Numeric; embedding vector dimensions V40 Numeric; embedding vector dimensions V41 Numeric; embedding vector dimensions V42 Numeric; embedding vector dimensions V43 Numeric; embedding vector dimensions V44 Numeric; embedding vector dimensions V45 Numeric; embedding vector dimensions V46 Numeric; embedding vector dimensions V47 Numeric; embedding vector dimensions V48 Numeric; embedding vector dimensions V49 Numeric; embedding vector dimensions V50 Numeric; embedding vector dimensions V51 Numeric; embedding vector dimensions V52 Numeric; embedding vector dimensions V53 Numeric; embedding vector dimensions V54 Numeric; embedding vector dimensions V55 Numeric; embedding vector dimensions V56 Numeric; embedding vector dimensions V57 Numeric; embedding vector dimensions V58 Numeric; embedding vector dimensions V59 Numeric; embedding vector dimensions V60 Numeric; embedding vector dimensions V61 Numeric; embedding vector dimensions V62 Numeric; embedding vector dimensions V63 Numeric; embedding vector dimensions V64 Numeric; embedding vector dimensions V65 Numeric; embedding vector dimensions V66 Numeric; embedding vector dimensions V67 Numeric; embedding vector dimensions V68 Numeric; embedding vector dimensions V69 Numeric; embedding vector dimensions V70 Numeric; embedding vector dimensions V71 Numeric; embedding vector dimensions V72 Numeric; embedding vector dimensions V73 Numeric; embedding vector dimensions V74 Numeric; embedding vector dimensions V75 Numeric; embedding vector dimensions V76 Numeric; embedding vector dimensions V77 Numeric; embedding vector dimensions V78 Numeric; embedding vector dimensions V79 Numeric; embedding vector dimensions V80 Numeric; embedding vector dimensions V81 Numeric; embedding vector dimensions V82 Numeric; embedding vector dimensions V83 Numeric; embedding vector dimensions V84 Numeric; embedding vector dimensions V85 Numeric; embedding vector dimensions V86 Numeric; embedding vector dimensions V87 Numeric; embedding vector dimensions V88 Numeric; embedding vector dimensions V89 Numeric; embedding vector dimensions V90 Numeric; embedding vector dimensions V91 Numeric; embedding vector dimensions V92 Numeric; embedding vector dimensions V93 Numeric; embedding vector dimensions V94 Numeric; embedding vector dimensions V95 Numeric; embedding vector dimensions V96 Numeric; embedding vector dimensions V97 Numeric; embedding vector dimensions V98 Numeric; embedding vector dimensions V99 Numeric; embedding vector dimensions V100 Numeric; embedding vector dimensions V101 Numeric; embedding vector dimensions V102 Numeric; embedding vector dimensions V103 Numeric; embedding vector dimensions V104 Numeric; embedding vector dimensions V105 Numeric; embedding vector dimensions V106 Numeric; embedding vector dimensions V107 Numeric; embedding vector dimensions V108 Numeric; embedding vector dimensions V109 Numeric; embedding vector dimensions V110 Numeric; embedding vector dimensions V111 Numeric; embedding vector dimensions V112 Numeric; embedding vector dimensions V113 Numeric; embedding vector dimensions V114 Numeric; embedding vector dimensions V115 Numeric; embedding vector dimensions V116 Numeric; embedding vector dimensions V117 Numeric; embedding vector dimensions V118 Numeric; embedding vector dimensions V119 Numeric; embedding vector dimensions V120 Numeric; embedding vector dimensions V121 Numeric; embedding vector dimensions V122 Numeric; embedding vector dimensions V123 Numeric; embedding vector dimensions V124 Numeric; embedding vector dimensions V125 Numeric; embedding vector dimensions V126 Numeric; embedding vector dimensions V127 Numeric; embedding vector dimensions V128 Numeric; embedding vector dimensions V129 Numeric; embedding vector dimensions V130 Numeric; embedding vector dimensions V131 Numeric; embedding vector dimensions V132 Numeric; embedding vector dimensions V133 Numeric; embedding vector dimensions V134 Numeric; embedding vector dimensions V135 Numeric; embedding vector dimensions V136 Numeric; embedding vector dimensions V137 Numeric; embedding vector dimensions V138 Numeric; embedding vector dimensions V139 Numeric; embedding vector dimensions V140 Numeric; embedding vector dimensions V141 Numeric; embedding vector dimensions V142 Numeric; embedding vector dimensions V143 Numeric; embedding vector dimensions V144 Numeric; embedding vector dimensions V145 Numeric; embedding vector dimensions V146 Numeric; embedding vector dimensions V147 Numeric; embedding vector dimensions V148 Numeric; embedding vector dimensions V149 Numeric; embedding vector dimensions V150 Numeric; embedding vector dimensions V151 Numeric; embedding vector dimensions V152 Numeric; embedding vector dimensions V153 Numeric; embedding vector dimensions V154 Numeric; embedding vector dimensions V155 Numeric; embedding vector dimensions V156 Numeric; embedding vector dimensions V157 Numeric; embedding vector dimensions V158 Numeric; embedding vector dimensions V159 Numeric; embedding vector dimensions V160 Numeric; embedding vector dimensions V161 Numeric; embedding vector dimensions V162 Numeric; embedding vector dimensions V163 Numeric; embedding vector dimensions V164 Numeric; embedding vector dimensions V165 Numeric; embedding vector dimensions V166 Numeric; embedding vector dimensions V167 Numeric; embedding vector dimensions V168 Numeric; embedding vector dimensions V169 Numeric; embedding vector dimensions V170 Numeric; embedding vector dimensions V171 Numeric; embedding vector dimensions V172 Numeric; embedding vector dimensions V173 Numeric; embedding vector dimensions V174 Numeric; embedding vector dimensions V175 Numeric; embedding vector dimensions V176 Numeric; embedding vector dimensions V177 Numeric; embedding vector dimensions V178 Numeric; embedding vector dimensions V179 Numeric; embedding vector dimensions V180 Numeric; embedding vector dimensions V181 Numeric; embedding vector dimensions V182 Numeric; embedding vector dimensions V183 Numeric; embedding vector dimensions V184 Numeric; embedding vector dimensions V185 Numeric; embedding vector dimensions V186 Numeric; embedding vector dimensions V187 Numeric; embedding vector dimensions V188 Numeric; embedding vector dimensions V189 Numeric; embedding vector dimensions V190 Numeric; embedding vector dimensions V191 Numeric; embedding vector dimensions V192 Numeric; embedding vector dimensions V193 Numeric; embedding vector dimensions V194 Numeric; embedding vector dimensions V195 Numeric; embedding vector dimensions V196 Numeric; embedding vector dimensions V197 Numeric; embedding vector dimensions V198 Numeric; embedding vector dimensions V199 Numeric; embedding vector dimensions V200 Numeric; embedding vector dimensions V201 Numeric; embedding vector dimensions V202 Numeric; embedding vector dimensions V203 Numeric; embedding vector dimensions V204 Numeric; embedding vector dimensions V205 Numeric; embedding vector dimensions V206 Numeric; embedding vector dimensions V207 Numeric; embedding vector dimensions V208 Numeric; embedding vector dimensions V209 Numeric; embedding vector dimensions V210 Numeric; embedding vector dimensions V211 Numeric; embedding vector dimensions V212 Numeric; embedding vector dimensions V213 Numeric; embedding vector dimensions V214 Numeric; embedding vector dimensions V215 Numeric; embedding vector dimensions V216 Numeric; embedding vector dimensions V217 Numeric; embedding vector dimensions V218 Numeric; embedding vector dimensions V219 Numeric; embedding vector dimensions V220 Numeric; embedding vector dimensions V221 Numeric; embedding vector dimensions V222 Numeric; embedding vector dimensions V223 Numeric; embedding vector dimensions V224 Numeric; embedding vector dimensions V225 Numeric; embedding vector dimensions V226 Numeric; embedding vector dimensions V227 Numeric; embedding vector dimensions V228 Numeric; embedding vector dimensions V229 Numeric; embedding vector dimensions V230 Numeric; embedding vector dimensions V231 Numeric; embedding vector dimensions V232 Numeric; embedding vector dimensions V233 Numeric; embedding vector dimensions V234 Numeric; embedding vector dimensions V235 Numeric; embedding vector dimensions V236 Numeric; embedding vector dimensions V237 Numeric; embedding vector dimensions V238 Numeric; embedding vector dimensions V239 Numeric; embedding vector dimensions V240 Numeric; embedding vector dimensions V241 Numeric; embedding vector dimensions V242 Numeric; embedding vector dimensions V243 Numeric; embedding vector dimensions V244 Numeric; embedding vector dimensions V245 Numeric; embedding vector dimensions V246 Numeric; embedding vector dimensions V247 Numeric; embedding vector dimensions V248 Numeric; embedding vector dimensions V249 Numeric; embedding vector dimensions V250 Numeric; embedding vector dimensions V251 Numeric; embedding vector dimensions V252 Numeric; embedding vector dimensions V253 Numeric; embedding vector dimensions V254 Numeric; embedding vector dimensions V255 Numeric; embedding vector dimensions V256 Numeric; embedding vector dimensions V257 Numeric; embedding vector dimensions V258 Numeric; embedding vector dimensions V259 Numeric; embedding vector dimensions V260 Numeric; embedding vector dimensions V261 Numeric; embedding vector dimensions V262 Numeric; embedding vector dimensions V263 Numeric; embedding vector dimensions V264 Numeric; embedding vector dimensions V265 Numeric; embedding vector dimensions V266 Numeric; embedding vector dimensions V267 Numeric; embedding vector dimensions V268 Numeric; embedding vector dimensions V269 Numeric; embedding vector dimensions V270 Numeric; embedding vector dimensions V271 Numeric; embedding vector dimensions V272 Numeric; embedding vector dimensions V273 Numeric; embedding vector dimensions V274 Numeric; embedding vector dimensions V275 Numeric; embedding vector dimensions V276 Numeric; embedding vector dimensions V277 Numeric; embedding vector dimensions V278 Numeric; embedding vector dimensions V279 Numeric; embedding vector dimensions V280 Numeric; embedding vector dimensions V281 Numeric; embedding vector dimensions V282 Numeric; embedding vector dimensions V283 Numeric; embedding vector dimensions V284 Numeric; embedding vector dimensions V285 Numeric; embedding vector dimensions V286 Numeric; embedding vector dimensions V287 Numeric; embedding vector dimensions V288 Numeric; embedding vector dimensions V289 Numeric; embedding vector dimensions V290 Numeric; embedding vector dimensions V291 Numeric; embedding vector dimensions V292 Numeric; embedding vector dimensions V293 Numeric; embedding vector dimensions V294 Numeric; embedding vector dimensions V295 Numeric; embedding vector dimensions V296 Numeric; embedding vector dimensions V297 Numeric; embedding vector dimensions V298 Numeric; embedding vector dimensions V299 Numeric; embedding vector dimensions V300 Numeric; embedding vector dimensions V301 Numeric; embedding vector dimensions V302 Numeric; embedding vector dimensions V303 Numeric; embedding vector dimensions V304 Numeric; embedding vector dimensions V305 Numeric; embedding vector dimensions V306 Numeric; embedding vector dimensions V307 Numeric; embedding vector dimensions V308 Numeric; embedding vector dimensions V309 Numeric; embedding vector dimensions V310 Numeric; embedding vector dimensions V311 Numeric; embedding vector dimensions V312 Numeric; embedding vector dimensions V313 Numeric; embedding vector dimensions V314 Numeric; embedding vector dimensions V315 Numeric; embedding vector dimensions V316 Numeric; embedding vector dimensions V317 Numeric; embedding vector dimensions V318 Numeric; embedding vector dimensions V319 Numeric; embedding vector dimensions V320 Numeric; embedding vector dimensions V321 Numeric; embedding vector dimensions V322 Numeric; embedding vector dimensions V323 Numeric; embedding vector dimensions V324 Numeric; embedding vector dimensions V325 Numeric; embedding vector dimensions V326 Numeric; embedding vector dimensions V327 Numeric; embedding vector dimensions V328 Numeric; embedding vector dimensions V329 Numeric; embedding vector dimensions V330 Numeric; embedding vector dimensions V331 Numeric; embedding vector dimensions V332 Numeric; embedding vector dimensions V333 Numeric; embedding vector dimensions V334 Numeric; embedding vector dimensions V335 Numeric; embedding vector dimensions V336 Numeric; embedding vector dimensions V337 Numeric; embedding vector dimensions V338 Numeric; embedding vector dimensions V339 Numeric; embedding vector dimensions V340 Numeric; embedding vector dimensions V341 Numeric; embedding vector dimensions V342 Numeric; embedding vector dimensions V343 Numeric; embedding vector dimensions V344 Numeric; embedding vector dimensions V345 Numeric; embedding vector dimensions V346 Numeric; embedding vector dimensions V347 Numeric; embedding vector dimensions V348 Numeric; embedding vector dimensions V349 Numeric; embedding vector dimensions V350 Numeric; embedding vector dimensions V351 Numeric; embedding vector dimensions V352 Numeric; embedding vector dimensions V353 Numeric; embedding vector dimensions V354 Numeric; embedding vector dimensions V355 Numeric; embedding vector dimensions V356 Numeric; embedding vector dimensions V357 Numeric; embedding vector dimensions V358 Numeric; embedding vector dimensions V359 Numeric; embedding vector dimensions V360 Numeric; embedding vector dimensions V361 Numeric; embedding vector dimensions V362 Numeric; embedding vector dimensions V363 Numeric; embedding vector dimensions V364 Numeric; embedding vector dimensions V365 Numeric; embedding vector dimensions V366 Numeric; embedding vector dimensions V367 Numeric; embedding vector dimensions V368 Numeric; embedding vector dimensions V369 Numeric; embedding vector dimensions V370 Numeric; embedding vector dimensions V371 Numeric; embedding vector dimensions V372 Numeric; embedding vector dimensions V373 Numeric; embedding vector dimensions V374 Numeric; embedding vector dimensions V375 Numeric; embedding vector dimensions V376 Numeric; embedding vector dimensions V377 Numeric; embedding vector dimensions V378 Numeric; embedding vector dimensions V379 Numeric; embedding vector dimensions V380 Numeric; embedding vector dimensions V381 Numeric; embedding vector dimensions V382 Numeric; embedding vector dimensions V383 Numeric; embedding vector dimensions V384 Numeric; embedding vector dimensions V385 Numeric; embedding vector dimensions V386 Numeric; embedding vector dimensions V387 Numeric; embedding vector dimensions V388 Numeric; embedding vector dimensions V389 Numeric; embedding vector dimensions V390 Numeric; embedding vector dimensions V391 Numeric; embedding vector dimensions V392 Numeric; embedding vector dimensions V393 Numeric; embedding vector dimensions V394 Numeric; embedding vector dimensions V395 Numeric; embedding vector dimensions V396 Numeric; embedding vector dimensions V397 Numeric; embedding vector dimensions V398 Numeric; embedding vector dimensions V399 Numeric; embedding vector dimensions V400 Numeric; embedding vector dimensions V401 Numeric; embedding vector dimensions V402 Numeric; embedding vector dimensions V403 Numeric; embedding vector dimensions V404 Numeric; embedding vector dimensions V405 Numeric; embedding vector dimensions V406 Numeric; embedding vector dimensions V407 Numeric; embedding vector dimensions V408 Numeric; embedding vector dimensions V409 Numeric; embedding vector dimensions V410 Numeric; embedding vector dimensions V411 Numeric; embedding vector dimensions V412 Numeric; embedding vector dimensions V413 Numeric; embedding vector dimensions V414 Numeric; embedding vector dimensions V415 Numeric; embedding vector dimensions V416 Numeric; embedding vector dimensions V417 Numeric; embedding vector dimensions V418 Numeric; embedding vector dimensions V419 Numeric; embedding vector dimensions V420 Numeric; embedding vector dimensions V421 Numeric; embedding vector dimensions V422 Numeric; embedding vector dimensions V423 Numeric; embedding vector dimensions V424 Numeric; embedding vector dimensions V425 Numeric; embedding vector dimensions V426 Numeric; embedding vector dimensions V427 Numeric; embedding vector dimensions V428 Numeric; embedding vector dimensions V429 Numeric; embedding vector dimensions V430 Numeric; embedding vector dimensions V431 Numeric; embedding vector dimensions V432 Numeric; embedding vector dimensions V433 Numeric; embedding vector dimensions V434 Numeric; embedding vector dimensions V435 Numeric; embedding vector dimensions V436 Numeric; embedding vector dimensions V437 Numeric; embedding vector dimensions V438 Numeric; embedding vector dimensions V439 Numeric; embedding vector dimensions V440 Numeric; embedding vector dimensions V441 Numeric; embedding vector dimensions V442 Numeric; embedding vector dimensions V443 Numeric; embedding vector dimensions V444 Numeric; embedding vector dimensions V445 Numeric; embedding vector dimensions V446 Numeric; embedding vector dimensions V447 Numeric; embedding vector dimensions V448 Numeric; embedding vector dimensions V449 Numeric; embedding vector dimensions V450 Numeric; embedding vector dimensions V451 Numeric; embedding vector dimensions V452 Numeric; embedding vector dimensions V453 Numeric; embedding vector dimensions V454 Numeric; embedding vector dimensions V455 Numeric; embedding vector dimensions V456 Numeric; embedding vector dimensions V457 Numeric; embedding vector dimensions V458 Numeric; embedding vector dimensions V459 Numeric; embedding vector dimensions V460 Numeric; embedding vector dimensions V461 Numeric; embedding vector dimensions V462 Numeric; embedding vector dimensions V463 Numeric; embedding vector dimensions V464 Numeric; embedding vector dimensions V465 Numeric; embedding vector dimensions V466 Numeric; embedding vector dimensions V467 Numeric; embedding vector dimensions V468 Numeric; embedding vector dimensions V469 Numeric; embedding vector dimensions V470 Numeric; embedding vector dimensions V471 Numeric; embedding vector dimensions V472 Numeric; embedding vector dimensions V473 Numeric; embedding vector dimensions V474 Numeric; embedding vector dimensions V475 Numeric; embedding vector dimensions V476 Numeric; embedding vector dimensions V477 Numeric; embedding vector dimensions V478 Numeric; embedding vector dimensions V479 Numeric; embedding vector dimensions V480 Numeric; embedding vector dimensions V481 Numeric; embedding vector dimensions V482 Numeric; embedding vector dimensions V483 Numeric; embedding vector dimensions V484 Numeric; embedding vector dimensions V485 Numeric; embedding vector dimensions V486 Numeric; embedding vector dimensions V487 Numeric; embedding vector dimensions V488 Numeric; embedding vector dimensions V489 Numeric; embedding vector dimensions V490 Numeric; embedding vector dimensions V491 Numeric; embedding vector dimensions V492 Numeric; embedding vector dimensions V493 Numeric; embedding vector dimensions V494 Numeric; embedding vector dimensions V495 Numeric; embedding vector dimensions V496 Numeric; embedding vector dimensions V497 Numeric; embedding vector dimensions V498 Numeric; embedding vector dimensions V499 Numeric; embedding vector dimensions V500 Numeric; embedding vector dimensions V501 Numeric; embedding vector dimensions V502 Numeric; embedding vector dimensions V503 Numeric; embedding vector dimensions V504 Numeric; embedding vector dimensions V505 Numeric; embedding vector dimensions V506 Numeric; embedding vector dimensions V507 Numeric; embedding vector dimensions V508 Numeric; embedding vector dimensions V509 Numeric; embedding vector dimensions V510 Numeric; embedding vector dimensions V511 Numeric; embedding vector dimensions V512 Numeric; embedding vector dimensions V513 Numeric; embedding vector dimensions V514 Numeric; embedding vector dimensions V515 Numeric; embedding vector dimensions V516 Numeric; embedding vector dimensions V517 Numeric; embedding vector dimensions V518 Numeric; embedding vector dimensions V519 Numeric; embedding vector dimensions V520 Numeric; embedding vector dimensions V521 Numeric; embedding vector dimensions V522 Numeric; embedding vector dimensions V523 Numeric; embedding vector dimensions V524 Numeric; embedding vector dimensions V525 Numeric; embedding vector dimensions V526 Numeric; embedding vector dimensions V527 Numeric; embedding vector dimensions V528 Numeric; embedding vector dimensions V529 Numeric; embedding vector dimensions V530 Numeric; embedding vector dimensions V531 Numeric; embedding vector dimensions V532 Numeric; embedding vector dimensions V533 Numeric; embedding vector dimensions V534 Numeric; embedding vector dimensions V535 Numeric; embedding vector dimensions V536 Numeric; embedding vector dimensions V537 Numeric; embedding vector dimensions V538 Numeric; embedding vector dimensions V539 Numeric; embedding vector dimensions V540 Numeric; embedding vector dimensions V541 Numeric; embedding vector dimensions V542 Numeric; embedding vector dimensions V543 Numeric; embedding vector dimensions V544 Numeric; embedding vector dimensions V545 Numeric; embedding vector dimensions V546 Numeric; embedding vector dimensions V547 Numeric; embedding vector dimensions V548 Numeric; embedding vector dimensions V549 Numeric; embedding vector dimensions V550 Numeric; embedding vector dimensions V551 Numeric; embedding vector dimensions V552 Numeric; embedding vector dimensions V553 Numeric; embedding vector dimensions V554 Numeric; embedding vector dimensions V555 Numeric; embedding vector dimensions V556 Numeric; embedding vector dimensions V557 Numeric; embedding vector dimensions V558 Numeric; embedding vector dimensions V559 Numeric; embedding vector dimensions V560 Numeric; embedding vector dimensions V561 Numeric; embedding vector dimensions V562 Numeric; embedding vector dimensions V563 Numeric; embedding vector dimensions V564 Numeric; embedding vector dimensions V565 Numeric; embedding vector dimensions V566 Numeric; embedding vector dimensions V567 Numeric; embedding vector dimensions V568 Numeric; embedding vector dimensions V569 Numeric; embedding vector dimensions V570 Numeric; embedding vector dimensions V571 Numeric; embedding vector dimensions V572 Numeric; embedding vector dimensions V573 Numeric; embedding vector dimensions V574 Numeric; embedding vector dimensions V575 Numeric; embedding vector dimensions V576 Numeric; embedding vector dimensions V577 Numeric; embedding vector dimensions V578 Numeric; embedding vector dimensions V579 Numeric; embedding vector dimensions V580 Numeric; embedding vector dimensions V581 Numeric; embedding vector dimensions V582 Numeric; embedding vector dimensions V583 Numeric; embedding vector dimensions V584 Numeric; embedding vector dimensions V585 Numeric; embedding vector dimensions V586 Numeric; embedding vector dimensions V587 Numeric; embedding vector dimensions V588 Numeric; embedding vector dimensions V589 Numeric; embedding vector dimensions V590 Numeric; embedding vector dimensions V591 Numeric; embedding vector dimensions V592 Numeric; embedding vector dimensions V593 Numeric; embedding vector dimensions V594 Numeric; embedding vector dimensions V595 Numeric; embedding vector dimensions V596 Numeric; embedding vector dimensions V597 Numeric; embedding vector dimensions V598 Numeric; embedding vector dimensions V599 Numeric; embedding vector dimensions V600 Numeric; embedding vector dimensions V601 Numeric; embedding vector dimensions V602 Numeric; embedding vector dimensions V603 Numeric; embedding vector dimensions V604 Numeric; embedding vector dimensions V605 Numeric; embedding vector dimensions V606 Numeric; embedding vector dimensions V607 Numeric; embedding vector dimensions V608 Numeric; embedding vector dimensions V609 Numeric; embedding vector dimensions V610 Numeric; embedding vector dimensions V611 Numeric; embedding vector dimensions V612 Numeric; embedding vector dimensions V613 Numeric; embedding vector dimensions V614 Numeric; embedding vector dimensions V615 Numeric; embedding vector dimensions V616 Numeric; embedding vector dimensions V617 Numeric; embedding vector dimensions V618 Numeric; embedding vector dimensions V619 Numeric; embedding vector dimensions V620 Numeric; embedding vector dimensions V621 Numeric; embedding vector dimensions V622 Numeric; embedding vector dimensions V623 Numeric; embedding vector dimensions V624 Numeric; embedding vector dimensions V625 Numeric; embedding vector dimensions V626 Numeric; embedding vector dimensions V627 Numeric; embedding vector dimensions V628 Numeric; embedding vector dimensions V629 Numeric; embedding vector dimensions V630 Numeric; embedding vector dimensions V631 Numeric; embedding vector dimensions V632 Numeric; embedding vector dimensions V633 Numeric; embedding vector dimensions V634 Numeric; embedding vector dimensions V635 Numeric; embedding vector dimensions V636 Numeric; embedding vector dimensions V637 Numeric; embedding vector dimensions V638 Numeric; embedding vector dimensions V639 Numeric; embedding vector dimensions V640 Numeric; embedding vector dimensions V641 Numeric; embedding vector dimensions V642 Numeric; embedding vector dimensions V643 Numeric; embedding vector dimensions V644 Numeric; embedding vector dimensions V645 Numeric; embedding vector dimensions V646 Numeric; embedding vector dimensions V647 Numeric; embedding vector dimensions V648 Numeric; embedding vector dimensions V649 Numeric; embedding vector dimensions V650 Numeric; embedding vector dimensions V651 Numeric; embedding vector dimensions V652 Numeric; embedding vector dimensions V653 Numeric; embedding vector dimensions V654 Numeric; embedding vector dimensions V655 Numeric; embedding vector dimensions V656 Numeric; embedding vector dimensions V657 Numeric; embedding vector dimensions V658 Numeric; embedding vector dimensions V659 Numeric; embedding vector dimensions V660 Numeric; embedding vector dimensions V661 Numeric; embedding vector dimensions V662 Numeric; embedding vector dimensions V663 Numeric; embedding vector dimensions V664 Numeric; embedding vector dimensions V665 Numeric; embedding vector dimensions V666 Numeric; embedding vector dimensions V667 Numeric; embedding vector dimensions V668 Numeric; embedding vector dimensions V669 Numeric; embedding vector dimensions V670 Numeric; embedding vector dimensions V671 Numeric; embedding vector dimensions V672 Numeric; embedding vector dimensions V673 Numeric; embedding vector dimensions V674 Numeric; embedding vector dimensions V675 Numeric; embedding vector dimensions V676 Numeric; embedding vector dimensions V677 Numeric; embedding vector dimensions V678 Numeric; embedding vector dimensions V679 Numeric; embedding vector dimensions V680 Numeric; embedding vector dimensions V681 Numeric; embedding vector dimensions V682 Numeric; embedding vector dimensions V683 Numeric; embedding vector dimensions V684 Numeric; embedding vector dimensions V685 Numeric; embedding vector dimensions V686 Numeric; embedding vector dimensions V687 Numeric; embedding vector dimensions V688 Numeric; embedding vector dimensions V689 Numeric; embedding vector dimensions V690 Numeric; embedding vector dimensions V691 Numeric; embedding vector dimensions V692 Numeric; embedding vector dimensions V693 Numeric; embedding vector dimensions V694 Numeric; embedding vector dimensions V695 Numeric; embedding vector dimensions V696 Numeric; embedding vector dimensions V697 Numeric; embedding vector dimensions V698 Numeric; embedding vector dimensions V699 Numeric; embedding vector dimensions V700 Numeric; embedding vector dimensions V701 Numeric; embedding vector dimensions V702 Numeric; embedding vector dimensions V703 Numeric; embedding vector dimensions V704 Numeric; embedding vector dimensions V705 Numeric; embedding vector dimensions V706 Numeric; embedding vector dimensions V707 Numeric; embedding vector dimensions V708 Numeric; embedding vector dimensions V709 Numeric; embedding vector dimensions V710 Numeric; embedding vector dimensions V711 Numeric; embedding vector dimensions V712 Numeric; embedding vector dimensions V713 Numeric; embedding vector dimensions V714 Numeric; embedding vector dimensions V715 Numeric; embedding vector dimensions V716 Numeric; embedding vector dimensions V717 Numeric; embedding vector dimensions V718 Numeric; embedding vector dimensions V719 Numeric; embedding vector dimensions V720 Numeric; embedding vector dimensions V721 Numeric; embedding vector dimensions V722 Numeric; embedding vector dimensions V723 Numeric; embedding vector dimensions V724 Numeric; embedding vector dimensions V725 Numeric; embedding vector dimensions V726 Numeric; embedding vector dimensions V727 Numeric; embedding vector dimensions V728 Numeric; embedding vector dimensions V729 Numeric; embedding vector dimensions V730 Numeric; embedding vector dimensions V731 Numeric; embedding vector dimensions V732 Numeric; embedding vector dimensions V733 Numeric; embedding vector dimensions V734 Numeric; embedding vector dimensions V735 Numeric; embedding vector dimensions V736 Numeric; embedding vector dimensions V737 Numeric; embedding vector dimensions V738 Numeric; embedding vector dimensions V739 Numeric; embedding vector dimensions V740 Numeric; embedding vector dimensions V741 Numeric; embedding vector dimensions V742 Numeric; embedding vector dimensions V743 Numeric; embedding vector dimensions V744 Numeric; embedding vector dimensions V745 Numeric; embedding vector dimensions V746 Numeric; embedding vector dimensions V747 Numeric; embedding vector dimensions V748 Numeric; embedding vector dimensions V749 Numeric; embedding vector dimensions V750 Numeric; embedding vector dimensions V751 Numeric; embedding vector dimensions V752 Numeric; embedding vector dimensions V753 Numeric; embedding vector dimensions V754 Numeric; embedding vector dimensions V755 Numeric; embedding vector dimensions V756 Numeric; embedding vector dimensions V757 Numeric; embedding vector dimensions V758 Numeric; embedding vector dimensions V759 Numeric; embedding vector dimensions V760 Numeric; embedding vector dimensions V761 Numeric; embedding vector dimensions V762 Numeric; embedding vector dimensions V763 Numeric; embedding vector dimensions V764 Numeric; embedding vector dimensions V765 Numeric; embedding vector dimensions V766 Numeric; embedding vector dimensions V767 Numeric; embedding vector dimensions V768 Numeric; embedding vector dimensions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/single_embedding_hf.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Single embedding result example from Hugging Face API — single_embedding_hf","text":"Generated using Hugging Face embedding model via EndpointR functions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Hugging Face classification response to tidy format — tidy_classification_response","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"Transforms nested JSON response Hugging Face classification endpoint tidy data frame one row columns classification label.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"","code":"tidy_classification_response(response)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"response Either httr2_response object Hugging Face API request parsed JSON object containing classification results","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"data frame one row columns classification label","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"function expects specific structure response, classification result containing 'label' 'score' field. flattens nested structure pivots data create wide-format data frame. function accepts either raw httr2_response object parsed JSON structure, making flexible different workflow patterns.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"","code":"if (FALSE) { # \\dontrun{   # Process response directly from API call   response <- hf_perform_request(req)   tidy_results <- tidy_classification_response(response)    # Or with an already-parsed JSON object   json_data <- httr2::resp_body_json(response)   tidy_results <- tidy_classification_response(json_data)    # Example of expected output structure   # A tibble: 1 × 2   #   positive negative   #      <dbl>    <dbl>   # 1    0.982    0.018 } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Process embedding API response into a tidy format — tidy_embedding_response","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"Converts nested list response Hugging Face Inference API embedding request tidy tibble.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"","code":"tidy_embedding_response(response)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"response httr2 response object parsed JSON response","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"tibble containing embedding vectors","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"","code":"if (FALSE) { # \\dontrun{   # Process response from httr2 request   req <- hf_build_request(text, endpoint_url, api_key)   resp <- httr2::req_perform(req)   embeddings <- tidy_embedding_response(resp)    # Process already parsed JSON   resp_json <- httr2::resp_body_json(resp)   embeddings <- tidy_embedding_response(resp_json) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","title":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","text":"Converts nested list response OpenAI embedding API request tidy tibble embedding vectors columns.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","text":"","code":"tidy_oai_embedding(response)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","text":"response httr2 response object parsed JSON response OpenAI's embedding API","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","text":"tibble containing embedding vectors columns (V1, V2, etc.) optionally oai_index column present response","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","text":"function handles single document batch embedding responses. extracts embedding vectors converts wide format tibble column (V1, V2, ..., Vn) represents one dimension embedding vector. response includes index information, adds oai_index column preserve ordering.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","text":"","code":"if (FALSE) { # \\dontrun{   # Process response from httr2 request   req <- oai_build_embedding_request(\"Hello world\")   resp <- httr2::req_perform(req)   embeddings <- tidy_oai_embedding(resp)    # Process already parsed JSON   resp_json <- httr2::resp_body_json(resp)   embeddings <- tidy_oai_embedding(resp_json) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"Checks endpoint URL valid accessible provided API key. function sends small test request verify endpoint works.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"","code":"validate_hf_endpoint(endpoint_url, key_name)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"logical TRUE endpoint valid, otherwise stops error","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"","code":"if (FALSE) { # \\dontrun{   # Validate endpoint retrieving API key from environment   validate_hf_endpoint(     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     key_name = \"HF_API_KEY\"   )    # Using default key name   validate_hf_endpoint(\"https://my-endpoint.huggingface.cloud\") } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate response data against schema — validate_response","title":"Validate response data against schema — validate_response","text":"Validate response data schema","code":""},{"path":"https://jpcompartir.github.io/EndpointR/news/index.html","id":"endpointr-011","dir":"Changelog","previous_headings":"","what":"EndpointR 0.1.1","title":"EndpointR 0.1.1","text":"oai_complete_chunks() function better support chunking/batching oai_complete_df() oai_complete_df() now writes file mitigate chance completely lost data","code":""},{"path":"https://jpcompartir.github.io/EndpointR/news/index.html","id":"endpointr-010","dir":"Changelog","previous_headings":"","what":"EndpointR 0.1.0","title":"EndpointR 0.1.0","text":"Initial BETA release, ships : Support embeddings classification Hugging Face Inference API & Dedicated Inference Endpoints Support text completion using OpenAI models via Chat Completions API Support embeddings OpenAI Embeddings API Structured outputs via JSON schemas validators","code":""}]
