[{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"commands","dir":"","previous_headings":"","what":"Commands","title":"EndpointR Development Guide","text":"Run tests: testthat::test() Run single test: testthat::test_file(\"tests/testthat/test-tmp.R\") Run specific test: testthat::test_that(\"test description\", { test code }) Check package: devtools::check() R CMD check Build package: devtools::build() R CMD build . Generate documentation: devtools::document()","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"language-style-guidelines","dir":"","previous_headings":"","what":"Language Style Guidelines","title":"EndpointR Development Guide","text":"Use British English Use straightforward language jargon","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"code-style-guidelines","dir":"","previous_headings":"","what":"Code Style Guidelines","title":"EndpointR Development Guide","text":"Use roxygen2 markdown documentation Follow tidyverse style: snake_case function names, descriptive variables Chain operations native pipe (|>) Use tibble/data.frame data structures appropriate Include parameter types roxygen docs Use tidyeval data frame functions: capture variable names rlang::ensym() (e.g., text_sym <- rlang::ensym(text_var)) non-standard evaluation Error handling: use stopifnot() assertions cli::cli_abort() user-facing errors Format error messages cli syntax ({.val}, {.code}) Wrap examples requiring API request \\dontrun{} Use explicit return values document Keep functions small focused single responsibility API keys saved environment variables, input raw code","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"comment-guidelines","dir":"","previous_headings":"","what":"Comment Guidelines","title":"EndpointR Development Guide","text":"Write comments explain made new choice, stuff like ‘Check required parameters’ - Let obvious code speak Use lowercase comments British English (let AI use uppercase distinguish - helpful know whether ’s human AI comment)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"api-consistency","dir":"","previous_headings":"","what":"API Consistency","title":"EndpointR Development Guide","text":"Functions R/core.R intended ‘quite’ general - .e. form foundations rest package Generally want deal single text, batch text, df texts consistently, least predictably differently, across providers tasks","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"ai-usage","dir":"","previous_headings":"","what":"AI Usage","title":"EndpointR Development Guide","text":"Don’t commit code don’t understand AI writes code, better write tests. AI writes tests, better write code. Let AI help docs, make sure keep --date. able know ’s code, docs.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 EndpointR authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"quick-start","dir":"Articles","previous_headings":"","what":"Quick start","title":"Managing API Keys Securely","text":"EndpointR two main functions managing API keys, get_api_key() set_api_key(), take ‘key_name’ input. security, set_api_key() uses askpass accept API keys rather code - means show .Rhistory, less likely leaked. First set API key: restart R session. Now get API key security reasons, print api_key console, pass function requires .","code":"set_api_key(\"TOY_API_KEY\") api_key <- get_api_key(\"TOY_API_KEY\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"what-is-an-api","dir":"Articles","previous_headings":"","what":"What is an API?","title":"Managing API Keys Securely","text":"API stands ‘Application Programming Interface’, mechanism two pieces software interact exchange data. APIs work requests - user sends request service, service sends response, fulfilling user’s request, providing error message. specific location within API allows user access service, function, called endpoint.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"what-is-an-api-key","dir":"Articles","previous_headings":"","what":"What is an API Key?","title":"Managing API Keys Securely","text":"many APIs provide access sensitive information, require credit card send requests, ’s important API providers able identify sending request, determine whether ’s safe respond, charge response. API key mechanism identifying sending request. See also: API keys","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"api-key-security","dir":"Articles","previous_headings":"","what":"API Key Security","title":"Managing API Keys Securely","text":"WARNING: EndpointR try help manage API keys safely, baseline level responsibility person using managed service needs take , set best practices need followed. paramount handle API keys securely. need avoid following things: Saving unencrypted API keys notes, emails, Google Docs etc. Uploading API keys web services (GitHub, etc.) Sharing API keys people Including unencrypted API keys code (scripts, R/Quarto/markdown files, .ipynb etc.) Unencypted API keys appearing .Rhistory file (especially file uploaded anywhere) suspect may done one - number - things, go directly got key, invalidate old one generate new one. key given , go directly person tell key compromised invalidate , provide new one. Instead , : Encrypt API keys Store environment variables using R/Rstudio/VScode Store managed secrets providers like GitHub use outside R/Rstudio/VScode","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"managing-multiple-keys","dir":"Articles","previous_headings":"","what":"Managing Multiple Keys","title":"Managing API Keys Securely","text":"endpoint EndpointR provides access , need correct environment variable stored .Renviron file API Key Lookup Table","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Using Hugging Face Inference Endpoints","text":"","code":"library(EndpointR) library(dplyr) library(tibble) library(httr2) library(purrr) library(tidyr) library(ggplot2)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"who-is-this-vignette-for","dir":"Articles","previous_headings":"Introduction","what":"Who is this vignette for?","title":"Using Hugging Face Inference Endpoints","text":"vignette people want embed classify data using pre-configured, dedicated Hugging Face Endpoints, assuming received API key endpoint URL/details. Experienced users looking fine-grained control API requests better served heading directly {httr2} package.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"what-is-hugging-face","dir":"Articles","previous_headings":"Introduction","what":"What is Hugging Face?","title":"Using Hugging Face Inference Endpoints","text":"past years Hugging Face de-facto location open-source AI. place save share datasets & Machine Learning models alike. Importantly, can train models upload Hugging Face Hub others use.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"what-are-dedicated-inference-endpoints","dir":"Articles","previous_headings":"Introduction","what":"What are Dedicated Inference Endpoints?","title":"Using Hugging Face Inference Endpoints","text":"model saved Hugging Face Hub can connected Hugging Face Inference Endpoint allowing us get predictions model using Hugging Face’s 1 hardware. Given modern-day Machine Learning models tend require large amount compute, everybody powerful laptops use models locally, democratising force useful tool disposal.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"common-use-cases","dir":"Articles","previous_headings":"Introduction > What are Dedicated Inference Endpoints?","what":"Common Use-cases","title":"Using Hugging Face Inference Endpoints","text":"Text embeddings via Sentence Transformers Sentiment analysis Text classification Image recognition Question answering Text generation vignette focuses using Hugging Face endpoints text embeddings.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"what-is-the-hugging-face-inference-api","dir":"Articles","previous_headings":"Introduction","what":"What is the Hugging Face Inference API?","title":"Using Hugging Face Inference Endpoints","text":"Hugging Face Inference API free service (paid options) provides access thousands models hosted Hub simple HTTP requests. Unlike Dedicated Endpoints, don’t rent specific hardware - instead, share computing resources users Hugging Face’s infrastructure. can lead wait times unknowable advance, production use-cases ’s generally recommended use Dedicated Inference Endpoints. However, casual usage testing purposes, Inference API suffice. TIP: EndpointR’s Hugging Face code work Dedicated Inference Endpoints Inference API simply changing URL.","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"api-key-management","dir":"Articles","previous_headings":"Getting Started - Dedicated Inference Endpoints","what":"API Key Management","title":"Using Hugging Face Inference Endpoints","text":"First read EndpointR API Keys vignette. Assuming endpoint set , ’ll need retrieve API key Hugging Face (team/department responsible providing API keys) store securely environment variable. set_api_key ask input value API key using {askpass}, reduces 2 likelihood API key ending code somewhere. EndpointR functions need API keys key_name argument, put name entered set_api_key EndpointR retrieve , without printing contents console.","code":"set_api_key(\"HF_TEST_API_KEY\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"quick-start---embeddings","dir":"Articles","previous_headings":"","what":"Quick Start - Embeddings","title":"Using Hugging Face Inference Endpoints","text":"EndpointR provides convenient functions common embedding tasks without exposing underlying request structure. EndpointR handle retries, errors, concurrent requests, batching requests configurable options.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"dedicated-inference-endpoints---embedding-a-single-text","dir":"Articles","previous_headings":"Quick Start - Embeddings","what":"Dedicated Inference Endpoints - Embedding a Single text","title":"Using Hugging Face Inference Endpoints","text":"example, previously set package-level key encrypting sensitive information, ENDPOINTR_KEY, can used {httr2}’s secret_encrypt secret_decrypt function encrypt information. example, can use functions encrypt API keys . Get endpoint’s URL Make sure key_name points correct API key endpoint Get single piece text want embed Call hf_embed_text() endpoint already active, take time initialise - model’s weights loaded onto server. process tends take ≈\\approx 20-30 seconds. Setting max_retries = 5 usually give endpoint enough time initialise, receive request return response. output tidied tibble 1 row 384 columns. column embedding dimension, ‘-minilm-l6-v2’ model used endpoint 384 dimensions. ##Inference API - Embedding Single Text switch URL send EndpointR functions, can get predictions Hugging Face Inference API instead Dedicated Inference Endpoints.","code":"endpoint_url <- httr2::secret_decrypt(\"kcZCsc92Ty7PuAk7_FdCcdOU_dlDpvWdDyfpwCWg-wW80eJxJPdQ68nz4V_0922SzSwM5_dYfpTOqsZ-GodUpLN4PQbwE73wlZkBCWIaIXc15g\", \"ENDPOINTR_KEY\")  embeddings <- hf_embed_text(   text = \"This is a sample text to embed\",   endpoint_url = endpoint_url,   key_name = \"HF_TEST_API_KEY\",   max_retries = 5 ) embeddings url_inference_api <-\"https://router.huggingface.co/hf-inference/models/sentence-transformers/all-mpnet-base-v2/pipeline/feature-extraction\"  embeddings <- hf_embed_text(   text = \"This is a sample text to embed\",   endpoint_url = url_inference_api,   key_name = \"HF_TEST_API_KEY\",   max_retries = 5 )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"working-with-multiple-texts","dir":"Articles","previous_headings":"Quick Start - Embeddings","what":"Working with Multiple Texts","title":"Using Hugging Face Inference Endpoints","text":"working multiple texts, possible create hf_embed_text function call text, save result variable: , many texts . get boring, quickly. Instead, nice embed multiple texts time.","code":"text_1_embeddings <- hf_embed_text(text = \"This is a sample text to embed\", endpoint_url = endpoint_url, key_name = \"HF_TEST_API_KEY\", max_retries = 5) text_2_embeddings <- hf_embed_text(text = \"This is a sample text nunber 2 to embed\", endpoint_url = endpoint_url, key_name = \"HF_TEST_API_KEY\", max_retries = 5)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"embedding-a-list-of-texts","dir":"Articles","previous_headings":"Quick Start - Embeddings","what":"Embedding a List of Texts","title":"Using Hugging Face Inference Endpoints","text":"Rather send 10 requests get 10 responses, batch texts together request multiple texts. following example ’ll send list 10 texts 2 separate requests. DISCLAIMER: Claude used generate sentences, instructed write 10 sentences embeddings, APIs, British, add humour saw fit. humour ‘mixed’ best. hf_embed_batch() function handles list texts, turns batches, sized according enter batch_size =. default value 8, ’ll change 5 send texts 2 requests. TIP: Remember, change endpoint_url url_inference_api send requests Hugging Face Inference API instead Dedicated Inference Endpoint. hf_embed_batch function return texts single data frame, text , two columns related errors, columns embedding dimension. .error .error_message introduced need know whether errors occur individual post level. example, batch 10 requests 2 errors, want see individual post errors, keep embeddings rest. Batching generally good idea, ’ll see later holding everything else equal, lead speed non-batched. However, try set batch_size high, likely hit rate limits, request rejected due payload large. TIP: Experiment find correct batch size data endpoint. Start small work upwards small incremeents.","code":"embedding_sentences <- c(   \"Text embedding models compress your rambling prose into compact vectors, much like how British commuters squeeze themselves into tube carriages during rush hour.\",   \"Setting up an inference endpoint without proper documentation is akin to attempting to navigate London using a map of Birmingham.\",   \"When your embedding model starts hallucinating, it's rather like watching Parliament during Question Time—entertaining but utterly unpredictable.\",   \"Optimising your inference endpoint is essential unless you fancy your users growing old whilst waiting for a response, perhaps even ageing enough to collect their pension.\",   \"The distance between word embeddings reveals semantic relationships, though sadly not the distance between what your client requested and what they actually wanted.\",   \"Creating multilingual embeddings is a bit like attempting to order tea properly across Europe—technically possible but fraught with cultural misunderstandings.\",   \"Batch processing through inference endpoints saves computing resources, much like how the British save conversation topics by discussing the weather exclusively.\",   \"Token limits on embedding APIs are the digital equivalent of a queue at a British post office—inevitably, you'll reach the front just as they close for lunch.\",   \"Fine-tuning embedding models on domain-specific corpora is rather like training a British child to apologise—it requires repetition, patience, and considerable examples.\",   \"When your inference endpoint crashes under load, it maintains that quintessentially British trait of breaking down precisely when most inconvenient.\" ) hf_embed_batch(   embedding_sentences,   endpoint_url,   \"HF_TEST_API_KEY\",   batch_size = 5, )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"embedding-a-data-frame-of-texts","dir":"Articles","previous_headings":"Quick Start - Embeddings","what":"Embedding a Data Frame of Texts","title":"Using Hugging Face Inference Endpoints","text":"data professionals, likely work data frames data structure. usually texts want embed, unique identifiers, selection columns work . EndpointR function - hf_embed_df() - built just use case. now, let’s create IDs build data frame sentences defined earlier: function requires input : text_var = column data frame text data need embed. id_var = column data frame identifies piece text - makes sure can link embeddings correct text. key_name ’ve seen endpoint_url ’ve seen optional arguments - optional default value don’t set . ’ll look optional arguments later section. key optional arguments hf_embed_df() : progress lets EndpointR know whether provide progress bar, really ’re worth ’re embedding lot data. now ’ll set FALSE max_retries tells EndpointR many times try repeat request moving next item. NOTE: ’ll look additional arguments, batch_size concurrent_requests later scaling performance now data frame original ID column, text column, ‘.error columns batching case, number columns ’V’ followed digits. digit represents ‘embedding dimension’, many ‘VXX’ columns embedding dimensions model using. can check whether errors number ways, like use dplyr::count() dplyr::filter(), ’s count way: need select columns contain embeddings - example want feed embeddings clustering, dimensionality reduction model, can select range columns using dplyr::select() - ’ll select V1:V384, gives embeddings. 768 embedding dimensions, input V1:V768, similarly 1024 dimensions, V1:V1024, get idea.","code":"embedding_ids <- 1:10  embedding_df <-   tibble(     id = embedding_ids,     sentence = embedding_sentences   ) (   result_df <- hf_embed_df(   df = embedding_df,   text_var = sentence,   id_var = id,   key_name = \"HF_TEST_API_KEY\",   endpoint_url = endpoint_url,   progress = FALSE,   max_retries = 5) ) count(result_df, .error) result_df |> select(V1:V384)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"improving-performance","dir":"Articles","previous_headings":"Quick Start - Embeddings","what":"Improving Performance","title":"Using Hugging Face Inference Endpoints","text":"section focus mainly hf_embed_df() function options improving throughput requests. far ’ve looked send single piece text, list batch texts, iterate data frame full texts, sending one request time. Whilst key workflows, ’re comfortable sending requests APIs receiving responses, ’ll probably need send bunch requests quickly, memory-efficient manner. get , let’s go bit . row data frame, need create request perform. need perform request sending information internet endpoint. endpoint accepts request, performs computation, sends successful response error. steps associated cost time, cost step equal - steps costly others. Creating request almost instant, larger data frame - e.g. 100,000 rows: creating 100,000 requests simultaneously take ≈\\approx 30s-90s. process , ’ll wait full amount time send requests receive responses. Clearly wasteful - created first request sent , wouldn’t waiting 90s just start sending requests. Sending request endpoint waiting process usually take lot longer preparing request. Therefore, can find ways reduce time spend waiting , able reduce overall time takes prepare send requests significantly. hf_embed_df function, main options increase number requests send , number texts send within request. concurrent_requests lets EndpointR know many requests send time. number 1 ~20-50 (extreme cases may ok 100, endpoints accept ) batch_size lets EndpointR know whether send requests individual texts, number texts ‘batched ’ together.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"concurrent-requests","dir":"Articles","previous_headings":"Quick Start - Embeddings","what":"Concurrent Requests","title":"Using Hugging Face Inference Endpoints","text":"Hugging Face’ Inference Endpoints can handle multiple request arriving time - configured ‘autoscale’ provide higher throughput backlog requests. rule, start ~5 concurrent requests, work ~20. start hitting rate limits go back ~10 find sweetspot. endpoint handling 20 requests continue increasing gradually ’s recommended unless sure endpoint can handle . send 5 concurrent requests - iterate data frame 5 rows time, send new requests responses returned, run data embed. Let’s benchmark performance (see appendix code) - overhead associated generating parallel requests, ’ll need bigger data frame understand type speed can get. Recording results: can see ~40% reduction processing time going 5-> 10 concurrent requests ~15% reduction going 10 -> 20 concurrent requests. *Exact times fluctuate, take approximates.","code":"hf_embed_df(   df = embedding_df,   text_var = sentence,   endpoint_url = endpoint_url,   key_name = \"HF_TEST_API_KEY\",   id_var= id,   concurrent_requests = 5,   progress = TRUE ) id_1000 <- 1:1000 sentences_1000 <- rep(embedding_df$sentence, 100) embedding_df_1000 <- tibble(id = id_1000, sentence = sentences_1000)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"batch-requests","dir":"Articles","previous_headings":"Quick Start - Embeddings","what":"Batch Requests","title":"Using Hugging Face Inference Endpoints","text":"endpoint allow multiple concurrent requests, ’s overloaded users, can still gain efficiency sending batches data frame request. hood looks quite similar section embedding list texts batch. TIP: experiment batch sizes find sweetspot - usually starting around 8-16, capping ~64. ’ll know ’ve gone high ’ll start seeing retry bar, /responses contain errors.","code":"hf_embed_df(   df = embedding_df_1000,   text_var = sentence,   endpoint_url = endpoint_url,   key_name = \"HF_TEST_API_KEY\",   id_var= id,   concurrent_requests = 5,   batch_size = 20,   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"batching-concurrent-requests","dir":"Articles","previous_headings":"Quick Start - Embeddings","what":"Batching Concurrent Requests","title":"Using Hugging Face Inference Endpoints","text":"maximum speed , can also send batches multiple concurrent requests. endpoint able handle , 10 concurrent requests batch_size = 10 marginally faster 10 concurrent requests batch_size = 5.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"benchmarking-batching-concurrent-requests","dir":"Articles","previous_headings":"Quick Start - Embeddings > Batching Concurrent Requests","what":"Benchmarking Batching & Concurrent Requests","title":"Using Hugging Face Inference Endpoints","text":"separate session benchmarking understand relationship batch size, concurrent requests, throughput, throughput number rows processed per second. looked combinations : batch_size = c(1, 4, 8, 16, 32) concurrent_requests = c(1, 5, 10, 15, 20) results effectively speak . embed ≈\\approx 2,000 documents sending 1 text 1 request time, get throughput approxapprox 2.16 texts per second, takes 15 minutes! end, 20 concurrent requests batch size 8, 20 concurrent requests batch size 32 throughout 195, close 200x quicker. get results back 10 seconds! NOTE: needed , re-run benchmarking code multiple times general trend clear.   parameter important speed, batch_size = concurrent_requests =? eye looks like concurrent_requests bigger effect, ’s clear parameters positive effect, within boundaries parameters, approximately linear relationship throughput. Putting together quick linear model confirms everything else equal, increasing concurrent_requests increases throughput batch_size QUESTION: might happen keep increasing batch_size concurrent_requests? relationship hold ?","code":"data(batch_concurrent_benchmark, package = \"EndpointR\")  knitr::kable(batch_concurrent_benchmark |> mutate_if(is.numeric, ~ round(.x, 2))) batch_concurrent_benchmark |>    mutate(batch_size = factor(batch_size), concurrent_requests= factor(concurrent_requests)) |>   ggplot(aes(x= batch_size, y = throughput, group = concurrent_requests)) +   geom_point(aes(colour = concurrent_requests)) +   geom_line(aes(colour = concurrent_requests)) +   labs(y = \"Throughput\", x = \"Batch Size\", title = \"Increasing `batch_size` and `concurrent_requests` increases throughput\") +   scale_colour_viridis_d() +   theme_minimal() +   theme(legend.position = \"bottom\") batch_concurrent_benchmark |>      mutate(batch_size = factor(batch_size),             concurrent_requests = factor(concurrent_requests)) |>       ggplot(aes(x = batch_size, y = concurrent_requests, fill = throughput)) +      geom_tile() +     geom_text(aes(label = round(throughput, 1)), colour = \"white\") +     scale_fill_viridis_c(name = \"Throughput\") +     theme_minimal() +     labs(x = \"Batch Size\", y = \"Concurrent Requests\",          title = \"Throughput by Batch Size and Concurrent Requests\") +     theme(legend.position = \"none\") model <- lm(throughput ~ batch_size + concurrent_requests, data = batch_concurrent_benchmark)  broom::tidy(model) |>     mutate(across(c(estimate, std.error, statistic), ~round(.x, 2))) #> # A tibble: 3 × 5 #>   term                estimate std.error statistic   p.value #>   <chr>                  <dbl>     <dbl>     <dbl>     <dbl> #> 1 (Intercept)             3.34     15.5       0.22 0.832     #> 2 batch_size              2.4       0.71      3.37 0.00366   #> 3 concurrent_requests     6.45      1.12      5.77 0.0000224"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"advanced-usage","dir":"Articles","previous_headings":"Quick Start - Embeddings","what":"Advanced Usage","title":"Using Hugging Face Inference Endpoints","text":"previous sections explored high-level API single texts, lists texts, data frames. Sometimes need something hf_embed_text, hf_embed_batch hf_embed_df functions allow, ‘helpful defaults’ get way. generally recommend go direct {httr2}, ’re intent using EndpointR material may forthcoming lower-level functions, can’t wait head repo explore functions !","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"quick-start---classification","dir":"Articles","previous_headings":"","what":"Quick Start - Classification","title":"Using Hugging Face Inference Endpoints","text":"TIP: section vignette assume familiarity section embeddings, .e. ’s expected know Hugging Face Inference Endpoint , various options common package like ‘concurrent_requests’, batch_size’ etc. Similarly embeddings section, EndpointR provides 3 separate high-level functions try help classify text without exposing underlying request structure. hf_classify_text() hf_classify_batch() hf_classify_df() one text: list texts: data frame texts IDs:","code":"endpoint_url <- httr2::secret_decrypt(\"c2nF_Tx_4V32AO0PZOohHvmKqXZ4Ysd0UfqL1loDrge5eIbLIF07ynGstlRtRJHUgeaMVROKOgnuiWs66VYqLM3SFIufcFv9vIDwOSQyTB0B5A\", \"ENDPOINTR_KEY\")  text <- \"Annual income twenty pounds, annual expenditure nineteen nineteen and six, result happiness. Annual income twenty pounds, annual expenditure twenty pounds ought and six, result misery.\"  hf_classify_text(   text,   endpoint_url,   \"HF_TEST_API_KEY\" ) texts <- c(   \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness.\",   \"Whether I shall turn out to be the hero of my own life, or whether that station will be held by anybody else, these pages must show.\",   \"There is nothing in the world so irresistibly contagious as laughter and good humour.\",   \"The pain of parting is nothing to the joy of meeting again.\",   \"No one is useless in this world who lightens the burdens of another.\" )  hf_classify_batch(   texts,   endpoint_url,   \"HF_TEST_API_KEY\",   batch_size = 2 ) text_df <- tibble(   text = texts ) |>    mutate(id = row_number(), .before = 1)  hf_classify_df(   text_df,   text,    id,   endpoint_url,   \"HF_TEST_API_KEY\" )"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"appendix---embeddings","dir":"Articles","previous_headings":"Appendix","what":"Appendix - Embeddings","title":"Using Hugging Face Inference Endpoints","text":"TODO: date post-refactoring ### Benchmarking Concurrent Requests","code":"run_benchmark <- function(num_concurrent, data, endpoint, key) {   start_time <- Sys.time()   res_df <- try(hf_embed_df(                   df = data,                   text_var = sentence,                    id_var= id,                            endpoint_url = endpoint,                   key_name = key,                   include_errors = FALSE,                    concurrent_requests = num_concurrent,                   progress = FALSE                  ), silent = TRUE)    processing_time <- Sys.time() - start_time   success <- !inherits(res_df, \"try-error\") && nrow(res_df) == nrow(data)   return(data.frame(     concurrent_requests = num_concurrent,     processing_time_secs = as.numeric(processing_time, units = \"secs\"),     success = success   )) }  num_requests_vec <- c(5, 10, 20) results_list <- lapply(num_requests_vec, function(n) {   run_benchmark(     num_concurrent = n,     data = embedding_df_1000,     endpoint = endpoint_url,     key = \"HF_TEST_API_KEY\"    ) })  (summary_df <- do.call(rbind, results_list))"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"benchmarking-batch-and-concurrent-requests","dir":"Articles","previous_headings":"Appendix > Appendix - Embeddings","what":"Benchmarking Batch and Concurrent Requests","title":"Using Hugging Face Inference Endpoints","text":"won’t able re-run exactly -, data frame provided package. bring data wanted .","code":"trust <- readr::read_csv(\"~/data/trust/trust_slice_spam_classification.csv\") |>    select(text) |>   mutate(id = row_number()) |>   filter(!is.na(text), text != \"\") # stop NAs and empty vals crashing anything  chunk_size <- 2000 total_chunks <- 20  # this chunking logic is actually rubbish. trust_chunks <- trust |>   mutate(chunk_id = ceiling(id / chunk_size)) |>   group_split(chunk_id) |>   head(total_chunks)  benchmark_params <- crossing(   batch_size = c(1, 4, 8, 16, 32),   concurrent_requests = c(1, 5, 10, 20) ) |>   mutate(chunk_index = row_number() %% total_chunks + 1)  benchmark_results <- benchmark_params |>   mutate(result = pmap(list(batch_size, concurrent_requests, chunk_index), function(bs, cr, ci) {     current_chunk <- trust_chunks[[ci]]      start_time <- Sys.time()      res <- try(hf_embed_df(       df = current_chunk,       text_var = text,       id_var = id,       endpoint_url = endpoint_url,       key_name = \"HF_TEST_API_KEY\",       batch_size = bs,       concurrent_requests = cr,       progress = TRUE     ), silent = TRUE)      elapsed_time <- as.numeric(Sys.time() - start_time, units = \"secs\")     success <- !inherits(res, \"try-error\")     rows_processed <- if(success) nrow(current_chunk) else 0      list(       elapsed_time = elapsed_time,       success = success,       rows_processed = rows_processed,       throughput = if(success) rows_processed / elapsed_time else 0     )   })) |>   nest_wider(result)"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"openai---the-basics","dir":"Articles","previous_headings":"","what":"OpenAI - The Basics","title":"Connecting to Major Model Providers","text":"get started, need make sure couple things place: First, get API key store “OPENAI_API_KEY” set_api_key() Second, figure need Responses API Chat Completions API Third, choose model - EndpointR configured select smaller, cheaper model free choose specific models Information: unsure whether prefer Completions API Responses API, checkout webpage","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"completions-api","dir":"Articles","previous_headings":"OpenAI - The Basics","what":"Completions API","title":"Connecting to Major Model Providers","text":"“Chat Completions API endpoint generate model response list messages comprising conversation.” use-cases EndpointR covers, conversation single interaction user model. Usually goal achieve narrow task repeatedly. TIP: looking persistent, open-ended chats LLMs, ’s best head Claude, ChatGPT, Gemini - provider choice. type tasks might use Completions API ? Document classification, e.g. Sentiment Analysis Translation Structured data extraction","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"sentiment-analysis","dir":"Articles","previous_headings":"OpenAI - The Basics > Completions API","what":"Sentiment Analysis","title":"Connecting to Major Model Providers","text":"Whilst generally recommend using OpenAI’s models sentiment analysis, 1 task people familiar . set basic 2 system prompt tell LLM want text send : can create request inspect {httr2}’s req_dry_run() function - ’s good hygiene check request formatted expect , particularly using first time. generated HTTP request look something like : EndpointR handles HTTP mechanics: authentication, request headers, endpoint configuration. constructs JSON payload specified model, system prompt, user message, whilst setting sensible defaults temperature 3 max_tokens 4. demonstrative purposes, ran prompt data twice (see ) First run: “sentiment text frustrated exasperated.” Second run: “sentiment text quite negative. speaker appears frustrated exasperated, expressing dissatisfaction situation.” Whilst responses directionally/approximately accurate, least qualitative sense, inconsistent output can unpleasant work . wanted response conform usual sentiment categories, need write custom parser extract ‘negative’, ‘neutral’, ‘positive’ output. Looking first output, ’s clear parser need reasonably sophisticated. send another request, asking model please output ‘positive’, ‘negative’, ‘neutral’ 5. Clearly work willing . ’ll look techniques deal systematically, achieve predictable outputs Structured Outputs section. model hand text difficult traditional, three-category 6, document-level sentiment analysis? [1] “sentiment text mixed, positive feelings expressed interface (”brilliant”) negative feelings performance (“absolutely dreadful”).” model smart enough recognise sentiment fit neatly ‘positive’, ‘negative’, ‘neutral’ output formatted nice way downstream use.","code":"system_prompt <- \"Analyse the text's sentiment: \"  text <- \"Oh man, I'm getting to the end of my limit with this thing. WHY DOESN'T IT JUST WORK?!?\" sentiment_request <- oai_build_completions_request(   text,   system_prompt = system_prompt )   sentiment_request |>   req_dry_run() POST /v1/chat/completions HTTP/1.1 accept: */* accept-encoding: deflate, gzip authorization:  content-length: 247 content-type: application/json host: api.openai.com user-agent: EndpointR  {   \"model\": \"gpt-4.1-nano\",   \"messages\": [     {       \"role\": \"system\",       \"content\": \"Analyse the text's sentiment: \"     },     {       \"role\": \"user\",       \"content\": \"Oh man, I'm getting to the end of my limit with this thing. WHY DOESN'T IT JUST WORK?!?\"     }   ],   \"temperature\": 0,   \"max_tokens\": 500 } sentiment_response <- sentiment_request |>    perform_request_or_return_error() sentiment_response |>   resp_check_status() |>    resp_body_json() |>    pluck(\"choices\", 1, \"message\", \"content\") ambiguous_text <- \"The interface is brilliant but the performance is absolutely dreadful\"  ambiguous_sentiment <- oai_build_completions_request(   ambiguous_text,   system_prompt = system_prompt ) |>    perform_request_or_return_error() |>    resp_body_json() |>    pluck(\"choices\", 1, \"message\", \"content\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"multiple-texts","dir":"Articles","previous_headings":"OpenAI - The Basics > Completions API","what":"Multiple Texts","title":"Connecting to Major Model Providers","text":"Individual requests useful applications EndpointR built handle , whilst taking care implementation details like concurrent requests, retries, failing gracefully. Let’s experiment better system prompt list texts: running testing, took 8.5 seconds 10 requests send one time. took 1.8 seconds send 10 requests parallel - ~4.7x speed , showing overhead cost sending requests parallel - .e. see 10x speed increase 10x requests. Now extract content response, can see 7 response classification belonging classes - making prompt slightly better helped us get better final result. However, repeat many texts - hundreds/thousands, ’s unlikely every single response conform categories. Without instruction, models tendency slightly change output unpredictable intervals.","code":"updated_sentiment_prompt <- \"Classify the text into sentiment categories. The accepted categories are 'positive', 'negative', 'neutral', and 'mixed'. A 'mixed' text contains elements of positive and negative. \" classical_texts <- c(  \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",  \"All happy families are alike; each unhappy family is unhappy in its own way.\",  \"It was the best of times, it was the worst of times.\",  \"Call me Ishmael.\",  \"The sun shone, having no alternative, on the nothing new.\",  \"All animals are equal, but some animals are more equal than others.\",  \"So we beat on, boats against the current, borne back ceaselessly into the past.\",  \"The heart was made to be broken.\",  \"Tomorrow, and tomorrow, and tomorrow, creeps in this petty pace from day to day.\",  \"I have always depended on the kindness of strangers.\" ) classical_requests <- oai_build_completions_request_list(   classical_texts,   system_prompt = updated_sentiment_prompt ) start_seq <- Sys.time() classical_responses  <- classical_requests |>    perform_requests_with_strategy() end_seq <- Sys.time() - start_seq start_par <- Sys.time() classical_responses  <- classical_requests |>    perform_requests_with_strategy(concurrent_requests = 10) end_par <- Sys.time() - start_par classical_responses |>    map(~ resp_body_json(.x) |>          pluck(\"choices\", 1, \"message\", \"content\") |>          as_tibble()   ) |>    list_rbind() |>    mutate(text = classical_texts, .before = 1)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"data-frame-of-texts","dir":"Articles","previous_headings":"OpenAI - The Basics > Completions API","what":"Data Frame of Texts","title":"Connecting to Major Model Providers","text":"","code":"mapped_responses <- tibble(   text = classical_texts ) |>    mutate(request = map(text, ~ oai_build_completions_request(input = .x, system_prompt = updated_sentiment_prompt)   )) |>    mutate(response = map(request, perform_request_or_return_error, .progress = TRUE)) pluck_text <- function(x) pluck(x, \"choices\", 1, \"message\", \"content\")  mapped_responses |>    pluck(\"response\", 1) |>    resp_body_json() |>    pluck_text()"},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"openai-structured-outputs","dir":"Articles","previous_headings":"OpenAI - The Basics","what":"OpenAI Structured Outputs","title":"Connecting to Major Model Providers","text":"textual responses get LLM providers difficult deal programmatically, make guarantees form response. example, ask LLM classify documents, sometimes give just classification - ‘positive’, sometimes add pre-amble ‘document positive’, sometimes something else entirely. detailed information creating JSON schemas structured outputs, see vignette(\"structured_outputs_json_schema\").","code":""},{"path":[]},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"introduction-to-structured-outputs","dir":"Articles","previous_headings":"","what":"Introduction to Structured Outputs","title":"Structured Outputs - JSON Schema","text":"Structured outputs ensure LLMs return data exactly format need. Instead parsing messy text, get validated JSON matching schema. use structured outputs? - eliminates parsing errors inconsistent formats - guarantees data types (numbers vs strings, booleans vs text) - enables reliable data extraction pipelines - reduces prompt engineering overhead Basic workflow: 1. define schema using helper functions 2. send requests OpenAI schema passed 3. validate response validate_response()","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"quick-start","dir":"Articles","previous_headings":"","what":"Quick Start","title":"Structured Outputs - JSON Schema","text":"WARNING: using schema structured outputs OpenAI, fields MUST required: fields must  required use Structured Outputs, fields function parameters must specified  required NOTE: first time send request schema, take longer usual. “Typical schemas take 10 seconds process first request, complex schemas may take minute.” 1","code":"contact_schema <- create_json_schema(  name = \"contact_info\",  schema = schema_object(    name = schema_string(\"person's full name\"),    email = schema_string(\"email address\"),    phone = schema_string(\"phone number\"),    required = list(\"name\", \"email\", \"phone\"),    additional_properties = FALSE  ) )  req <- oai_build_completions_request(   input = \"Am I speaking with Margaret Phillips? Yes, ok, and your email is mphil@hotmail.co.uk. Ok perfect, and your phone number? Was that 07564789789? Ok great. Just a second please Margaret, you're verified\",   schema = contact_schema )  resp <- req_perform(req)  resp |>   resp_body_json() |>    pluck(\"choices\", 1, \"message\", \"content\") |>    validate_response(schema = contact_schema) |>    tibble::as_tibble()"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"schema-types","dir":"Articles","previous_headings":"","what":"Schema Types","title":"Structured Outputs - JSON Schema","text":"EndpointR provides several schema types match different data extraction needs. type enforces specific constraints validations: schema_string() - text data schema_number() / schema_integer() - numeric values optional min/max schema_boolean() - true/false values schema_enum() - predefined choices schema_array() - lists items schema_object() - nested structures Let’s explore type practical examples: can inspect schema print human-readable form json_dump() jsonlite::toJSON() Another, complicated, schema product review extraction, introduce schema_array:","code":"# text classification with enums sentiment_schema <- create_json_schema(   name = \"sentiment_analysis\",   schema = schema_object(     sentiment = schema_enum(       c(\"positive\", \"negative\", \"neutral\"),       \"overall sentiment of text\"     ),     confidence = schema_number(       \"confidence score\",        minimum = 0,        maximum = 1     ),     is_spam = schema_boolean(\"contains spam content\"),     required = c(\"sentiment\", \"confidence\", \"is_spam\")   ) ) json_dump(sentiment_schema) |>    jsonlite::toJSON(pretty = TRUE, auto_unbox = TRUE) rating_schema <- create_json_schema(   name = \"product_review\",   schema = schema_object(     rating = schema_integer(\"star rating\", minimum = 1, maximum = 5),     title = schema_string(\"review title\"),     pros = schema_array(       schema_string(),       \"positive aspects mentioned\"     ),     cons = schema_array(       schema_string(),       \"negative aspects mentioned\"     ),     would_recommend = schema_boolean(\"recommends product\"),     required = c(\"rating\",\"title\", \"pros\", \"cons\", \"would_recommend\")   ) )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"complex-nested-structures","dir":"Articles","previous_headings":"","what":"Complex Nested Structures","title":"Structured Outputs - JSON Schema","text":"Finally fairly complex schema - supplier schema_object within schema_object, line_items sits within schema_object, schema_array, schema_object, multiple schema_* objects.","code":"# invoice parsing with line items invoice_schema <- create_json_schema(   name = \"invoice_data\",   schema = schema_object(     # header information     invoice_number = schema_string(\"invoice reference number\"),     issue_date = schema_string(\"date issued (YYYY-MM-DD format)\"),     due_date = schema_string(\"payment due date (YYYY-MM-DD format)\"),     # billing details     supplier = schema_object(       name = schema_string(\"supplier company name\"),       address = schema_string(\"supplier address\"),       vat_number = schema_string(\"VAT registration number\"),       required = c(\"name\")     ),     customer = schema_object(       name = schema_string(\"customer name\"),       address = schema_string(\"customer address\"),       required = c(\"name\")     ),     # line items array     line_items = schema_array(       schema_object(         description = schema_string(\"item description\"),         quantity = schema_integer(\"quantity ordered\", minimum = 1),         unit_price = schema_number(\"price per unit\", minimum = 0),         line_total = schema_number(\"total for this line\", minimum = 0),         required = c(\"description\", \"quantity\", \"unit_price\", \"line_total\")       ),       \"invoice line items\",       min_items = 1     ),     # totals     subtotal = schema_number(\"subtotal before tax\", minimum = 0),     vat_amount = schema_number(\"VAT amount\", minimum = 0),     total_amount = schema_number(\"final total amount\", minimum = 0)   ) )  invoice_schema@schema$required <-  names(invoice_schema@schema$properties)  # This helper line ensures ALL properties are marked as required, which is   mandatory for OpenAI's structured outputs. Without this, the API will reject  the schema. Use this pattern when you want all fields to be required rather  than listing them individually."},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"validation","dir":"Articles","previous_headings":"","what":"Validation","title":"Structured Outputs - JSON Schema","text":"schema type enforces specific constraints. method validating whether specific responses meet schema’s constraints.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"how-validation-works","dir":"Articles","previous_headings":"Validation","what":"How Validation Works","title":"Structured Outputs - JSON Schema","text":"using structured outputs LLM providers: API-side enforcement: provider ensures generated responses match schema Local validation: - validate_response() double-checks data integrity locally dual approach catches generation errors data transmission issues. ’s comprehensive example: QUESTION: ’s wrong schema want use OpenAI API? properties set required fields, meaning use schema OpenAI’s structured outputs. Now let’s see happens try validate mocked response object conforms schema: try validate mocked response object conform schema:","code":"user_profile_schema <- create_json_schema(  name = \"user_profile\",  schema = schema_object(    # string fields    name = schema_string(\"full name\"),    bio = schema_string(\"user biography\"),        # numeric fields    age = schema_integer(\"age in years\", minimum = 13, maximum = 120),    account_balance = schema_number(\"balance in pounds\", minimum = 0),    is_verified = schema_boolean(\"account verified status\"),    newsletter_opt_in = schema_boolean(\"subscribed to newsletter\"),    subscription_tier = schema_enum(      c(\"free\", \"premium\", \"enterprise\"),      \"subscription level\"),    priority = schema_enum(      c(1, 2, 3),      \"support priority level\",      type = \"integer\"    ),    interests = schema_array(      schema_string(),      \"user interests\",      min_items = 1,      max_items = 10    ),        required = c(\"name\", \"age\", \"is_verified\", \"subscription_tier\")  ) ) valid_user <- '{   \"name\": \"Alice Smith\",   \"age\": 28,   \"account_balance\": 156.75,   \"is_verified\": true,   \"newsletter_opt_in\": false,   \"subscription_tier\": \"premium\",   \"priority\": 2,   \"interests\": [\"data science\", \"functional programming\", \"statistics\"] }'  validated_data <- validate_response(user_profile_schema, valid_user) str(validated_data) invalid_age <- '{   \"name\": \"Young User\",   \"age\": 10,   \"is_verified\": true,   \"subscription_tier\": \"free\" }'  validate_response(user_profile_schema, invalid_age)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"working-with-s7-objects","dir":"Articles","previous_headings":"","what":"Working with S7 Objects","title":"Structured Outputs - JSON Schema","text":"EndpointR uses S7 objects schema system. provides better type safety validation, means familiar S3 methods won’t work expected. Understanding work objects help debug issues customise schemas. example, call: jsonlite::toJSON(contact_schema). try, ’ll get error: Error: method asJSON S3 class: S7_object Instead, use S7 method json_dump defined json_schema class. converts schema R list ready converted JSON. won’t print list -long ugly, instead can check structure: Alongside json_dump EndpointR::json_schema given validate_response method saw used earlier quickstart.","code":"contact_json_dump <- json_dump(contact_schema) str(contact_json_dump)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"converting-schema-objects-to-json","dir":"Articles","previous_headings":"Working with S7 Objects","what":"Converting Schema Objects to JSON","title":"Structured Outputs - JSON Schema","text":"can convert dumped schema JSON object using {jsonlite}‘s toJSON function. object now class ’json’. can convert schema back regular R list {jsonlite}’s fromJSON function:","code":"contact_json_schema <-    toJSON(contact_json_dump,                   pretty = TRUE,                   auto_unbox = TRUE)  class(contact_json_schema) {   \"type\": \"json_schema\",   \"json_schema\": {     \"name\": \"contact_info\",     \"schema\": {       \"type\": \"object\",       \"properties\": {         \"name\": {           \"type\": \"string\",           \"description\": \"person's full name\"         },         \"email\": {           \"type\": \"string\",           \"description\": \"email address\"         },         \"phone\": {           \"type\": \"string\",           \"description\": \"phone number\"         }       },       \"additionalProperties\": false,       \"required\": [\"name\", \"email\", \"phone\"]     },     \"strict\": true   } } from_contact_json_schema <- fromJSON(contact_json_schema)  class(from_contact_json_schema)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best Practices","title":"Structured Outputs - JSON Schema","text":"Schema design principles: Use descriptive field names descriptions Set appropriate constraints (min/max values, required fields) Prefer enums free text categories Nest objects logically complex data Validate mock responses advance","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"troubleshooting-tips","dir":"Articles","previous_headings":"","what":"Troubleshooting tips:","title":"Structured Outputs - JSON Schema","text":"Use json_dump() inspect final schema structure use jsonlite::toJSON(x, pretty = TRUE) view schema human-readable form Test schemas mock data using validate_response() Start simple add complexity incrementally Check enum values match expected model outputs Validate required fields cover essential data Make sure properties required using OpenAI API structured outputs","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jack Penzer. Author, maintainer.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Penzer J (2025). EndpointR: Connects various Machine Learning inference providers. R package version 0.0.0.9000, https://jpcompartir.github.io/EndpointR/.","code":"@Manual{,   title = {EndpointR: Connects to various Machine Learning inference providers},   author = {Jack Penzer},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://jpcompartir.github.io/EndpointR/}, }"},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"endpointr","dir":"","previous_headings":"","what":"EndpointR","title":"EndpointR","text":"EndpointR ‘batteries included’, open-source R package connecting various Application Programming Interfaces (APIs) Machine Learning model predictions. TIP: experienced programmer, experience hitting APIs, consider going directly httr2","code":""},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"EndpointR","text":"EndpointR put CRAN, can download install latest development version following code:","code":"library(EndpointR) remotes::install_github(\"jpcompartir/EndpointR\")"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"hugging-face---embeddings","dir":"","previous_headings":"","what":"Hugging Face - embeddings","title":"EndpointR","text":"Securely set API key: Embed single text: Embed list texts batches: Embed data frame texts:","code":"set_api_key(\"HF_API_KEY\")  endpoint_url <- httr2::secret_decrypt(\"kcZCsc92Ty7PuAk7_FdCcdOU_dlDpvWdDyfpwCWg-wW80eJxJPdQ68nz4V_0922SzSwM5_dYfpTOqsZ-GodUpLN4PQbwE73wlZkBCWIaIXc15g\", \"ENDPOINTR_KEY\") # encrypted - for demo purposes embeddings <- hf_embed_text(   text = \"Convert this text to embeddings\",   endpoint_url = endpoint_url,   key_name = \"HF_API_KEY\" ) results <- hf_embed_batch(   texts = c(\"Text one\", \"Text two\", \"Text three\"),   endpoint_url = endpoint_url,   key_name = \"HF_API_KEY\",   batch_size = 10,   concurrent_requests = 5 ) df_with_embeddings <- hf_embed_df(   df = your_data,   text_var = review_text,   id_var = review_id,   endpoint_url = endpoint_url,   key_name = \"HF_API_KEY\" )"},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"hugging-face---classification","dir":"","previous_headings":"","what":"Hugging Face - Classification","title":"EndpointR","text":"Classify single text: Classify data frame: Read Hugging Face Inference Vignette infromation embedding classifying using Dedicated Inference Endpoints Inference API Hugging Face.","code":"classification <- hf_classify_text(   text = \"This restaurant has terrible service\",   endpoint_url = your_classification_endpoint,   key_name = \"HF_API_KEY\" ) classified_df <- hf_classify_df(   df = customer_reviews,   text_var = review_text,   id_var = review_id,   endpoint_url = your_classification_endpoint,   key_name = \"HF_API_KEY\",   batch_size = 8,   concurrent_requests = 3 )"},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"openai---chat-completions-api","dir":"","previous_headings":"","what":"OpenAI - Chat Completions API","title":"EndpointR","text":"Complete single text: Complete Data Frame texts: Read LLM Providers Vignette, Structured Outputs Vignette information common workflows OpenAI Chat Completions API 1","code":"library(EndpointR)  set_api_key(\"OPENAI_API_KEY\")  oai_complete_text(input = \"Oh man, that's absolutely terrible.\",                   system_prompt = \"Classify the sentiment of the following text: \") oai_complete_df(   df = customer_reviews,   text_var = review_text,   id_var = review_id,   system_prompt = \"Classify the following review:\",   key_name = \"OPENAI_API_KEY\",   concurrent_requests = 5 # send 5 rows of data simultaneously )"},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"api-key-security","dir":"","previous_headings":"","what":"API Key Security","title":"EndpointR","text":"Read httr2 vignette managing API keys securely encrypting . Read EndpointR API Keys vignette information API keys need wach endpoint support, securely import API keys .Renvironfile.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/EndpointR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"EndpointR: Connects to various Machine Learning inference providers — EndpointR-package","title":"EndpointR: Connects to various Machine Learning inference providers — EndpointR-package","text":"EndpointR 'batteries included', open-source R package connecting various APIs Machine Learning model predictions. EndpointR built company-specific use cases, may useful wide audience.","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/reference/EndpointR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"EndpointR: Connects to various Machine Learning inference providers — EndpointR-package","text":"Maintainer: Jack Penzer Jack.penzer@sharecreative.com","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.html","id":null,"dir":"Reference","previous_headings":"","what":"Batch concurrent benchmark results — batch_concurrent_benchmark","title":"Batch concurrent benchmark results — batch_concurrent_benchmark","text":"Benchmark data comparing different batch sizes concurrent request configurations Hugging Face API calls.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Batch concurrent benchmark results — batch_concurrent_benchmark","text":"","code":"batch_concurrent_benchmark"},{"path":"https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Batch concurrent benchmark results — batch_concurrent_benchmark","text":"data frame 20 rows 7 variables: batch_size Integer; size batch processed concurrent_requests Integer; number concurrent requests chunk_index Integer; index processed chunk elapsed_time Numeric; time taken seconds success Logical; whether operation succeeded rows_processed Integer; number rows successfully processed throughput Numeric; requests processed per second","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Batch concurrent benchmark results — batch_concurrent_benchmark","text":"Internal benchmarking EndpointR functions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.html","id":null,"dir":"Reference","previous_headings":"","what":"Split a data frame into chunks for batch processing — chunk_dataframe","title":"Split a data frame into chunks for batch processing — chunk_dataframe","text":"Splits data frame chunks specified size.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split a data frame into chunks for batch processing — chunk_dataframe","text":"","code":"chunk_dataframe(df, chunk_size)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split a data frame into chunks for batch processing — chunk_dataframe","text":"df data frame split batches chunk_size Number rows per batch","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split a data frame into chunks for batch processing — chunk_dataframe","text":"list data frames, chunk_size rows","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/create_json_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a JSON Schema object — create_json_schema","title":"Create a JSON Schema object — create_json_schema","text":"Create JSON Schema object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/create_json_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a JSON Schema object — create_json_schema","text":"","code":"create_json_schema(name, schema, strict = TRUE, description = \"\")"},{"path":"https://jpcompartir.github.io/EndpointR/reference/create_json_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a JSON Schema object — create_json_schema","text":"name Name schema schema JSON schema definition list strict Whether enforce strict mode (default TRUE) description Optional description schema","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/create_json_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a JSON Schema object — create_json_schema","text":"json_schema S7 object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"Retrieve API key .Renviron. API key set via set_api_key()manually placed .Renviron file.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"","code":"get_api_key(key_name)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"key_name name API format \"ENDPOINT_API_KEY\" -> \"ANTHROPIC_API_KEY\"","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"character","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"","code":"if (FALSE) { # \\dontrun{   # retrieve an Anthropic API key   anthropic_key <- get_api_key(\"ANTHROPIC_API_KEY\")    # use the key in a function call   my_function(api_key = anthropic_key) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare a single text embedding request — hf_build_request","title":"Prepare a single text embedding request — hf_build_request","text":"Creates httr2 request object obtaining response Hugging Face Inference endpoint single text input. function can used multiple tasks, .e. embedding input, classifying input","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare a single text embedding request — hf_build_request","text":"","code":"hf_build_request(   input,   endpoint_url,   key_name,   parameters = list(),   max_retries = 5,   timeout = 10,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare a single text embedding request — hf_build_request","text":"input Character string get response endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key parameters Advanced usage: parameters pass API endpoint max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating request","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare a single text embedding request — hf_build_request","text":"httr2 request object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare a single text embedding request — hf_build_request","text":"developers, function can form basis single requests, mapped list requests.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare a single text embedding request — hf_build_request","text":"","code":"if (FALSE) { # \\dontrun{   # Create request using API key from environment   req <- hf_build_request(     input = \"This is a sample text to embed\",     endpoint_url = \"https://my-endpoint.huggingface.cloud/embedding_api\",     key_name = \"HF_API_KEY\"   )    # Using default key name   req <- hf_build_request(     input = \"This is a sample text to classify\",     endpoint_url = \"https://my-endpoint.huggingface.cloud/classification_api\"   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare a batch request for multiple texts — hf_build_request_batch","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"Creates httr2 request object obtaining response Hugging Face Inference endpoint multiple text inputs single batch. function can used various tasks, embedding classifying multiple inputs simultaneously.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"","code":"hf_build_request_batch(   inputs,   parameters = list(),   endpoint_url,   key_name,   max_retries = 5,   timeout = 10,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"inputs Vector list character strings process batch parameters Parameters send inputs endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating request","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"httr2 request object configured batch processing","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"developers, function forms basis batch requests, enabling efficient processing multiple inputs single API call.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"","code":"if (FALSE) { # \\dontrun{   # Create batch request using API key from environment   batch_req <- hf_build_request_batch(     inputs = c(\"First text to embed\", \"Second text to embed\"),     endpoint_url = \"https://my-endpoint.huggingface.cloud/embedding_api\",     key_name = \"HF_API_KEY\"   )    # Using custom timeout and retry settings   batch_req <- hf_build_request_batch(     inputs = c(\"Text one\", \"Text two\", \"Text three\"),     endpoint_url = \"https://my-endpoint.huggingface.cloud/embedding_api\",     key_name = \"HF_API_KEY\",     max_retries = 3,     timeout = 15   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare embedding requests for texts in a data frame — hf_build_request_df","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"Creates httr2 request objects text data frame column. Thus function handles request creation, handle performing request, tidying response. perform request, select appropriate *_perform_* function.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"","code":"hf_build_request_df(   df,   text_var,   id_var,   endpoint_url,   key_name,   parameters = list(),   max_retries = 3,   timeout = 10,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"df data frame containing texts embed text_var Name column containing text send endpoint id_var Name column use ID (optional) endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key parameters Parameters send inputs max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating requests","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"data frame original data plus request objects","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"","code":"if (FALSE) { # \\dontrun{   # Prepare requests for a data frame   df <- data.frame(     id = 1:3,     text = c(\"First example\", \"Second example\", \"Third example\")   )    requests_df <- hf_build_request_df(     df = df,     text_var = text,     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     id_var = id   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"Classifies batch texts using Hugging Face classification endpoint returns classification scores tidy format. Handles batching, concurrent requests, error recovery automatically.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"","code":"hf_classify_batch(   texts,   endpoint_url,   key_name,   ...,   tidy_func = tidy_batch_classification_response,   parameters = list(return_all_scores = TRUE),   batch_size = 8,   progress = TRUE,   concurrent_requests = 5,   max_retries = 5,   timeout = 30,   include_texts = TRUE,   relocate_col = 2 )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"texts Character vector texts classify endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... Additional arguments passed request functions tidy_func Function process API responses, defaults tidy_batch_classification_response parameters List parameters API endpoint, defaults list(return_all_scores = TRUE) batch_size Integer; number texts per batch (default: 8) progress Logical; whether show progress bar (default: TRUE) concurrent_requests Integer; number concurrent requests (default: 5) max_retries Integer; maximum retry attempts (default: 5) timeout Numeric; request timeout seconds (default: 20) include_texts Logical; whether include original texts output (default: TRUE) relocate_col Integer; column position text column (default: 2)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"Data frame classification scores text, plus columns original text (include_texts=TRUE), error status, error messages","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"function processes multiple texts efficiently splitting batches optionally sending concurrent requests. includes robust error handling progress reporting large batches. function automatically handles request failures retries includes error information output requests fail. Original text order preserved results. function currently handle list(return_all_scores = FALSE).","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"","code":"if (FALSE) { # \\dontrun{   texts <- c(     \"This product is brilliant!\",     \"Terrible quality, waste of money\",     \"Average product, nothing special\"   )    results <- hf_classify_batch(     texts = texts,     endpoint_url = \"redacted\",     key_name = \"API_KEY\",     batch_size = 3   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"Classifies texts data frame column using Hugging Face classification endpoint joins results back original data frame.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"","code":"hf_classify_df(   df,   text_var,   id_var,   endpoint_url,   key_name,   ...,   tidy_func = tidy_batch_classification_response,   parameters = list(return_all_scores = TRUE),   batch_size = 4,   concurrent_requests = 1,   max_retries = 5,   timeout = 30,   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"df Data frame containing texts classify text_var Column name containing texts classify (unquoted) id_var Column name use identifier joining (unquoted) endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... Additional arguments passed request functions tidy_func Function process API responses, defaults tidy_batch_classification_response parameters List parameters API endpoint, defaults list(return_all_scores = TRUE) batch_size Integer; number texts per batch (default: 4) concurrent_requests Integer; number concurrent requests (default: 1) max_retries Integer; maximum retry attempts (default: 5) timeout Numeric; request timeout seconds (default: 30) progress Logical; whether show progress bar (default: TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"Original data frame additional columns classification scores, classification results table row counts match","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"function extracts texts specified column, classifies using hf_classify_batch(), joins classification results back original data frame using specified ID column. function preserves original data frame structure adds new columns classification scores. number rows match processing (due errors), returns classification results separately warning. function currently handle list(return_all_scores = FALSE).","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"","code":"if (FALSE) { # \\dontrun{   df <- data.frame(     id = 1:3,     review = c(\"Excellent service\", \"Poor quality\", \"Average experience\")   )    classified_df <- hf_classify_df(     df = df,     text_var = review,     id_var = id,     endpoint_url = \"redacted\",     key_name = \"API_KEY\"   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"Sends text Hugging Face classification endpoint returns classification scores. default, returns tidied data frame one row columns classification label.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"","code":"hf_classify_text(   text,   endpoint_url,   key_name,   ...,   parameters = list(return_all_scores = TRUE),   tidy = TRUE,   max_retries = 5,   timeout = 20,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"text Character string classify endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... Additional arguments passed hf_perform_request ultimately httr2::req_perform parameters Advanced usage: parameters pass API endpoint, defaults list(return_all_scores = TRUE). tidy Logical; TRUE (default), returns tidied data frame max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Logical; whether validate endpoint creating request","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"tidied data frame classification scores (tidy=TRUE) raw API response","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"function handles entire process creating request Hugging Face Inference API endpoint text classification, sending request, processing response. function automatically retry failed requests according max_retries parameter. tidy=TRUE (default), transforms nested JSON response tidy data frame one row columns classification label. tidying fails, function returns raw response informative message.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"","code":"if (FALSE) { # \\dontrun{   # Basic classification with default parameters   result <- hf_classify_text(     text = \"This product is excellent!\",     endpoint_url = \"redacted\",     key_name = \"API_KEY\"   )    # Classification with custom parameters for a spam detection model   spam_result <- hf_classify_text(     text = \"URGENT: You've won a free holiday! Call now to claim.\",     endpoint_url = \"redacted\",     parameters = list(return_all_scores = TRUE)   )    # Get raw response without tidying   raw_result <- hf_classify_text(     text = \"I love this movie\",     endpoint_url = \"redacted\",     key_name = \"API_KEY\",     tidy = FALSE   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate batches of embeddings for a list of texts — hf_embed_batch","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"High-level function generate embeddings multiple text strings. function handles batching parallel processing embedding requests, attempts handle errors gracefully.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"","code":"hf_embed_batch(   texts,   endpoint_url,   key_name,   ...,   tidy_func = tidy_embedding_response,   parameters = list(),   batch_size = 8,   include_texts = TRUE,   concurrent_requests = 5,   max_retries = 5,   timeout = 10,   validate = FALSE,   relocate_col = 2 )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"texts Vector list character strings get embeddings endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... ellipsis sent hf_perform_request TODO (reserved ATM) tidy_func Function process/tidy raw API response (default: tidy_embedding_response) parameters Advanced usage: parameters pass API endpoint. batch_size Number texts process one batch include_texts Whether return original texts return tibble concurrent_requests Number requests send simultaneously max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating request relocate_col position data frame relocate results .","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"tibble containing embedding vectors","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"","code":"if (FALSE) { # \\dontrun{   # Generate embeddings for multiple texts using default batch size   embeddings <- hf_embed_batch(     texts = c(\"First example\", \"Second example\", \"Third example\"),     endpoint_url = \"https://my-endpoint.huggingface.cloud\"   )    # With custom batch size and concurrent requests   embeddings <- hf_embed_batch(     texts = c(\"First example\", \"Second example\", \"Third example\"),     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     batch_size = 10,     concurrent_requests = 5   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate embeddings for texts in a data frame — hf_embed_df","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"High-level function generate embeddings texts data frame. function handles entire process request creation response processing, options batching & parallel execution. Setting number retries","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"","code":"hf_embed_df(   df,   text_var,   id_var,   endpoint_url,   key_name,   batch_size = 8,   concurrent_requests = 1,   max_retries = 5,   timeout = 15,   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"df data frame containing texts embed text_var Name column containing text embed id_var Name column use ID endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key batch_size Number texts process one batch (NULL batching) concurrent_requests Number requests send . APIs allow multiple requests. max_retries Maximum number retry attempts failed requests. timeout Request timeout seconds progress Whether display progress bar","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"data frame original data plus embedding columns","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"","code":"if (FALSE) { # \\dontrun{   # Generate embeddings for a data frame   df <- data.frame(     id = 1:3,     text = c(\"First example\", \"Second example\", \"Third example\")   )    # Use parallel processing without batching   embeddings_df <- hf_embed_df(     df = df,     text_var = text,     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     id_var = id,     parallel = TRUE,     batch_size = NULL   )    # Use batching without parallel processing   embeddings_df <- hf_embed_df(     df = df,     text_var = text,     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     id_var = id,     parallel = FALSE,     batch_size = 10   )    # Use both batching and parallel processing   embeddings_df <- hf_embed_df(     df = df,     text_var = text,     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     id_var = id,     parallel = TRUE,     batch_size = 10   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate embeddings for a single text — hf_embed_text","title":"Generate embeddings for a single text — hf_embed_text","text":"High-level function generate embeddings single text string. function handles entire process request creation response processing.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate embeddings for a single text — hf_embed_text","text":"","code":"hf_embed_text(   text,   endpoint_url,   key_name,   ...,   parameters = list(),   tidy = TRUE,   max_retries = 3,   timeout = 10,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate embeddings for a single text — hf_embed_text","text":"text Character string get embeddings endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... ellipsis sent hf_perform_request, forwards httr2::req_perform parameters Advanced usage: parameters pass API endpoint tidy Whether attempt tidy response max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating request","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate embeddings for a single text — hf_embed_text","text":"tibble containing embedding vectors","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate embeddings for a single text — hf_embed_text","text":"","code":"if (FALSE) { # \\dontrun{   # Generate embeddings using API key from environment   embeddings <- hf_embed_text(     text = \"This is a sample text to embed\",     endpoint_url = \"https://my-endpoint.huggingface.cloud\"   )    # With custom API key environment variable name   embeddings <- hf_embed_text(     text = \"This is a sample text to embed\",     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     key_name = \"MY_CUSTOM_API_KEY\"   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute a single embedding request and process the response — hf_perform_request","title":"Execute a single embedding request and process the response — hf_perform_request","text":"Performs prepared request returns response","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute a single embedding request and process the response — hf_perform_request","text":"","code":"hf_perform_request(request, ...)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute a single embedding request and process the response — hf_perform_request","text":"request httr2 request object created hf_build_request ... ellipsis sent httr2::req_perform, e.g. path verbosityarguments.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute a single embedding request and process the response — hf_perform_request","text":"httr2 response object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Execute a single embedding request and process the response — hf_perform_request","text":"","code":"if (FALSE) { # \\dontrun{   # Create and perform request   req <- hf_build_request(     input = \"This is a sample text to embed\",     endpoint_url = \"https://my-endpoint.huggingface.cloud\"   )   embeddings <- hf_perform_request(req)  } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/json_dump.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert json_schema to API format — json_dump","title":"Convert json_schema to API format — json_dump","text":"Convert json_schema API format","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/json_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create JSON Schema S7 class for structured outputs — json_schema","title":"Create JSON Schema S7 class for structured outputs — json_schema","text":"S7 class JSON Schema definitions used , e.g. OpenAI structured outputs","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/json_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create JSON Schema S7 class for structured outputs — json_schema","text":"","code":"json_schema(   name = character(0),   schema = list(),   strict = TRUE,   description = \"\" )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/json_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create JSON Schema S7 class for structured outputs — json_schema","text":"name Name schema schema JSON schema definition list strict Whether enforce strict mode (default TRUE) description Optional description schema","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Build an OpenAI API Chat Completions request — oai_build_completions_request","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"function constructs httr2 request object specifically tailored interacting OpenAI's Chat Completions API. handles formatting messages, model parameters, optional JSON schema structured responses.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"","code":"oai_build_completions_request(   input,   model = \"gpt-4.1-nano\",   temperature = 0,   max_tokens = 500L,   schema = NULL,   system_prompt = NULL,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\",   timeout = 20,   max_retries = 5 )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"input Text input send model model OpenAI model use (default: \"gpt-4.1-nano\") temperature Sampling temperature (0-2), higher values = randomness max_tokens Maximum tokens response schema Optional JSON schema structured output (json_schema object list) system_prompt Optional system prompt key_name Environment variable name API key endpoint_url OpenAI API endpoint URL timeout Request timeout seconds max_retries Maximum number retry attempts failed requests","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"httr2 request object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"function simplifies process making calls OpenAI Chat Completions API assembling request body according API's specifications. input system_prompt (provided) automatically structured required 'messages' array format API. structured outputs (JSON mode), need provide valid JSON schema via schema parameter. can pre-formatted list object class \"json_schema\". schema provided, function automatically set schema$additionalProperties <- FALSE ensure schema$strict <- TRUE (strict already defined schema) encourage predictable reliable structured outputs API. also good idea set temperature 0 extracting structured outputs.","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Build OpenAI requests for batch processing — oai_build_completions_request_list","title":"Build OpenAI requests for batch processing — oai_build_completions_request_list","text":"Build OpenAI requests batch processing","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build OpenAI requests for batch processing — oai_build_completions_request_list","text":"","code":"oai_build_completions_request_list(   inputs,   model = \"gpt-4.1-nano\",   temperature = 0,   max_tokens = 500L,   schema = NULL,   system_prompt = NULL,   max_retries = 5L,   timeout = 30,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\" )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build OpenAI requests for batch processing — oai_build_completions_request_list","text":"inputs Character vector text inputs model OpenAI model use temperature Sampling temperature max_tokens Maximum tokens per response schema Optional JSON schema structured output system_prompt Optional system prompt max_retries Integer; maximum retry attempts (default: 5) timeout Numeric; request timeout seconds (default: 30) key_name Environment variable name API key endpoint_url OpenAI API endpoint URL","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build OpenAI requests for batch processing — oai_build_completions_request_list","text":"List httr2 request objects","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Process a data frame through OpenAI's Chat Completions API — oai_complete_df","title":"Process a data frame through OpenAI's Chat Completions API — oai_complete_df","text":"function takes data frame text inputs processes row OpenAI's Chat Completions API, returning results tidy format. handles concurrent requests, retries, structured output validation automatically.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process a data frame through OpenAI's Chat Completions API — oai_complete_df","text":"","code":"oai_complete_df(   df,   text_var,   id_var,   model = \"gpt-4.1-nano\",   system_prompt = NULL,   schema = NULL,   concurrent_requests = 1L,   max_retries = 5L,   timeout = 30,   temperature = 0,   max_tokens = 500L,   progress = TRUE,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\" )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process a data frame through OpenAI's Chat Completions API — oai_complete_df","text":"df Data frame containing text process text_var Column name (unquoted) containing text inputs id_var Column name (unquoted) unique row identifiers model OpenAI model use (default: \"gpt-4.1-nano\") system_prompt Optional system prompt applied requests schema Optional JSON schema structured output (json_schema object list) concurrent_requests Number simultaneous API requests (default: 1) max_retries Maximum retry attempts per request (default: 5) timeout Request timeout seconds (default: 30) temperature Sampling temperature (0-2), lower = deterministic (default: 0) max_tokens Maximum tokens per response (default: 500) progress Show progress bar (default: TRUE) key_name Environment variable name API key (default: \"OPENAI_API_KEY\") endpoint_url OpenAI API endpoint URL","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process a data frame through OpenAI's Chat Completions API — oai_complete_df","text":"tibble","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process a data frame through OpenAI's Chat Completions API — oai_complete_df","text":"function streamlines processing data OpenAI's API. extracts specified text column data frame, sends text separate API request, returns results joined back original data. function preserves row identity id_var parameter, ensuring results can matched back source data even requests fail. Failed requests marked .error = TRUE include error messages. using structured outputs schema, function automatically validates unnests (top level) JSON responses separate columns. makes ideal tasks like entity extraction, classification, structured data generation. best performance, adjust concurrent_requests based API rate limits. Higher values speed processing may hit rate limits frequently.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process a data frame through OpenAI's Chat Completions API — oai_complete_df","text":"","code":"if (FALSE) { # \\dontrun{  sample_texts <- rep(\"sample_text\", 100) # Parallel requests without schema: large_df <- data.frame(   doc_id = 1:100,   text = sample_texts )  results <- oai_complete_df(   large_df,   text_var = text,   id_var = doc_id,   concurrent_requests = 5,  # process 5 at a time   max_retries = 3 )  # Structured outputs with a schema: contact_schema <- create_json_schema(  name = \"contact_info\",  schema = schema_object(   name = schema_string(\"person's full name\"),   email = schema_string(\"email address\"),   phone = schema_string(\"phone number\"),   required = list(\"name\", \"email\", \"phone\"),   additional_properties = FALSE ))  text <- \"Am I speaking with Margaret Phillips? Yes, ok, and your email is mphil@hotmail.co.uk. Ok perfect, and your phone number? Was that 07564789789? Ok great. Just a second please Margaret, you're verified\"  schema_df <- data.frame(id = 1, text = text)  results <- oai_complete_df(   schema_df,   text_var = text,   id_var = id,   schema = contact_schema,   temperature = 0  # recommended for structured outputs )  } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Safely perform an embedding request with error handling — safely_perform_request","title":"Safely perform an embedding request with error handling — safely_perform_request","text":"Wrapper around httr2::req_perform handles errors gracefully. Wrapper around httr2::req_perform handles errors gracefully.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Safely perform an embedding request with error handling — safely_perform_request","text":"","code":"safely_perform_request(request)  safely_perform_request(request)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Safely perform an embedding request with error handling — safely_perform_request","text":"request httr2 request object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Safely perform an embedding request with error handling — safely_perform_request","text":"list components $result $error list components $result $error","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Create array property schema — schema_array","title":"Create array property schema — schema_array","text":"Defines list/array fields containing multiple items type. Use tags, categories, collection data. Can constrain array length.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create array property schema — schema_array","text":"","code":"schema_array(items, description = NULL, min_items = NULL, max_items = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create array property schema — schema_array","text":"items Schema definition array elements (use schema_* helpers) description Human-readable field description min_items Minimum number array elements max_items Maximum number array elements","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_array.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create array property schema — schema_array","text":"","code":"# List of tags schema_array(schema_string(), \"Product tags\", max_items = 10) #> $type #> [1] \"array\" #>  #> $items #> $items$type #> [1] \"string\" #>  #>  #> $description #> [1] \"Product tags\" #>  #> $maxItems #> [1] 10 #>   # Array of objects schema_array(   schema_object(     name = schema_string(\"Ingredient name\"),     amount = schema_number(\"Amount needed\")   ),   \"Recipe ingredients\" ) #> $type #> [1] \"array\" #>  #> $items #> $items$type #> [1] \"object\" #>  #> $items$properties #> $items$properties$name #> $items$properties$name$type #> [1] \"string\" #>  #> $items$properties$name$description #> [1] \"Ingredient name\" #>  #>  #> $items$properties$amount #> $items$properties$amount$type #> [1] \"number\" #>  #> $items$properties$amount$description #> [1] \"Amount needed\" #>  #>  #>  #> $items$additionalProperties #> [1] FALSE #>  #>  #> $description #> [1] \"Recipe ingredients\" #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_boolean.html","id":null,"dir":"Reference","previous_headings":"","what":"Create boolean property schema — schema_boolean","title":"Create boolean property schema — schema_boolean","text":"Defines true/false fields. Use flags, yes/questions, binary states. Ensures LLM returns exactly true false, \"yes\"/\"\" strings.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_boolean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create boolean property schema — schema_boolean","text":"","code":"schema_boolean(description = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_boolean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create boolean property schema — schema_boolean","text":"description Human-readable field description","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_boolean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create boolean property schema — schema_boolean","text":"","code":"# Feature flag schema_boolean(\"Has premium subscription\") #> $type #> [1] \"boolean\" #>  #> $description #> [1] \"Has premium subscription\" #>   # Classification result schema_boolean(\"Contains sensitive information\") #> $type #> [1] \"boolean\" #>  #> $description #> [1] \"Contains sensitive information\" #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_enum.html","id":null,"dir":"Reference","previous_headings":"","what":"Create enumerated property schema — schema_enum","title":"Create enumerated property schema — schema_enum","text":"Defines fields constrained specific allowed values. flexible schema_string enum parameter - supports numeric enums mixed types. Use categories, status codes, multi-choice field.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_enum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create enumerated property schema — schema_enum","text":"","code":"schema_enum(values, description = NULL, type = \"string\")"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_enum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create enumerated property schema — schema_enum","text":"values Vector allowed values description Human-readable field description type Data type enum values (\"string\", \"integer\", \"number\")","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_enum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create enumerated property schema — schema_enum","text":"","code":"# Status categories schema_enum(c(\"draft\", \"published\", \"archived\"), \"Document status\") #> $type #> [1] \"string\" #>  #> $enum #> [1] \"draft\"     \"published\" \"archived\"  #>  #> $description #> [1] \"Document status\" #>   # Priority levels as numbers schema_enum(c(1, 2, 3, 4, 5), \"Priority level\", type = \"integer\") #> $type #> [1] \"integer\" #>  #> $enum #> [1] 1 2 3 4 5 #>  #> $description #> [1] \"Priority level\" #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_integer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create integer property schema — schema_integer","title":"Create integer property schema — schema_integer","text":"Defines whole number fields optional range constraints. Use counts, IDs, quantities, discrete numeric values. restrictive schema_number.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_integer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create integer property schema — schema_integer","text":"","code":"schema_integer(description = NULL, minimum = NULL, maximum = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_integer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create integer property schema — schema_integer","text":"description Human-readable field description minimum Minimum allowed value (inclusive) maximum Maximum allowed value (inclusive)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_integer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create integer property schema — schema_integer","text":"","code":"# Item quantity schema_integer(\"Quantity ordered\", minimum = 1) #> $type #> [1] \"integer\" #>  #> $description #> [1] \"Quantity ordered\" #>  #> $minimum #> [1] 1 #>   # Rating scale schema_integer(\"Star rating\", minimum = 1, maximum = 5) #> $type #> [1] \"integer\" #>  #> $description #> [1] \"Star rating\" #>  #> $minimum #> [1] 1 #>  #> $maximum #> [1] 5 #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_number.html","id":null,"dir":"Reference","previous_headings":"","what":"Create numeric property schema — schema_number","title":"Create numeric property schema — schema_number","text":"Defines decimal number fields optional range constraints. Use prices, percentages, ratings, continuous numeric data.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_number.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create numeric property schema — schema_number","text":"","code":"schema_number(description = NULL, minimum = NULL, maximum = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_number.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create numeric property schema — schema_number","text":"description Human-readable field description minimum Minimum allowed value (inclusive) maximum Maximum allowed value (inclusive)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_number.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create numeric property schema — schema_number","text":"","code":"# Product price schema_number(\"Price in USD\", minimum = 0) #> $type #> [1] \"number\" #>  #> $description #> [1] \"Price in USD\" #>  #> $minimum #> [1] 0 #>   # Percentage score schema_number(\"Confidence score\", minimum = 0, maximum = 100) #> $type #> [1] \"number\" #>  #> $description #> [1] \"Confidence score\" #>  #> $minimum #> [1] 0 #>  #> $maximum #> [1] 100 #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_object.html","id":null,"dir":"Reference","previous_headings":"","what":"Create JSON Schema object definitions — schema_object","title":"Create JSON Schema object definitions — schema_object","text":"helper functions simplify creating JSON Schema definitions structured outputs (e.g. OpenAI). provide type-safe, validated schemas ensure consistent LLM responses matching expected data structure. JSON Schema constrains LLM outputs specific formats, preventing parsing errors ensuring reliable data extraction unstructured text. Create object schema nested properties Defines JSON object typed properties. Use structured data like user profiles, API responses, nested data structure. LLM return JSON matching exactly schema.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_object.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create JSON Schema object definitions — schema_object","text":"","code":"schema_object(..., required = NULL, additional_properties = FALSE)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_object.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create JSON Schema object definitions — schema_object","text":"... Named arguments defining object properties (use schema_* helpers) required Character vector required property names additional_properties Whether allow extra properties beyond defined","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_object.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create JSON Schema object definitions — schema_object","text":"","code":"# User profile with required fields schema_object(   name = schema_string(\"Full name\"),   age = schema_integer(\"Age in years\"),   required = c(\"name\", \"age\") ) #> $type #> [1] \"object\" #>  #> $properties #> $properties$name #> $properties$name$type #> [1] \"string\" #>  #> $properties$name$description #> [1] \"Full name\" #>  #>  #> $properties$age #> $properties$age$type #> [1] \"integer\" #>  #> $properties$age$description #> [1] \"Age in years\" #>  #>  #>  #> $additionalProperties #> [1] FALSE #>  #> $required #> [1] \"name\" \"age\"  #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_string.html","id":null,"dir":"Reference","previous_headings":"","what":"Create string property schema — schema_string","title":"Create string property schema — schema_string","text":"Defines text fields optional constraints. Use names, descriptions, textual data. Can restrict specific values via enum parameter.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_string.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create string property schema — schema_string","text":"","code":"schema_string(description = NULL, enum = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_string.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create string property schema — schema_string","text":"description Human-readable field description (helps LLM understand context) enum Character vector allowed values (creates dropdown-like constraint)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_string.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create string property schema — schema_string","text":"","code":"# Simple text field schema_string(\"User's email address\") #> $type #> [1] \"string\" #>  #> $description #> [1] \"User's email address\" #>   # Constrained to specific values schema_string(\"Sentiment\", enum = c(\"positive\", \"negative\", \"neutral\")) #> $type #> [1] \"string\" #>  #> $description #> [1] \"Sentiment\" #>  #> $enum #> [1] \"positive\" \"negative\" \"neutral\"  #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Set your API keys so they can be accessed by EndpointR — set_api_key","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"Set API key endpoint - endpoint Anthropic, OpenAI, specific Hugging Face Inference endpoint, another supported provider. Add overwrite=TRUE need update existing key. able retrieve key get_api_key() function.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"","code":"set_api_key(key_name, overwrite = FALSE)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"key_name name API format \"ENDPOINT_API_KEY\" -> \"ANTHROPIC_API_KEY\" overwrite Whether overwrite existing value API key.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"Nothing","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"","code":"if (FALSE) { # \\dontrun{   # set an Anthropic API key   set_api_key(\"ANTHROPIC_API_KEY\")    # update an existing OpenAI key   set_api_key(\"OPENAI_API_KEY\", overwrite = TRUE) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Hugging Face classification response to tidy format — tidy_classification_response","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"Transforms nested JSON response Hugging Face classification endpoint tidy data frame one row columns classification label.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"","code":"tidy_classification_response(response)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"response Either httr2_response object Hugging Face API request parsed JSON object containing classification results","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"data frame one row columns classification label","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"function expects specific structure response, classification result containing 'label' 'score' field. flattens nested structure pivots data create wide-format data frame. function accepts either raw httr2_response object parsed JSON structure, making flexible different workflow patterns.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"","code":"if (FALSE) { # \\dontrun{   # Process response directly from API call   response <- hf_perform_request(req)   tidy_results <- tidy_classification_response(response)    # Or with an already-parsed JSON object   json_data <- httr2::resp_body_json(response)   tidy_results <- tidy_classification_response(json_data)    # Example of expected output structure   # A tibble: 1 × 2   #   positive negative   #      <dbl>    <dbl>   # 1    0.982    0.018 } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Process embedding API response into a tidy format — tidy_embedding_response","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"Converts nested list response Hugging Face Inference API embedding request tidy tibble.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"","code":"tidy_embedding_response(response)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"response httr2 response object parsed JSON response","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"tibble containing embedding vectors","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"","code":"if (FALSE) { # \\dontrun{   # Process response from httr2 request   req <- hf_build_request(text, endpoint_url, api_key)   resp <- httr2::req_perform(req)   embeddings <- tidy_embedding_response(resp)    # Process already parsed JSON   resp_json <- httr2::resp_body_json(resp)   embeddings <- tidy_embedding_response(resp_json) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"Checks endpoint URL valid accessible provided API key. function sends small test request verify endpoint works.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"","code":"validate_hf_endpoint(endpoint_url, key_name)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"logical TRUE endpoint valid, otherwise stops error","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"","code":"if (FALSE) { # \\dontrun{   # Validate endpoint retrieving API key from environment   validate_hf_endpoint(     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     key_name = \"HF_API_KEY\"   )    # Using default key name   validate_hf_endpoint(\"https://my-endpoint.huggingface.cloud\") } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate response data against schema — validate_response","title":"Validate response data against schema — validate_response","text":"Validate response data schema","code":""},{"path":"https://jpcompartir.github.io/EndpointR/news/index.html","id":"endpointr-010","dir":"Changelog","previous_headings":"","what":"EndpointR 0.1.0","title":"EndpointR 0.1.0","text":"Initial release","code":""}]
