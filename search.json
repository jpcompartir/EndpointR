[{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"commands","dir":"","previous_headings":"","what":"Commands","title":"EndpointR Development Guide","text":"Run tests: testthat::test() Run single test: testthat::test_file(\"tests/testthat/test-tmp.R\") Run specific test: testthat::test_that(\"test description\", { test code }) Check package: devtools::check() R CMD check Build package: devtools::build() R CMD build . Generate documentation: devtools::document()","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"language-style-guidelines","dir":"","previous_headings":"","what":"Language Style Guidelines","title":"EndpointR Development Guide","text":"Use British English Use straightforward language jargon","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"code-style-guidelines","dir":"","previous_headings":"","what":"Code Style Guidelines","title":"EndpointR Development Guide","text":"Use roxygen2 markdown documentation Follow tidyverse style: snake_case function names, descriptive variables Chain operations native pipe (|>) Use tibble/data.frame data structures appropriate Include parameter types roxygen docs Use tidyeval data frame functions: capture variable names rlang::ensym() (e.g., text_sym <- rlang::ensym(text_var)) non-standard evaluation Error handling: use stopifnot() assertions cli::cli_abort() user-facing errors Format error messages cli syntax ({.val}, {.code}) Wrap examples requiring API request \\dontrun{} Use explicit return values document Keep functions small focused single responsibility API keys saved environment variables, input raw code","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"comment-guidelines","dir":"","previous_headings":"","what":"Comment Guidelines","title":"EndpointR Development Guide","text":"Write comments explain made new choice, stuff like ‘Check required parameters’ - Let obvious code speak Use lowercase comments British English (let AI use uppercase distinguish - helpful know whether ’s human AI comment)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"api-consistency","dir":"","previous_headings":"","what":"API Consistency","title":"EndpointR Development Guide","text":"Functions R/core.R intended ‘quite’ general - .e. form foundations rest package Generally want deal single text, batch text, df texts consistently, least predictably differently, across providers tasks","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"ai-usage","dir":"","previous_headings":"","what":"AI Usage","title":"EndpointR Development Guide","text":"Don’t commit code don’t understand AI writes code, better write tests. AI writes tests, better write code. Let AI help docs, make sure keep --date. able know ’s code, docs.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/CONTRIBUTORS.html","id":"git-branches","dir":"","previous_headings":"","what":"Git Branches","title":"EndpointR Development Guide","text":"Provide descriptive names possible feature/** bugfix/** documentation/** tests/**","code":""},{"path":"https://jpcompartir.github.io/EndpointR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 EndpointR authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"quick-start","dir":"Articles","previous_headings":"","what":"Quick start","title":"Managing API Keys Securely","text":"EndpointR two main functions managing API keys, get_api_key() set_api_key(), take ‘key_name’ input. security, set_api_key() uses askpass accept API keys rather code - means show .Rhistory, less likely leaked. First set API key: restart R session. Now get API key security reasons, print api_key console, pass function requires .","code":"set_api_key(\"TOY_API_KEY\") api_key <- get_api_key(\"TOY_API_KEY\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"what-is-an-api","dir":"Articles","previous_headings":"","what":"What is an API?","title":"Managing API Keys Securely","text":"API stands ‘Application Programming Interface’, mechanism two pieces software interact exchange data. APIs work requests - user sends request service, service sends response, fulfilling user’s request, providing error message. specific location within API allows user access service, function, called endpoint.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"what-is-an-api-key","dir":"Articles","previous_headings":"","what":"What is an API Key?","title":"Managing API Keys Securely","text":"many APIs provide access sensitive information, require credit card send requests, ’s important API providers able identify sending request, determine whether ’s safe respond, charge response. API key mechanism identifying sending request. See also: API keys","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"api-key-security","dir":"Articles","previous_headings":"","what":"API Key Security","title":"Managing API Keys Securely","text":"WARNING: EndpointR try help manage API keys safely, baseline level responsibility person using managed service needs take , set best practices need followed. paramount handle API keys securely. need avoid following things: Saving unencrypted API keys notes, emails, Google Docs etc. Uploading API keys web services (GitHub, etc.) Sharing API keys people Including unencrypted API keys code (scripts, R/Quarto/markdown files, .ipynb etc.) Unencypted API keys appearing .Rhistory file (especially file uploaded anywhere) suspect may done one - number - things, go directly got key, invalidate old one generate new one. key given , go directly person tell key compromised invalidate , provide new one. Instead , : Encrypt API keys Store environment variables using R/Rstudio/VScode Store managed secrets providers like GitHub use outside R/Rstudio/VScode","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/api_keys.html","id":"managing-multiple-keys","dir":"Articles","previous_headings":"","what":"Managing Multiple Keys","title":"Managing API Keys Securely","text":"endpoint EndpointR provides access , need correct environment variable stored .Renviron file API Key Lookup Table","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"what-are-text-embeddings","dir":"Articles","previous_headings":"","what":"What are Text Embeddings?","title":"Embeddings Providers","text":"Text embeddings numerical representations text capture semantic meaning. Think coordinates high-dimensional space similar texts closer together. ’re foundation : Semantic search Clustering similar documents Finding duplicates Building recommendation systems Powering RAG (Retrieval-Augmented Generation) applications EndpointR makes easy generate embeddings text data using either Hugging Face OpenAI APIs.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Embeddings Providers","text":"","code":"library(EndpointR) library(dplyr) library(tibble)  sample_texts <- tibble(   id = 1:3,   text = c(     \"Machine learning is transforming how we process information\",     \"I love building applications with embeddings\",      \"Natural language processing enables computers to understand text\"   ),   category = c(\"ML\", \"embeddings\", \"NLP\") )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"provider-comparison","dir":"Articles","previous_headings":"","what":"Provider Comparison","title":"Embeddings Providers","text":"diving code, let’s understand key differences:","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"setting-up","dir":"Articles","previous_headings":"Hugging Face Embeddings","what":"Setting Up","title":"Embeddings Providers","text":"First, get API key Hugging Face set . set endpoint’s URL. ’ve chosen endpoint accessing embeddings -mpnet-base-v2 model.","code":"set_api_key(\"HF_TEST_API_KEY\")  embed_url <-  \"https://router.huggingface.co/hf-inference/models/sentence-transformers/all-mpnet-base-v2/pipeline/feature-extraction\""},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"single-text","dir":"Articles","previous_headings":"Hugging Face Embeddings","what":"Single Text","title":"Embeddings Providers","text":"simplest case - embed one piece text:","code":"embedding <- hf_embed_text(   text = \"I want to understand the meaning of this sentence\",   endpoint_url = embed_url,   key_name = \"HF_TEST_API_KEY\" )  dim(embedding) # result: a tibble with 768 columns (V1 to V768)  embedding"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"batch-processing","dir":"Articles","previous_headings":"Hugging Face Embeddings","what":"Batch Processing","title":"Embeddings Providers","text":"multiple texts, use hf_embed_batch() handles batching automatically. feed vector inputs batch_size, function takes care batching vector many batches necessary. result includes: - text: original text - .error .error_message: error tracking - V1 V768: embedding dimensions","code":"texts_to_embed <- c(   \"First document about machine learning\",   \"Second document about deep learning\",   \"Third document about neural networks\",   \"Fourth document about data science\" )  batch_embeddings <- hf_embed_batch(   texts = texts_to_embed,   endpoint_url = embed_url,   key_name = \"HF_TEST_API_KEY\",   batch_size = 2,  # process 2 texts per API call   concurrent_requests = 2  # run 2 requests in parallel )  # Check results glimpse(batch_embeddings[1,1:10 ]) # truncated for ease"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"data-frame-integration","dir":"Articles","previous_headings":"Hugging Face Embeddings","what":"Data Frame Integration","title":"Embeddings Providers","text":"commonly, ’ll want embed column data frame:","code":"embedded_df <- hf_embed_df(   df = sample_texts,   text_var = text,      # column containing text   id_var = id,          # unique identifier column   endpoint_url = embed_url,   key_name = \"HF_TEST_API_KEY\",   batch_size = 3,   concurrent_requests = 1 )  # Original data + embeddings names(embedded_df)[1:10]  # shows: id, text, category, .error, .error_message, V1, V2...  embedded_df"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"setting-up-1","dir":"Articles","previous_headings":"OpenAI Embeddings","what":"Setting Up","title":"Embeddings Providers","text":"Get API key OpenAI website set :","code":"set_api_key(\"OPENAI_API_KEY\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"single-text-1","dir":"Articles","previous_headings":"OpenAI Embeddings","what":"Single Text","title":"Embeddings Providers","text":"OpenAI offers configurable embedding dimensions:","code":"# Default dimensions (1536 for text-embedding-3-small) embedding <- oai_embed_text(   text = \"I want to understand the meaning of this sentence\" )  # Custom dimensions for smaller embeddings small_embedding <- oai_embed_text(   text = \"I want to understand the meaning of this sentence\",   model = \"text-embedding-3-small\",   dimensions = 512  # reduce size by ~67% )  dim(small_embedding)  # 1 row, 512 embedding columns + index"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"batch-processing-1","dir":"Articles","previous_headings":"OpenAI Embeddings","what":"Batch Processing","title":"Embeddings Providers","text":"OpenAI allows multiple texts single API call, oai_embed_batch() leverages.","code":"texts_to_embed <- c(   \"First document about machine learning\",   \"Second document about deep learning\",   \"Third document about neural networks\",   \"Fourth document about data science\" )  batch_embeddings <- oai_embed_batch(   texts = texts_to_embed,   model = \"text-embedding-3-small\",   dimensions = 1536,  # default for this model   batch_size = 10,    # texts per API request   concurrent_requests = 3  # parallel requests )   batch_embeddings |>   reframe(     total = n(),     succeeded = sum(!.error),     failed = sum(.error)   )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"data-frame-integration-1","dir":"Articles","previous_headings":"OpenAI Embeddings","what":"Data Frame Integration","title":"Embeddings Providers","text":"","code":"embedded_df <- oai_embed_df(   df = sample_texts,   text_var = text,   id_var = id,   model = \"text-embedding-3-large\",  # higher quality embeddings   dimensions = 3072,  # maximum dimensions for this model   batch_size = 20,   concurrent_requests = 5 )  # Extract just the embeddings for downstream use embedded_df |>    select(starts_with(\"V\"))"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"openai-limits","dir":"Articles","previous_headings":"Handling Sequence Length","what":"OpenAI Limits","title":"Embeddings Providers","text":"OpenAI token limit 8,192 per request. Since 1 token ≈\\approx 4 characters: can truncate texts substr() function practice course want use intelligent splitting procedure.","code":"long_texts <- tibble(   id = 1:3,   text = c(     paste(rep(\"word\", 100), collapse = \" \"),    # ~400 chars, safe     paste(rep(\"word\", 8000), collapse = \" \"),   # ~32k chars, near limit       paste(rep(\"word\", 10000), collapse = \" \")   # ~40k chars, too long!   ) )  long_texts |>   mutate(     char_count = nchar(text),     approx_tokens = char_count / 4,     will_fail = approx_tokens > 8192   ) # truncation (data / information loss happens!) long_texts |>   mutate(     text = ifelse(nchar(text) > 32000,                    substr(text, 1, 32000),                    text)   ) |>   mutate(     char_count = nchar(text),     approx_tokens = char_count / 4,     will_fail = approx_tokens > 8192   )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"hugging-face-limits","dir":"Articles","previous_headings":"Handling Sequence Length","what":"Hugging Face Limits","title":"Embeddings Providers","text":"using Inference API, limits vary model. Check model’s documentation. models handle ~512 tokens well. modern models can handle (check model card). Dedicated Inference Endpoints receive many requests assigned hardware able handle. code chunk shows chunk texts ’re finding errors due payload size:","code":"chunk_text <- function(text, max_chars = 2000) {   if (nchar(text) <= max_chars) return(list(text))      # v. simple chunking (consider sentence boundaries/ more intelligent chunking in production)   chunks <- substring(text,                       seq(1, nchar(text), max_chars),                       seq(max_chars, nchar(text) + max_chars - 1, max_chars))   as.list(chunks[nchar(chunks) > 0]) }"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"error-handling","dir":"Articles","previous_headings":"Best Practices","what":"Error Handling","title":"Embeddings Providers","text":"Always check errors results. chunk shows send failures another batch request, beware ’ll need handle resulting data frames.","code":"results <- oai_embed_batch(texts = texts_to_embed)  # Check overall success if (any(results$.error)) {   failed <- results |>      filter(.error) |>     select(text, .error_message)      print(failed)      # Retry failed texts with adjusted parameters   retry_texts <- failed$text   retry_results <- oai_embed_batch(     texts = retry_texts,     batch_size = 1,  # one at a time     timeout = 30     # longer timeout   ) }"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"performance-tips","dir":"Articles","previous_headings":"Best Practices","what":"Performance Tips","title":"Embeddings Providers","text":"Start small: Begin batch_size = 5 concurrent_requests = 1. Scale gradually: Increase parameters whilst monitoring errors Hugging Face: -MiniLM-L6-v2 speed (384 dims) OpenAI: text-embedding-3-small custom dimensions flexibility Consider dedicated endpoints production Hugging Face deployments TIP: Check organisation’s tier OpenAI, tier 5 organisatons can send many requests tier 1. OpenAI Rate Limits","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"cost-optimisation","dir":"Articles","previous_headings":"Best Practices","what":"Cost Optimisation","title":"Embeddings Providers","text":"OpenAI: Reduce dimensions save storage computation WARNING: ’ll need compare performance vs size trade-particular use-case.","code":"compact_embeddings <- oai_embed_batch(   texts = texts_to_embed,   model = \"text-embedding-3-small\",   dimensions = 360 )"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"semantic-search","dir":"Articles","previous_headings":"Common Use Cases","what":"Semantic Search","title":"Embeddings Providers","text":"basic implementation semantic search (AKA dense embedding/vector search AKA neural search ) Embed document corpus Embed search query Find similar documents (cosine similarity) Extract top 5 similar TODO: add code ? NOTE: use-cases hybrid approach comprising full-text search semantic search yield best result","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"clustering","dir":"Articles","previous_headings":"Common Use Cases","what":"Clustering","title":"Embeddings Providers","text":"Generate embeddings Extract embeddings matrix clustering algorithm Run clustering algorithm Add clusters data frame TIP: practice need inspect outputs clustering model tune . code run model small part modelling process","code":"embeddings_for_clustering <- hf_embed_df(   df = sample_texts,   text_var = text,   id_var = id,   endpoint_url = embed_url,   key_name = \"HF_TEST_API_KEY\" )  embedding_matrix <- embeddings_for_clustering |>   select(starts_with(\"V\")) |>   as.matrix()  kmeans_result <- kmeans(embedding_matrix, centers = 2)  clustered_texts <- sample_texts |>   mutate(cluster = kmeans_result$cluster)"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"rate-limits","dir":"Articles","previous_headings":"Troubleshooting","what":"Rate Limits","title":"Embeddings Providers","text":"Use fewer concurrent_requests ’re running rate limit issues. rate limits request, others tokens. ’re running token limits, solution wait longer requests. ’re running request limits, increase batch_size, embed data fewer requests.","code":"results <- hf_embed_batch(   texts = large_text_collection,   batch_size = 3,   concurrent_requests = 1  # sequential processing )  results <- oai_embed_batch(   texts = large_text_collection,   batch_size = 5, # fewer requests with larger batch size   concurrent_requests = 2 )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"timeouts","dir":"Articles","previous_headings":"Troubleshooting","what":"Timeouts","title":"Embeddings Providers","text":"Increase value timeout parameter sending many requests, responses begin timing .","code":"results <- oai_embed_batch(   texts = texts_to_embed,   timeout = 60,    max_retries = 5 )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"memory-issues","dir":"Articles","previous_headings":"Troubleshooting","what":"Memory Issues","title":"Embeddings Providers","text":"Process chunks large datasets TIP: also write splits individual files, iterate files avoid reading data memory .","code":"library(purrr)  text_chunks <- split(large_text_vector,                       ceiling(seq_along(large_text_vector) / 1000))  all_embeddings <- map(text_chunks, ~{   oai_embed_batch(.x, batch_size = 10) }) |>   list_rbind()"},{"path":"https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Embeddings Providers","text":"See Improving Performance vignette optimisation tips Check Hugging Face Inference classification tasks Explore different embedding models specific use case Remember: embeddings foundation many NLP applications. Choose provider based needs quality, speed, cost, flexibility.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Using Hugging Face Inference Endpoints","text":"Follow Hugging Face’s docs generate Hugging Face token, register EndpointR:","code":"library(EndpointR) library(dplyr) library(httr2) library(tibble)  my_data <- tibble(   id = 1:3,   text = c(     \"Machine learning is fascinating\",     \"I love working with embeddings\",      \"Natural language processing is powerful\"   ),   category = c(\"ML\", \"embeddings\", \"NLP\") ) set_api_key(\"HF_TEST_API_KEY\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"choosing-your-service","dir":"Articles","previous_headings":"","what":"Choosing Your Service","title":"Using Hugging Face Inference Endpoints","text":"Hugging Face offers two inference options: Inference API: Free, good testing Dedicated Endpoints: Paid, reliable, fast vignette, ’ll use Inference API. switch dedicated endpoints, just change URL.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting Started","title":"Using Hugging Face Inference Endpoints","text":"Go Hugging Face’s models hub fetch Inference API’s URL model want embed data . models available via Hugging Face Inference API, need use model available may need deploy Dedicated Inference Endpoint.","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"single-text","dir":"Articles","previous_headings":"Embeddings","what":"Single Text","title":"Using Hugging Face Inference Endpoints","text":"Embed one piece text: result tibble one row 384 columns (V1 V384). column embedding dimension. Note: number columns depends model. Check model’s Hugging Face page embedding size.","code":"# inference api url for embeddings embed_url <- \"https://router.huggingface.co/hf-inference/models/sentence-transformers/all-mpnet-base-v2/pipeline/feature-extraction\"  result <- hf_embed_text(   text = \"This is a sample text to embed\",   endpoint_url = embed_url,   key_name = \"HF_API_KEY\" )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"list-of-texts","dir":"Articles","previous_headings":"Embeddings","what":"List of Texts","title":"Using Hugging Face Inference Endpoints","text":"Embed multiple texts using batching: result includes: text: original text .error: TRUE something went wrong .error_message: went wrong (anything) V1 V384: embedding values","code":"texts <- c(   \"First text to embed\",   \"Second text to embed\",   \"Third text to embed\" )  batch_result <- hf_embed_batch(   texts,   endpoint_url = embed_url,   key_name = \"HF_API_KEY\",   batch_size = 3  # process 3 texts per request )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"data-frame","dir":"Articles","previous_headings":"Embeddings","what":"Data Frame","title":"Using Hugging Face Inference Endpoints","text":"commonly, ’ll want embed column data frame: Check errors: Extract just embeddings:","code":"embedding_result <- hf_embed_df(   df = my_data,   text_var = text,      # column with your text   id_var = id,          # column with unique ids   endpoint_url = embed_url,   key_name = \"HF_API_KEY\" ) embedding_result |> count(.error) embeddings_only <- embedding_result |> select(V1:V384)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"classification","dir":"Articles","previous_headings":"","what":"Classification","title":"Using Hugging Face Inference Endpoints","text":"Classification works way embeddings, just different URL output format. neceessary, can also provide custom function tidying output.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"single-text-1","dir":"Articles","previous_headings":"Classification","what":"Single Text","title":"Using Hugging Face Inference Endpoints","text":"","code":"classify_url <- \"https://router.huggingface.co/hf-inference/models/distilbert/distilbert-base-uncased-finetuned-sst-2-english\"  sentiment <- hf_classify_text(   text = \"I love this package!\",   endpoint_url = classify_url,   key_name = \"HF_API_KEY\" )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"data-frame-1","dir":"Articles","previous_headings":"Classification","what":"Data Frame","title":"Using Hugging Face Inference Endpoints","text":"result includes: original id column Classification labels (e.g., POSITIVE, NEGATIVE) Confidence scores Error tracking columns. NOTE: Classification labels model task specific.","code":"classification_result <- hf_classify_df(   df = my_data,   text_var = text,   id_var = id,   endpoint_url = classify_url,   key_name = \"HF_API_KEY\" )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"using-dedicated-endpoints","dir":"Articles","previous_headings":"","what":"Using Dedicated Endpoints","title":"Using Hugging Face Inference Endpoints","text":"use dedicated endpoints instead Inference API: Deploy model dedicated endpoint (see Hugging Face docs) Get endpoint URL Replace URL function: Note: Dedicated endpoints take 20-30 seconds start ’re idle. Set max_retries = 5 give time wake .","code":"# just change this line dedicated_url <- \"https://your-endpoint-name.endpoints.huggingface.cloud\"  # everything else stays the same result <- hf_embed_text(   text = \"Sample text\",   endpoint_url = dedicated_url,  # <- only change   key_name = \"HF_API_KEY\" )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"tips","dir":"Articles","previous_headings":"","what":"Tips","title":"Using Hugging Face Inference Endpoints","text":"Start small batch sizes (3-5) increase gradually Inference API rate limits - dedicated endpoints hardware constraints, increase hardware higher limits production use, choose dedicated endpoints Check Improving Performance vignette speed tips","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"common-issues","dir":"Articles","previous_headings":"","what":"Common Issues","title":"Using Hugging Face Inference Endpoints","text":"Rate limits: Reduce batch size add delays requests Model available: models work Inference API. Check model page use dedicated endpoints. Timeouts: Increase max_retries reduce batch size","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.html","id":"improving-performance","dir":"Articles","previous_headings":"","what":"Improving Performance","title":"Using Hugging Face Inference Endpoints","text":"EndpointR’s functions come knobs dials can turn improve throughput performance. Visit Improving Performance vignette information.","code":""},{"path":[]},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"understanding-the-time-problem","dir":"Articles","previous_headings":"","what":"Understanding the Time Problem","title":"Improving Performance","text":"row data frame, need create request perform. perform request sending information internet endpoint. endpoint accepts request, performs computation, sends successful response error. step associated cost time, cost step equal - steps costly others. Creating request almost instant, larger data frame - e.g. 100,000 rows: creating 100,000 requests simultaneously take ≈\\approx 30s-90s. create , ’ll wait full amount time begin sending requests. Clearly wasteful - created first request sent , save least 90 seconds. Sending request endpoint waiting process usually take lot longer preparing request. Therefore, can find ways reduce time spend waiting , able reduce overall time takes prepare send requests significantly. vignette focus mainly hf_embed_df() function options improving throughput requests. ideas apply hf_classify_df() function, ideas transfer providers functions, e.g. oai_*().","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"set-up","dir":"Articles","previous_headings":"","what":"Set Up","title":"Improving Performance","text":"TODO: Copied hugging_face_inference.Rmd, need re-order edit ’ll need data, ’ll grab sentences data frame introductory vignette Hugging Face Inference: WARNING: follow along need provide endpoint_url set API key. using encrypted URL security purposes. ENDPOINTR_KEY person-specific (.e. access key) environment variable can used encrypt information, stored securely offline, committed GitHub online services.","code":"library(tibble) library(tidyr) library(EndpointR) library(ggplot2) library(httr2) library(dplyr) embedding_sentences <- c(   \"Text embedding models compress your rambling prose into compact vectors, much like how British commuters squeeze themselves into tube carriages during rush hour.\",   \"Setting up an inference endpoint without proper documentation is akin to attempting to navigate London using a map of Birmingham.\",   \"When your embedding model starts hallucinating, it's rather like watching Parliament during Question Time—entertaining but utterly unpredictable.\",   \"Optimising your inference endpoint is essential unless you fancy your users growing old whilst waiting for a response, perhaps even ageing enough to collect their pension.\",   \"The distance between word embeddings reveals semantic relationships, though sadly not the distance between what your client requested and what they actually wanted.\",   \"Creating multilingual embeddings is a bit like attempting to order tea properly across Europe—technically possible but fraught with cultural misunderstandings.\",   \"Batch processing through inference endpoints saves computing resources, much like how the British save conversation topics by discussing the weather exclusively.\",   \"Token limits on embedding APIs are the digital equivalent of a queue at a British post office—inevitably, you'll reach the front just as they close for lunch.\",   \"Fine-tuning embedding models on domain-specific corpora is rather like training a British child to apologise—it requires repetition, patience, and considerable examples.\",   \"When your inference endpoint crashes under load, it maintains that quintessentially British trait of breaking down precisely when most inconvenient.\" )  embedding_ids <- 1:10  embedding_df <-   tibble(     id = embedding_ids,     sentence = embedding_sentences   ) endpoint_url <- httr2::secret_decrypt(\"kcZCsc92Ty7PuAk7_FdCcdOU_dlDpvWdDyfpwCWg-wW80eJxJPdQ68nz4V_0922SzSwM5_dYfpTOqsZ-GodUpLN4PQbwE73wlZkBCWIaIXc15g\", \"ENDPOINTR_KEY\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"improving-performance---time","dir":"Articles","previous_headings":"","what":"Improving Performance - Time","title":"Improving Performance","text":"hf_embed_df function, main options increase number requests send simultaneously, number texts send within request. concurrent_requests lets EndpointR know many requests send time. number 1 ~20-50 (extreme cases may ok 100, endpoints accept ) batch_size lets EndpointR know whether send requests individual texts, number texts ‘batched ’ together.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"solution-1-concurrent-requests","dir":"Articles","previous_headings":"Improving Performance - Time","what":"Solution 1: Concurrent Requests","title":"Improving Performance","text":"Hugging Face Inference Endpoints can handle multiple requests arriving time. goal send just maximum number requests endpoint can handle. example, endpoint powerful GPU may able handle hundreds documents time, whereas endpoint small CPU able handle handful. guide, start ~5 concurrent requests, work ~20. start hitting rate limits go back ~10 find sweetspot. endpoint handling 20 requests without returning rate limit errors, experiment > 20 requests. **TIP*:: Endpoints can configured ‘autoscale’ - meaning provide additional hardware number queued requests configured threshold configured duration time. send 5 concurrent requests - iterate data frame 5 rows time, send new requests responses returned, run data embed. Let’s benchmark performance (see appendix code) - overhead associated generating parallel requests, ’ll need bigger data frame understand type speed can get. Recording results: can see ~40% reduction processing time going 5-> 10 concurrent requests ~15% reduction going 10 -> 20 concurrent requests. *Exact times fluctuate, take approximates.","code":"hf_embed_df(   df = embedding_df,   text_var = sentence,   endpoint_url = endpoint_url,   key_name = \"HF_TEST_API_KEY\",   id_var= id,   concurrent_requests = 5,   batch_size = 1,   progress = TRUE ) id_1000 <- 1:1000 sentences_1000 <- rep(embedding_df$sentence, 100) embedding_df_1000 <- tibble(id = id_1000, sentence = sentences_1000)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"solution-2-batching-requests","dir":"Articles","previous_headings":"Improving Performance - Time","what":"Solution 2: Batching Requests","title":"Improving Performance","text":"Another method reducing processing time send data within request. mean can send fewer requests overall, reduce time spent sending information network. Sending multiple rows data within request creates batch request. 1,000 rows data, batch_size = 10 result 100 requests sent, instead 1,000 batch_size = 1. TIP: experiment batch sizes find sweetspot - usually starting around 8-16, capping ~64. ’ll know ’ve gone high ’ll start seeing retry bar, /responses contain errors.","code":"hf_embed_df(   df = embedding_df_1000,   text_var = sentence,   endpoint_url = endpoint_url,   key_name = \"HF_TEST_API_KEY\",   id_var= id,   concurrent_requests = 1,   batch_size = 20,   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"solution-3-combining-concurrency-and-batching","dir":"Articles","previous_headings":"Improving Performance - Time","what":"Solution 3: Combining Concurrency and Batching","title":"Improving Performance","text":"maximum speed , can also send batches multiple concurrent requests. endpoint able handle , 10 concurrent requests batch_size = 10 faster 10 concurrent requests batch_size = 5.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"benchmarking-solutions","dir":"Articles","previous_headings":"","what":"Benchmarking Solutions","title":"Improving Performance","text":"separate session using dataset available package, explored relationship batch size, concurrent requests, throughput ; throughput number rows processed per second. looked combinations : batch_size = c(1, 4, 8, 16, 32) concurrent_requests = c(1, 5, 10, 15, 20) embed ≈\\approx 2,000 documents sending 1 text 1 request time, get throughput ≈\\approx 2.16 texts per second, takes 15 minutes! end, 20 concurrent requests batch size 8, 20 concurrent requests batch size 32 throughput $$195, close 200x quicke! get results back 10 seconds. NOTE: needed , re-run benchmarking code multiple times general trend clear.   parameter important speed, batch_size = concurrent_requests =? eye looks like concurrent_requests bigger effect, ’s clear parameters positive effect, within boundaries parameters, approximately linear relationship throughput. Putting together quick linear model confirms everything else equal, increasing concurrent_requests increases throughput batch_size QUESTION: might happen keep increasing batch_size concurrent_requests? relationship hold ?","code":"data(batch_concurrent_benchmark, package = \"EndpointR\")  knitr::kable(batch_concurrent_benchmark |> mutate_if(is.numeric, ~ round(.x, 2))) batch_concurrent_benchmark |>    mutate(batch_size = factor(batch_size), concurrent_requests= factor(concurrent_requests)) |>   ggplot(aes(x= batch_size, y = throughput, group = concurrent_requests)) +   geom_point(aes(colour = concurrent_requests)) +   geom_line(aes(colour = concurrent_requests)) +   labs(y = \"Throughput\", x = \"Batch Size\", title = \"Increasing `batch_size` and `concurrent_requests` increases throughput\") +   scale_colour_viridis_d() +   theme_minimal() +   theme(legend.position = \"bottom\") batch_concurrent_benchmark |>      mutate(batch_size = factor(batch_size),             concurrent_requests = factor(concurrent_requests)) |>       ggplot(aes(x = batch_size, y = concurrent_requests, fill = throughput)) +      geom_tile() +     geom_text(aes(label = round(throughput, 1)), colour = \"white\") +     scale_fill_viridis_c(name = \"Throughput\") +     theme_minimal() +     labs(x = \"Batch Size\", y = \"Concurrent Requests\",          title = \"Throughput by Batch Size and Concurrent Requests\") +     theme(legend.position = \"none\") model <- lm(throughput ~ batch_size + concurrent_requests, data = batch_concurrent_benchmark)  broom::tidy(model) |>     mutate(across(c(estimate, std.error, statistic), ~round(.x, 2))) #> # A tibble: 3 × 5 #>   term                estimate std.error statistic   p.value #>   <chr>                  <dbl>     <dbl>     <dbl>     <dbl> #> 1 (Intercept)             3.34     15.5       0.22 0.832     #> 2 batch_size              2.4       0.71      3.37 0.00366   #> 3 concurrent_requests     6.45      1.12      5.77 0.0000224"},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"understanding-the-memory-problem","dir":"Articles","previous_headings":"","what":"Understanding the Memory Problem","title":"Improving Performance","text":"TODO:","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"appendix---embeddings","dir":"Articles","previous_headings":"Understanding the Memory Problem","what":"Appendix - Embeddings","title":"Improving Performance","text":"TODO: date post-refactoring","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"benchmarking-concurrent-requests","dir":"Articles","previous_headings":"Understanding the Memory Problem > Appendix - Embeddings","what":"Benchmarking Concurrent Requests","title":"Improving Performance","text":"","code":"run_benchmark <- function(num_concurrent, data, endpoint, key) {   start_time <- Sys.time()   res_df <- try(hf_embed_df(                   df = data,                   text_var = sentence,                    id_var= id,                            endpoint_url = endpoint,                   key_name = key,                   include_errors = FALSE,                    concurrent_requests = num_concurrent,                   progress = FALSE                  ), silent = TRUE)    processing_time <- Sys.time() - start_time   success <- !inherits(res_df, \"try-error\") && nrow(res_df) == nrow(data)   return(data.frame(     concurrent_requests = num_concurrent,     processing_time_secs = as.numeric(processing_time, units = \"secs\"),     success = success   )) }  num_requests_vec <- c(5, 10, 20) results_list <- lapply(num_requests_vec, function(n) {   run_benchmark(     num_concurrent = n,     data = embedding_df_1000,     endpoint = endpoint_url,     key = \"HF_TEST_API_KEY\"    ) })  (summary_df <- do.call(rbind, results_list))"},{"path":"https://jpcompartir.github.io/EndpointR/articles/improving_performance.html","id":"benchmarking-batch-and-concurrent-requests","dir":"Articles","previous_headings":"Understanding the Memory Problem > Appendix - Embeddings","what":"Benchmarking Batch and Concurrent Requests","title":"Improving Performance","text":"won’t able re-run exactly -, data frame provided package. bring data wanted .","code":"trust <- readr::read_csv(\"~/data/trust/trust_slice_spam_classification.csv\") |>    select(text) |>   mutate(id = row_number()) |>   filter(!is.na(text), text != \"\") # stop NAs and empty vals crashing anything  chunk_size <- 2000 total_chunks <- 20  # this chunking logic is actually rubbish. trust_chunks <- trust |>   mutate(chunk_id = ceiling(id / chunk_size)) |>   group_split(chunk_id) |>   head(total_chunks)  benchmark_params <- crossing(   batch_size = c(1, 4, 8, 16, 32),   concurrent_requests = c(1, 5, 10, 20) ) |>   mutate(chunk_index = row_number() %% total_chunks + 1)  benchmark_results <- benchmark_params |>   mutate(result = pmap(list(batch_size, concurrent_requests, chunk_index), function(bs, cr, ci) {     current_chunk <- trust_chunks[[ci]]      start_time <- Sys.time()      res <- try(hf_embed_df(       df = current_chunk,       text_var = text,       id_var = id,       endpoint_url = endpoint_url,       key_name = \"HF_TEST_API_KEY\",       batch_size = bs,       concurrent_requests = cr,       progress = TRUE     ), silent = TRUE)      elapsed_time <- as.numeric(Sys.time() - start_time, units = \"secs\")     success <- !inherits(res, \"try-error\")     rows_processed <- if(success) nrow(current_chunk) else 0      list(       elapsed_time = elapsed_time,       success = success,       rows_processed = rows_processed,       throughput = if(success) rows_processed / elapsed_time else 0     )   })) |>   nest_wider(result)"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"openai---the-basics","dir":"Articles","previous_headings":"","what":"OpenAI - The Basics","title":"Connecting to Major Model Providers","text":"get started, need make sure couple things place: First, get API key store “OPENAI_API_KEY” set_api_key() Second, figure need Responses API Chat Completions API Third, choose model - EndpointR configured select smaller, cheaper model free choose specific models Information: unsure whether prefer Completions API Responses API, checkout webpage","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"completions-api","dir":"Articles","previous_headings":"OpenAI - The Basics","what":"Completions API","title":"Connecting to Major Model Providers","text":"“Chat Completions API endpoint generate model response list messages comprising conversation.” use-cases EndpointR covers, conversation single interaction user model. Usually goal achieve narrow task repeatedly. TIP: looking persistent, open-ended chats LLMs, ’s best head Claude, ChatGPT, Gemini - provider choice. type tasks might use Completions API ? Document classification, e.g. Sentiment Analysis Translation Structured data extraction","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"sentiment-analysis","dir":"Articles","previous_headings":"OpenAI - The Basics > Completions API","what":"Sentiment Analysis","title":"Connecting to Major Model Providers","text":"Whilst generally recommend using OpenAI’s models sentiment analysis, 1 task people familiar . set basic 2 system prompt tell LLM want text send : can create request inspect {httr2}’s req_dry_run() function - ’s good hygiene check request formatted expect , particularly using first time. example, intend system prompt, {“role”: “system”} key:value pair inside messages list? value “model”: valid model ID? Etc. TIP:may need familiarise OpenAI API Documentation proceeding. generated HTTP request look something like : EndpointR handles HTTP mechanics: authentication, request headers, endpoint configuration. constructs JSON payload specified model, system prompt, user message, whilst setting sensible defaults temperature 3 max_tokens 4. demonstrative purposes, ran prompt data twice (see ) First run: “sentiment text frustrated exasperated.” Second run: “sentiment text quite negative. speaker appears frustrated exasperated, expressing dissatisfaction situation.” Third run: ““sentiment text quite negative. speaker appears frustrated exasperated, expressing dissatisfaction impatience situation.” Whilst responses directionally/approximately accurate, inconsistent output can unpleasant work . scaled hundreds thousands requests, inconsistency can extremely draining, productivity reducing. wanted response conform usual sentiment categories, need write custom parser extract ‘negative’, ‘neutral’, ‘positive’ output. Looking first output, ’s clear parser need reasonably sophisticated. send another request, asking model please output ‘positive’, ‘negative’, ‘neutral’ 5. Clearly work willing . ’ll look techniques deal systematically, achieve predictable outputs Structured Outputs section. model hand text difficult traditional, three-category 6, document-level sentiment analysis? [1] “sentiment text mixed, positive feelings expressed interface (”brilliant”) negative feelings performance (“absolutely dreadful”).” model smart enough recognise sentiment fit neatly ‘positive’, ‘negative’, ‘neutral’ output formatted nice way downstream use.","code":"system_prompt <- \"Analyse the text's sentiment: \"  text <- \"Oh man, I'm getting to the end of my limit with this thing. WHY DOESN'T IT JUST WORK?!?\" sentiment_request <- oai_build_completions_request(   text,   system_prompt = system_prompt )   sentiment_request |>   req_dry_run() POST /v1/chat/completions HTTP/1.1 accept: */* accept-encoding: deflate, gzip authorization:  content-length: 247 content-type: application/json host: api.openai.com user-agent: EndpointR  {   \"model\": \"gpt-4.1-nano\",   \"messages\": [     {       \"role\": \"system\",       \"content\": \"Analyse the text's sentiment: \"     },     {       \"role\": \"user\",       \"content\": \"Oh man, I'm getting to the end of my limit with this thing. WHY DOESN'T IT JUST WORK?!?\"     }   ],   \"temperature\": 0,   \"max_tokens\": 500 } sentiment_response <- sentiment_request |>    perform_request_or_return_error() sentiment_response |>   resp_check_status() |>    resp_body_json() |>    pluck(\"choices\", 1, \"message\", \"content\") ambiguous_text <- \"The interface is brilliant but the performance is absolutely dreadful\"  ambiguous_sentiment <- oai_build_completions_request(   ambiguous_text,   system_prompt = system_prompt ) |>    perform_request_or_return_error() |>    resp_body_json() |>    pluck(\"choices\", 1, \"message\", \"content\")"},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"multiple-texts","dir":"Articles","previous_headings":"OpenAI - The Basics > Completions API","what":"Multiple Texts","title":"Connecting to Major Model Providers","text":"Individual requests useful applications EndpointR built handle , whilst taking care implementation details like concurrent requests, retries, failing gracefully. Let’s experiment better system prompt list texts: running testing, took 8.5 seconds 10 requests send one time. took 1.8 seconds send 10 requests parallel - ~4.7x speed , showing overhead cost sending requests parallel - .e. see 10x speed increase 10x requests. Now extract content response, can see 7 response classification belonging classes - making prompt slightly better helped us get better final result. However, repeat many texts - hundreds/thousands, ’s unlikely every single response conform categories. Without instruction, models tendency slightly change output unpredictable intervals.","code":"updated_sentiment_prompt <- \"Classify the text into sentiment categories. The accepted categories are 'positive', 'negative', 'neutral', and 'mixed'. A 'mixed' text contains elements of positive and negative. \" classical_texts <- c(  \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",  \"All happy families are alike; each unhappy family is unhappy in its own way.\",  \"It was the best of times, it was the worst of times.\",  \"Call me Ishmael.\",  \"The sun shone, having no alternative, on the nothing new.\",  \"All animals are equal, but some animals are more equal than others.\",  \"So we beat on, boats against the current, borne back ceaselessly into the past.\",  \"The heart was made to be broken.\",  \"Tomorrow, and tomorrow, and tomorrow, creeps in this petty pace from day to day.\",  \"I have always depended on the kindness of strangers.\" ) classical_requests <- oai_build_completions_request_list(   classical_texts,   system_prompt = updated_sentiment_prompt ) start_seq <- Sys.time() classical_responses  <- classical_requests |>    perform_requests_with_strategy() end_seq <- Sys.time() - start_seq start_par <- Sys.time() classical_responses  <- classical_requests |>    perform_requests_with_strategy(concurrent_requests = 10) end_par <- Sys.time() - start_par classical_responses |>    map(~ resp_body_json(.x) |>          pluck(\"choices\", 1, \"message\", \"content\") |>          as_tibble()   ) |>    list_rbind() |>    mutate(text = classical_texts, .before = 1)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"data-frame-of-texts","dir":"Articles","previous_headings":"OpenAI - The Basics > Completions API","what":"Data Frame of Texts","title":"Connecting to Major Model Providers","text":"function oai_complete_df() takes data frame, id variable, text variable mandatory inputs, returns data frame columns: id_var, text+var, .error_msg, .error, .batch. oai_complete_df() function knobs can turn improve throughput, reduce memory usage, increase likelihood successful response. default split data frame chunk_size = chunks. chunk processed order, file created store results previous batches. end chunks results retrieved file. help reduce likelihood : Lose API responses long-running calculations fail part-way Run memory responses list grows size data frame. function reports progress happens, letting know chunk processed, e.g. 1/5, progress bar requests within chunk. function provides summary failures successes directly console chunk, end chunks.","code":"df_classical_texts <- tibble(   id = 1:10,   text = classical_texts ) oai_complete_df(df_classical_texts,                  text_var = text,                  id_var = id,                 concurrent_requests = 5                 )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"openai-structured-outputs","dir":"Articles","previous_headings":"OpenAI - The Basics","what":"OpenAI Structured Outputs","title":"Connecting to Major Model Providers","text":"textual responses get LLM providers difficult deal programmatically, make guarantees form response. example, ask LLM classify documents, sometimes give just classification - ‘positive’, sometimes add pre-amble ‘document positive’, sometimes something else entirely. detailed information creating JSON schemas structured outputs, see vignette(\"structured_outputs_json_schema\").","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"anthropic","dir":"Articles","previous_headings":"","what":"Anthropic","title":"Connecting to Major Model Providers","text":"TBC","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/llm_providers.html","id":"google","dir":"Articles","previous_headings":"","what":"Google","title":"Connecting to Major Model Providers","text":"TBC","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"introduction-to-structured-outputs","dir":"Articles","previous_headings":"","what":"Introduction to Structured Outputs","title":"Structured Outputs - JSON Schema","text":"Structured outputs ensure LLMs return data exactly format need. Instead parsing messy text, get validated JSON matching schema. use structured outputs? - eliminates parsing errors inconsistent formats - guarantees data types (numbers vs strings, booleans vs text) - enables reliable data extraction pipelines - reduces prompt engineering overhead Basic workflow: 1. define schema using helper functions 2. send requests OpenAI schema passed 3. validate response validate_response()","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"quick-start","dir":"Articles","previous_headings":"","what":"Quick Start","title":"Structured Outputs - JSON Schema","text":"WARNING: using schema structured outputs OpenAI, fields MUST required: NOTE: first time send request schema, take longer usual. “Typical schemas take 10 seconds process first request, complex schemas may take minute.” 1","code":"contact_schema <- create_json_schema(  name = \"contact_info\",  schema = schema_object(    name = schema_string(\"person's full name\"),    email = schema_string(\"email address\"),    phone = schema_string(\"phone number\"),    required = list(\"name\", \"email\", \"phone\"),    additional_properties = FALSE  ) )  req <- oai_build_completions_request(   input = \"Am I speaking with Margaret Phillips? Yes, ok, and your email is mphil@hotmail.co.uk. Ok perfect, and your phone number? Was that 07564789789? Ok great. Just a second please Margaret, you're verified\",   schema = contact_schema )  resp <- req_perform(req)  resp |>   resp_body_json() |>    pluck(\"choices\", 1, \"message\", \"content\") |>    validate_response(schema = contact_schema) |>    tibble::as_tibble()"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"schema-types","dir":"Articles","previous_headings":"","what":"Schema Types","title":"Structured Outputs - JSON Schema","text":"EndpointR provides several schema types match different data extraction needs. type enforces specific constraints validations: schema_string() - text data schema_number() / schema_integer() - numeric values optional min/max schema_boolean() - true/false values schema_enum() - predefined choices schema_array() - lists items schema_object() - nested structures Let’s explore type practical examples: can inspect schema print human-readable form json_dump() jsonlite::toJSON() Another, complicated, schema product review extraction, introduce schema_array:","code":"# text classification with enums sentiment_schema <- create_json_schema(   name = \"sentiment_analysis\",   schema = schema_object(     sentiment = schema_enum(       c(\"positive\", \"negative\", \"neutral\"),       \"overall sentiment of text\"     ),     confidence = schema_number(       \"confidence score\",        minimum = 0,        maximum = 1     ),     is_spam = schema_boolean(\"contains spam content\"),     required = c(\"sentiment\", \"confidence\", \"is_spam\")   ) ) json_dump(sentiment_schema) |>    jsonlite::toJSON(pretty = TRUE, auto_unbox = TRUE) rating_schema <- create_json_schema(   name = \"product_review\",   schema = schema_object(     rating = schema_integer(\"star rating\", minimum = 1, maximum = 5),     title = schema_string(\"review title\"),     pros = schema_array(       schema_string(),       \"positive aspects mentioned\"     ),     cons = schema_array(       schema_string(),       \"negative aspects mentioned\"     ),     would_recommend = schema_boolean(\"recommends product\"),     required = c(\"rating\",\"title\", \"pros\", \"cons\", \"would_recommend\")   ) )"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"complex-nested-structures","dir":"Articles","previous_headings":"","what":"Complex Nested Structures","title":"Structured Outputs - JSON Schema","text":"Finally fairly complex schema - supplier schema_object within schema_object, line_items sits within schema_object, schema_array, schema_object, multiple schema_* objects.","code":"# invoice parsing with line items invoice_schema <- create_json_schema(   name = \"invoice_data\",   schema = schema_object(     # header information     invoice_number = schema_string(\"invoice reference number\"),     issue_date = schema_string(\"date issued (YYYY-MM-DD format)\"),     due_date = schema_string(\"payment due date (YYYY-MM-DD format)\"),     # billing details     supplier = schema_object(       name = schema_string(\"supplier company name\"),       address = schema_string(\"supplier address\"),       vat_number = schema_string(\"VAT registration number\"),       required = c(\"name\")     ),     customer = schema_object(       name = schema_string(\"customer name\"),       address = schema_string(\"customer address\"),       required = c(\"name\")     ),     # line items array     line_items = schema_array(       schema_object(         description = schema_string(\"item description\"),         quantity = schema_integer(\"quantity ordered\", minimum = 1),         unit_price = schema_number(\"price per unit\", minimum = 0),         line_total = schema_number(\"total for this line\", minimum = 0),         required = c(\"description\", \"quantity\", \"unit_price\", \"line_total\")       ),       \"invoice line items\",       min_items = 1     ),     # totals     subtotal = schema_number(\"subtotal before tax\", minimum = 0),     vat_amount = schema_number(\"VAT amount\", minimum = 0),     total_amount = schema_number(\"final total amount\", minimum = 0)   ) )  invoice_schema@schema$required <-  names(invoice_schema@schema$properties)  # This helper line ensures ALL properties are marked as required, which is   mandatory for OpenAI's structured outputs. Without this, the API will reject  the schema. Use this pattern when you want all fields to be required rather  than listing them individually."},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"validation","dir":"Articles","previous_headings":"","what":"Validation","title":"Structured Outputs - JSON Schema","text":"schema type enforces specific constraints. method validating whether specific responses meet schema’s constraints.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"how-validation-works","dir":"Articles","previous_headings":"Validation","what":"How Validation Works","title":"Structured Outputs - JSON Schema","text":"using structured outputs LLM providers: API-side enforcement: provider ensures generated responses match schema Local validation: - validate_response() double-checks data integrity locally dual approach catches generation errors data transmission issues. ’s comprehensive example: QUESTION: ’s wrong schema want use OpenAI API? properties set required fields, meaning use schema OpenAI’s structured outputs. Now let’s see happens try validate mocked response object conforms schema: try validate mocked response object conform schema:","code":"user_profile_schema <- create_json_schema(  name = \"user_profile\",  schema = schema_object(    # string fields    name = schema_string(\"full name\"),    bio = schema_string(\"user biography\"),        # numeric fields    age = schema_integer(\"age in years\", minimum = 13, maximum = 120),    account_balance = schema_number(\"balance in pounds\", minimum = 0),    is_verified = schema_boolean(\"account verified status\"),    newsletter_opt_in = schema_boolean(\"subscribed to newsletter\"),    subscription_tier = schema_enum(      c(\"free\", \"premium\", \"enterprise\"),      \"subscription level\"),    priority = schema_enum(      c(1, 2, 3),      \"support priority level\",      type = \"integer\"    ),    interests = schema_array(      schema_string(),      \"user interests\",      min_items = 1,      max_items = 10    ),        required = c(\"name\", \"age\", \"is_verified\", \"subscription_tier\")  ) ) valid_user <- '{   \"name\": \"Alice Smith\",   \"age\": 28,   \"account_balance\": 156.75,   \"is_verified\": true,   \"newsletter_opt_in\": false,   \"subscription_tier\": \"premium\",   \"priority\": 2,   \"interests\": [\"data science\", \"functional programming\", \"statistics\"] }'  validated_data <- validate_response(user_profile_schema, valid_user) str(validated_data) invalid_age <- '{   \"name\": \"Young User\",   \"age\": 10,   \"is_verified\": true,   \"subscription_tier\": \"free\" }'  validate_response(user_profile_schema, invalid_age)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"working-with-s7-objects","dir":"Articles","previous_headings":"","what":"Working with S7 Objects","title":"Structured Outputs - JSON Schema","text":"EndpointR uses S7 objects schema system. provides better type safety validation, means familiar S3 methods won’t work expected. Understanding work objects help debug issues customise schemas. example, call: jsonlite::toJSON(contact_schema). try, ’ll get error: Error: method asJSON S3 class: S7_object Instead, use S7 method json_dump defined json_schema class. converts schema R list ready converted JSON. won’t print list -long ugly, instead can check structure: Alongside json_dump EndpointR::json_schema given validate_response method saw used earlier quickstart.","code":"contact_json_dump <- json_dump(contact_schema) str(contact_json_dump)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"converting-schema-objects-to-json","dir":"Articles","previous_headings":"Working with S7 Objects","what":"Converting Schema Objects to JSON","title":"Structured Outputs - JSON Schema","text":"can convert dumped schema JSON object using {jsonlite}‘s toJSON function. object now class ’json’. can convert schema back regular R list {jsonlite}’s fromJSON function:","code":"contact_json_schema <-    toJSON(contact_json_dump,                   pretty = TRUE,                   auto_unbox = TRUE)  class(contact_json_schema) {   \"type\": \"json_schema\",   \"json_schema\": {     \"name\": \"contact_info\",     \"schema\": {       \"type\": \"object\",       \"properties\": {         \"name\": {           \"type\": \"string\",           \"description\": \"person's full name\"         },         \"email\": {           \"type\": \"string\",           \"description\": \"email address\"         },         \"phone\": {           \"type\": \"string\",           \"description\": \"phone number\"         }       },       \"additionalProperties\": false,       \"required\": [\"name\", \"email\", \"phone\"]     },     \"strict\": true   } } from_contact_json_schema <- fromJSON(contact_json_schema)  class(from_contact_json_schema)"},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best Practices","title":"Structured Outputs - JSON Schema","text":"Schema design principles: Use descriptive field names descriptions Set appropriate constraints (min/max values, required fields) Prefer enums free text categories Nest objects logically complex data Validate mock responses advance","code":""},{"path":"https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.html","id":"troubleshooting-tips","dir":"Articles","previous_headings":"","what":"Troubleshooting tips:","title":"Structured Outputs - JSON Schema","text":"Use json_dump() inspect final schema structure use jsonlite::toJSON(x, pretty = TRUE) view schema human-readable form Test schemas mock data using validate_response() Start simple add complexity incrementally Check enum values match expected model outputs Validate required fields cover essential data Make sure properties required using OpenAI API structured outputs","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jack Penzer. Author, maintainer.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Penzer J (2025). EndpointR: Connects various Machine Learning inference providers. R package version 0.1.1, https://jpcompartir.github.io/EndpointR/.","code":"@Manual{,   title = {EndpointR: Connects to various Machine Learning inference providers},   author = {Jack Penzer},   year = {2025},   note = {R package version 0.1.1},   url = {https://jpcompartir.github.io/EndpointR/}, }"},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"endpointr","dir":"","previous_headings":"","what":"EndpointR","title":"EndpointR","text":"EndpointR ‘batteries included’, open-source R package connecting various Application Programming Interfaces (APIs) Machine Learning model predictions. TIP: experienced programmer, experience hitting APIs, consider going directly httr2","code":""},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"EndpointR","text":"EndpointR put CRAN, can download install latest development version following code:","code":"library(EndpointR) remotes::install_github(\"jpcompartir/EndpointR\")"},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"hugging-face---embeddings","dir":"","previous_headings":"","what":"Hugging Face - embeddings","title":"EndpointR","text":"Securely set API key Point endpoint - ‘-mpnet-base-v2’ model feature extraction (embeddings) Embed single text: Embed list texts batches: Embed data frame texts:","code":"set_api_key(\"HF_API_KEY\") endpoint_url <- \"https://router.huggingface.co/hf-inference/models/sentence-transformers/all-mpnet-base-v2/pipeline/feature-extraction\" hf_embed_text(   text = \"Convert this text to embeddings\",   endpoint_url = endpoint_url,   key_name = \"HF_API_KEY\" ) review_texts <-c(     \"Absolutely fantastic service! The staff were incredibly helpful and friendly.\",     \"Terrible experience. Food was cold and the waiter was rude.\",     \"Pretty good overall, but nothing special. Average food and service.\",     \"Outstanding meal! Best restaurant I've been to in years. Highly recommend!\",     \"Disappointed with the long wait times. Food was okay when it finally arrived.\"   )  hf_embed_batch(   texts = review_texts,   endpoint_url = endpoint_url,   key_name = \"HF_API_KEY\",   batch_size = 3,   concurrent_requests = 2 ) review_data <- tibble::tibble(   review_id = 1:5,   review_text = review_texts ) hf_embed_df(   df = review_data,   text_var = review_text,   id_var = review_id,   endpoint_url = endpoint_url,   key_name = \"HF_API_KEY\",   concurrent_requests = 2,   batch_size = 3 )"},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"hugging-face---classification","dir":"","previous_headings":"","what":"Hugging Face - Classification","title":"EndpointR","text":"Select Classification Endpoint URL Classify single text: ’ll need grab label2id mapping model’s card: Cardiff NLP model info Classify data frame: Read Hugging Face Inference Vignette infromation embedding classifying using Dedicated Inference Endpoints Inference API Hugging Face.","code":"sentiment_endpoint <- \"https://router.huggingface.co/hf-inference/models/cardiffnlp/twitter-roberta-base-sentiment\" labelid_2class <- function() {   return(list(negative = \"LABEL_0\",               neutral = \"LABEL_1\",               positive = \"LABEL_2\")) }   hf_classify_text(   text = review_texts[[1]],   endpoint_url = sentiment_endpoint,   key_name = \"HF_API_KEY\" ) |>     dplyr::rename(!!!labelid_2class()) hf_classify_df(   df = review_data,   text_var = review_text,   id_var = review_id,   endpoint_url = sentiment_endpoint,   key_name = \"HF_API_KEY\",   batch_size = 8,   concurrent_requests = 3 ) |>   dplyr::rename(!!!labelid_2class())"},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"openai---chat-completions-api","dir":"","previous_headings":"","what":"OpenAI - Chat Completions API","title":"EndpointR","text":"Make sure ’ve set API key: Complete single text: Complete single text schema tidy: Complete Data Frame texts: Complete Data Frame texts schema: Read LLM Providers Vignette, Structured Outputs Vignette information common workflows OpenAI Chat Completions API 1","code":"set_api_key(\"OPENAI_API_KEY\") oai_complete_text(   text = review_texts[[2]],   system_prompt = \"Classify the sentiment of the following text: \" ) sentiment_schema <- create_json_schema(   name = \"sentiment_analysis\",   schema = schema_object(     sentiment = schema_string(\"positive, negative, or neutral\"),     confidence = schema_number(\"confidence score between 0 and 1\"), # we don't necessarily recommend asking a model for its confidence score, this is mainly a schema-construction demo!     required = list(\"sentiment\", \"confidence\")   ) )  oai_complete_text(   text = review_texts[[2]],   system_prompt = \"Classify the sentiment of the following text: \",   schema = sentiment_schema,   tidy = TRUE ) |>    tibble::as_tibble() oai_complete_df(   df = review_data,   text_var = review_text,   id_var = review_id,   system_prompt = \"Classify the following review:\",   key_name = \"OPENAI_API_KEY\",   concurrent_requests = 5 # send 5 rows of data simultaneously ) oai_complete_df(   df = review_data,   text_var = review_text,   id_var = review_id,   system_prompt = \"Classify the following review:\",   schema = sentiment_schema,   key_name = \"OPENAI_API_KEY\",   concurrent_requests = 5 # send 5 rows of data simultaneously )"},{"path":"https://jpcompartir.github.io/EndpointR/index.html","id":"api-key-security","dir":"","previous_headings":"","what":"API Key Security","title":"EndpointR","text":"Read httr2 vignette managing API keys securely encrypting . Read EndpointR API Keys vignette information API keys need wach endpoint support, securely import API keys .Renvironfile.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/EndpointR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"EndpointR: Connects to various Machine Learning inference providers — EndpointR-package","title":"EndpointR: Connects to various Machine Learning inference providers — EndpointR-package","text":"EndpointR 'batteries included', open-source R package connecting various APIs Machine Learning model predictions. EndpointR built company-specific use cases, may useful wide audience.","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/reference/EndpointR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"EndpointR: Connects to various Machine Learning inference providers — EndpointR-package","text":"Maintainer: Jack Penzer Jack.penzer@sharecreative.com","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/base_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a base HTTP POST request for API endpoints — base_request","title":"Create a base HTTP POST request for API endpoints — base_request","text":"Constructs base httr2 POST request object common headers authentication. function sets foundation API requests standard configuration.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/base_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a base HTTP POST request for API endpoints — base_request","text":"","code":"base_request(endpoint_url, api_key)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/base_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a base HTTP POST request for API endpoints — base_request","text":"endpoint_url Character string containing API endpoint URL api_key Character string containing API authentication key","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/base_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a base HTTP POST request for API endpoints — base_request","text":"httr2_request object configured POST method, JSON content type, bearer token authentication","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/base_request.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a base HTTP POST request for API endpoints — base_request","text":"","code":"if (FALSE) { # \\dontrun{   # Create a base POST request for an API endpoint   req <- base_request(     endpoint_url = \"https://api.example.com/v1/endpoint\",     api_key = \"your-api-key-here\"   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.html","id":null,"dir":"Reference","previous_headings":"","what":"Batch concurrent benchmark results — batch_concurrent_benchmark","title":"Batch concurrent benchmark results — batch_concurrent_benchmark","text":"Benchmark data comparing different batch sizes concurrent request configurations Hugging Face API calls.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Batch concurrent benchmark results — batch_concurrent_benchmark","text":"","code":"batch_concurrent_benchmark"},{"path":"https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Batch concurrent benchmark results — batch_concurrent_benchmark","text":"data frame 20 rows 7 variables: batch_size Integer; size batch processed concurrent_requests Integer; number concurrent requests chunk_index Integer; index processed chunk elapsed_time Numeric; time taken seconds success Logical; whether operation succeeded rows_processed Integer; number rows successfully processed throughput Numeric; requests processed per second","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Batch concurrent benchmark results — batch_concurrent_benchmark","text":"Internal benchmarking EndpointR functions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.html","id":null,"dir":"Reference","previous_headings":"","what":"Split a data frame into chunks for batch processing — chunk_dataframe","title":"Split a data frame into chunks for batch processing — chunk_dataframe","text":"Splits data frame chunks specified size.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split a data frame into chunks for batch processing — chunk_dataframe","text":"","code":"chunk_dataframe(df, chunk_size)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split a data frame into chunks for batch processing — chunk_dataframe","text":"df data frame split batches chunk_size Number rows per batch","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split a data frame into chunks for batch processing — chunk_dataframe","text":"list data frames, chunk_size rows","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/create_json_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a JSON Schema object — create_json_schema","title":"Create a JSON Schema object — create_json_schema","text":"Create JSON Schema object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/create_json_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a JSON Schema object — create_json_schema","text":"","code":"create_json_schema(name, schema, strict = TRUE, description = \"\")"},{"path":"https://jpcompartir.github.io/EndpointR/reference/create_json_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a JSON Schema object — create_json_schema","text":"name Name schema schema JSON schema definition list strict Whether enforce strict mode (default TRUE) description Optional description schema","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/create_json_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a JSON Schema object — create_json_schema","text":"json_schema S7 object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_embeddings_hf.html","id":null,"dir":"Reference","previous_headings":"","what":"Example embedding results from Hugging Face API — df_embeddings_hf","title":"Example embedding results from Hugging Face API — df_embeddings_hf","text":"sample dataset containing text embeddings generated using Hugging Face's embedding API. dataset demonstrates structure results returned hf_embed_batch() hf_embed_df() functions.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_embeddings_hf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example embedding results from Hugging Face API — df_embeddings_hf","text":"","code":"df_embeddings_hf"},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_embeddings_hf.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example embedding results from Hugging Face API — df_embeddings_hf","text":"data frame 3 rows 773 variables: id Integer; unique identifier text text Character; original text embedded category Character; category classification text .error Logical; whether embedding process failed .error_message Character; error message embedding failed (NA successful) V1, V2, ..., V768 Numeric; embedding vector dimensions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_embeddings_hf.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Example embedding results from Hugging Face API — df_embeddings_hf","text":"Generated using Hugging Face embedding model via EndpointR functions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_sentiment_classification_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Example sentiment classification results from Hugging Face API — df_sentiment_classification_example","title":"Example sentiment classification results from Hugging Face API — df_sentiment_classification_example","text":"sample dataset containing sentiment classification results generated using Hugging Face's classification API. dataset demonstrates structure results returned hf_classify_batch() hf_classify_df() functions.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_sentiment_classification_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example sentiment classification results from Hugging Face API — df_sentiment_classification_example","text":"","code":"df_sentiment_classification_example"},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_sentiment_classification_example.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example sentiment classification results from Hugging Face API — df_sentiment_classification_example","text":"data frame 3 rows 7 variables: id Integer; unique identifier text text Character; original text classified category Character; category classification text NEGATIVE Numeric; probability score negative sentiment (0-1) POSITIVE Numeric; probability score positive sentiment (0-1) .error Logical; whether classification process failed .error_message Character; error message classification failed (NA successful)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/df_sentiment_classification_example.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Example sentiment classification results from Hugging Face API — df_sentiment_classification_example","text":"Generated using Hugging Face sentiment classification model via EndpointR functions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/dot-create_error_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Create standardised error tibble for failed requests — .create_error_tibble","title":"Create standardised error tibble for failed requests — .create_error_tibble","text":"Internal function create consistent error tibble structure. Ensures uniform error reporting across different failure modes.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/dot-create_error_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create standardised error tibble for failed requests — .create_error_tibble","text":"","code":".create_error_tibble(indices, error_message)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/dot-create_error_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create standardised error tibble for failed requests — .create_error_tibble","text":"indices Vector indices indicating original request positions error_message Character string condition object describing error","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/dot-create_error_tibble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create standardised error tibble for failed requests — .create_error_tibble","text":"tibble columns: original_index: Position original request batch .error: Always TRUE error tibbles .error_message: Character description error","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/dot-extract_successful_completion_content.html","id":null,"dir":"Reference","previous_headings":"","what":"should only pass in successes here, as we're not going to validate types here or try to catch errors and this is marginally quicker/memory efficient than pipe + pluck — .extract_successful_completion_content","title":"should only pass in successes here, as we're not going to validate types here or try to catch errors and this is marginally quicker/memory efficient than pipe + pluck — .extract_successful_completion_content","text":"pass successes , going validate types try catch errors marginally quicker/memory efficient pipe + pluck","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/dot-extract_successful_completion_content.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"should only pass in successes here, as we're not going to validate types here or try to catch errors and this is marginally quicker/memory efficient than pipe + pluck — .extract_successful_completion_content","text":"","code":".extract_successful_completion_content(resp)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"Retrieve API key .Renviron. API key set via set_api_key()manually placed .Renviron file.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"","code":"get_api_key(key_name)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"key_name name API format \"ENDPOINT_API_KEY\" -> \"ANTHROPIC_API_KEY\"","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"character","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/get_api_key.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve an API key which has been stored as an Environment Variable. — get_api_key","text":"","code":"if (FALSE) { # \\dontrun{   # retrieve an Anthropic API key   anthropic_key <- get_api_key(\"ANTHROPIC_API_KEY\")    # use the key in a function call   my_function(api_key = anthropic_key) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare a single text embedding request — hf_build_request","title":"Prepare a single text embedding request — hf_build_request","text":"Creates httr2 request object obtaining response Hugging Face Inference endpoint single text input. function can used multiple tasks, .e. embedding input, classifying input","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare a single text embedding request — hf_build_request","text":"","code":"hf_build_request(   input,   endpoint_url,   key_name,   parameters = list(),   max_retries = 5,   timeout = 10,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare a single text embedding request — hf_build_request","text":"input Character string get response endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key parameters Advanced usage: parameters pass API endpoint max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating request","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare a single text embedding request — hf_build_request","text":"httr2 request object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare a single text embedding request — hf_build_request","text":"developers, function can form basis single requests, mapped list requests.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare a single text embedding request — hf_build_request","text":"","code":"if (FALSE) { # \\dontrun{   # Create request using API key from environment   req <- hf_build_request(     input = \"This is a sample text to embed\",     endpoint_url = \"https://my-endpoint.huggingface.cloud/embedding_api\",     key_name = \"HF_API_KEY\"   )    # Using default key name   req <- hf_build_request(     input = \"This is a sample text to classify\",     endpoint_url = \"https://my-endpoint.huggingface.cloud/classification_api\"   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare a batch request for multiple texts — hf_build_request_batch","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"Creates httr2 request object obtaining response Hugging Face Inference endpoint multiple text inputs single batch. function can used various tasks, embedding classifying multiple inputs simultaneously.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"","code":"hf_build_request_batch(   inputs,   parameters = list(),   endpoint_url,   key_name,   max_retries = 5,   timeout = 10,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"inputs Vector list character strings process batch parameters Parameters send inputs endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating request","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"httr2 request object configured batch processing","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"developers, function forms basis batch requests, enabling efficient processing multiple inputs single API call.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare a batch request for multiple texts — hf_build_request_batch","text":"","code":"if (FALSE) { # \\dontrun{   # Create batch request using API key from environment   batch_req <- hf_build_request_batch(     inputs = c(\"First text to embed\", \"Second text to embed\"),     endpoint_url = \"https://my-endpoint.huggingface.cloud/embedding_api\",     key_name = \"HF_API_KEY\"   )    # Using custom timeout and retry settings   batch_req <- hf_build_request_batch(     inputs = c(\"Text one\", \"Text two\", \"Text three\"),     endpoint_url = \"https://my-endpoint.huggingface.cloud/embedding_api\",     key_name = \"HF_API_KEY\",     max_retries = 3,     timeout = 15   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare embedding requests for texts in a data frame — hf_build_request_df","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"Creates httr2 request objects text data frame column. Thus function handles request creation, handle performing request, tidying response. perform request, select appropriate *_perform_* function.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"","code":"hf_build_request_df(   df,   text_var,   id_var,   endpoint_url,   key_name,   parameters = list(),   max_retries = 3,   timeout = 10,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"df data frame containing texts embed text_var Name column containing text send endpoint id_var Name column use ID (optional) endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key parameters Parameters send inputs max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating requests","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"data frame original data plus request objects","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare embedding requests for texts in a data frame — hf_build_request_df","text":"","code":"if (FALSE) { # \\dontrun{   # Prepare requests for a data frame   df <- data.frame(     id = 1:3,     text = c(\"First example\", \"Second example\", \"Third example\")   )    requests_df <- hf_build_request_df(     df = df,     text_var = text,     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     id_var = id   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"Classifies batch texts using Hugging Face classification endpoint returns classification scores tidy format. Handles batching, concurrent requests, error recovery automatically.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"","code":"hf_classify_batch(   texts,   endpoint_url,   key_name,   ...,   tidy_func = tidy_batch_classification_response,   parameters = list(return_all_scores = TRUE),   batch_size = 8,   progress = TRUE,   concurrent_requests = 5,   max_retries = 5,   timeout = 30,   include_texts = TRUE,   relocate_col = 2 )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"texts Character vector texts classify endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... Additional arguments passed request functions tidy_func Function process API responses, defaults tidy_batch_classification_response parameters List parameters API endpoint, defaults list(return_all_scores = TRUE) batch_size Integer; number texts per batch (default: 8) progress Logical; whether show progress bar (default: TRUE) concurrent_requests Integer; number concurrent requests (default: 5) max_retries Integer; maximum retry attempts (default: 5) timeout Numeric; request timeout seconds (default: 20) include_texts Logical; whether include original texts output (default: TRUE) relocate_col Integer; column position text column (default: 2)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"Data frame classification scores text, plus columns original text (include_texts=TRUE), error status, error messages","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"function processes multiple texts efficiently splitting batches optionally sending concurrent requests. includes robust error handling progress reporting large batches. function automatically handles request failures retries includes error information output requests fail. Original text order preserved results. function currently handle list(return_all_scores = FALSE).","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classify multiple texts using Hugging Face Inference Endpoints — hf_classify_batch","text":"","code":"if (FALSE) { # \\dontrun{   texts <- c(     \"This product is brilliant!\",     \"Terrible quality, waste of money\",     \"Average product, nothing special\"   )    results <- hf_classify_batch(     texts = texts,     endpoint_url = \"redacted\",     key_name = \"API_KEY\",     batch_size = 3   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"Classifies texts data frame column using Hugging Face classification endpoint joins results back original data frame.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"","code":"hf_classify_df(   df,   text_var,   id_var,   endpoint_url,   key_name,   ...,   tidy_func = tidy_batch_classification_response,   parameters = list(return_all_scores = TRUE),   batch_size = 4,   concurrent_requests = 1,   max_retries = 5,   timeout = 30,   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"df Data frame containing texts classify text_var Column name containing texts classify (unquoted) id_var Column name use identifier joining (unquoted) endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... Additional arguments passed request functions tidy_func Function process API responses, defaults tidy_batch_classification_response parameters List parameters API endpoint, defaults list(return_all_scores = TRUE) batch_size Integer; number texts per batch (default: 4) concurrent_requests Integer; number concurrent requests (default: 1) max_retries Integer; maximum retry attempts (default: 5) timeout Numeric; request timeout seconds (default: 30) progress Logical; whether show progress bar (default: TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"Original data frame additional columns classification scores, classification results table row counts match","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"function extracts texts specified column, classifies using hf_classify_batch(), joins classification results back original data frame using specified ID column. function preserves original data frame structure adds new columns classification scores. number rows match processing (due errors), returns classification results separately warning. function currently handle list(return_all_scores = FALSE).","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classify a data frame of texts using Hugging Face Inference Endpoints — hf_classify_df","text":"","code":"if (FALSE) { # \\dontrun{   df <- data.frame(     id = 1:3,     review = c(\"Excellent service\", \"Poor quality\", \"Average experience\")   )    classified_df <- hf_classify_df(     df = df,     text_var = review,     id_var = id,     endpoint_url = \"redacted\",     key_name = \"API_KEY\"   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"Sends text Hugging Face classification endpoint returns classification scores. default, returns tidied data frame one row columns classification label.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"","code":"hf_classify_text(   text,   endpoint_url,   key_name,   ...,   parameters = list(return_all_scores = TRUE),   tidy = TRUE,   max_retries = 5,   timeout = 20,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"text Character string classify endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... Additional arguments passed hf_perform_request ultimately httr2::req_perform parameters Advanced usage: parameters pass API endpoint, defaults list(return_all_scores = TRUE). tidy Logical; TRUE (default), returns tidied data frame max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Logical; whether validate endpoint creating request","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"tidied data frame classification scores (tidy=TRUE) raw API response","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"function handles entire process creating request Hugging Face Inference API endpoint text classification, sending request, processing response. function automatically retry failed requests according max_retries parameter. tidy=TRUE (default), transforms nested JSON response tidy data frame one row columns classification label. tidying fails, function returns raw response informative message.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classify text using a Hugging Face Inference API endpoint — hf_classify_text","text":"","code":"if (FALSE) { # \\dontrun{   # Basic classification with default parameters   result <- hf_classify_text(     text = \"This product is excellent!\",     endpoint_url = \"redacted\",     key_name = \"API_KEY\"   )    # Classification with custom parameters for a spam detection model   spam_result <- hf_classify_text(     text = \"URGENT: You've won a free holiday! Call now to claim.\",     endpoint_url = \"redacted\",     parameters = list(return_all_scores = TRUE)   )    # Get raw response without tidying   raw_result <- hf_classify_text(     text = \"I love this movie\",     endpoint_url = \"redacted\",     key_name = \"API_KEY\",     tidy = FALSE   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate batches of embeddings for a list of texts — hf_embed_batch","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"High-level function generate embeddings multiple text strings. function handles batching parallel processing embedding requests, attempts handle errors gracefully.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"","code":"hf_embed_batch(   texts,   endpoint_url,   key_name,   ...,   tidy_func = tidy_embedding_response,   parameters = list(),   batch_size = 8,   include_texts = TRUE,   concurrent_requests = 5,   max_retries = 5,   timeout = 10,   validate = FALSE,   relocate_col = 2 )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"texts Vector list character strings get embeddings endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... ellipsis sent hf_perform_request TODO (reserved ATM) tidy_func Function process/tidy raw API response (default: tidy_embedding_response) parameters Advanced usage: parameters pass API endpoint. batch_size Number texts process one batch include_texts Whether return original texts return tibble concurrent_requests Number requests send simultaneously max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating request relocate_col position data frame relocate results .","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"tibble containing embedding vectors","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate batches of embeddings for a list of texts — hf_embed_batch","text":"","code":"if (FALSE) { # \\dontrun{   # Generate embeddings for multiple texts using default batch size   embeddings <- hf_embed_batch(     texts = c(\"First example\", \"Second example\", \"Third example\"),     endpoint_url = \"https://my-endpoint.huggingface.cloud\"   )    # With custom batch size and concurrent requests   embeddings <- hf_embed_batch(     texts = c(\"First example\", \"Second example\", \"Third example\"),     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     batch_size = 10,     concurrent_requests = 5   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate embeddings for texts in a data frame — hf_embed_df","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"High-level function generate embeddings texts data frame. function handles entire process request creation response processing, options batching & parallel execution. Setting number retries","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"","code":"hf_embed_df(   df,   text_var,   id_var,   endpoint_url,   key_name,   batch_size = 8,   concurrent_requests = 1,   max_retries = 5,   timeout = 15,   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"df data frame containing texts embed text_var Name column containing text embed id_var Name column use ID endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key batch_size Number texts process one batch (NULL batching) concurrent_requests Number requests send . APIs allow multiple requests. max_retries Maximum number retry attempts failed requests. timeout Request timeout seconds progress Whether display progress bar","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"data frame original data plus embedding columns","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate embeddings for texts in a data frame — hf_embed_df","text":"","code":"if (FALSE) { # \\dontrun{   # Generate embeddings for a data frame   df <- data.frame(     id = 1:3,     text = c(\"First example\", \"Second example\", \"Third example\")   )    # Use parallel processing without batching   embeddings_df <- hf_embed_df(     df = df,     text_var = text,     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     id_var = id,     parallel = TRUE,     batch_size = NULL   )    # Use batching without parallel processing   embeddings_df <- hf_embed_df(     df = df,     text_var = text,     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     id_var = id,     parallel = FALSE,     batch_size = 10   )    # Use both batching and parallel processing   embeddings_df <- hf_embed_df(     df = df,     text_var = text,     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     id_var = id,     parallel = TRUE,     batch_size = 10   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate embeddings for a single text — hf_embed_text","title":"Generate embeddings for a single text — hf_embed_text","text":"High-level function generate embeddings single text string. function handles entire process request creation response processing.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate embeddings for a single text — hf_embed_text","text":"","code":"hf_embed_text(   text,   endpoint_url,   key_name,   ...,   parameters = list(),   tidy = TRUE,   max_retries = 3,   timeout = 10,   validate = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate embeddings for a single text — hf_embed_text","text":"text Character string get embeddings endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key ... ellipsis sent hf_perform_request, forwards httr2::req_perform parameters Advanced usage: parameters pass API endpoint tidy Whether attempt tidy response max_retries Maximum number retry attempts failed requests timeout Request timeout seconds validate Whether validate endpoint creating request","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate embeddings for a single text — hf_embed_text","text":"tibble containing embedding vectors","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate embeddings for a single text — hf_embed_text","text":"","code":"if (FALSE) { # \\dontrun{   # Generate embeddings using API key from environment   embeddings <- hf_embed_text(     text = \"This is a sample text to embed\",     endpoint_url = \"https://my-endpoint.huggingface.cloud\"   )    # With custom API key environment variable name   embeddings <- hf_embed_text(     text = \"This is a sample text to embed\",     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     key_name = \"MY_CUSTOM_API_KEY\"   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute a single embedding request and process the response — hf_perform_request","title":"Execute a single embedding request and process the response — hf_perform_request","text":"Performs prepared request returns response","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute a single embedding request and process the response — hf_perform_request","text":"","code":"hf_perform_request(request, ...)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute a single embedding request and process the response — hf_perform_request","text":"request httr2 request object created hf_build_request ... ellipsis sent httr2::req_perform, e.g. path verbosityarguments.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute a single embedding request and process the response — hf_perform_request","text":"httr2 response object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Execute a single embedding request and process the response — hf_perform_request","text":"","code":"if (FALSE) { # \\dontrun{   # Create and perform request   req <- hf_build_request(     input = \"This is a sample text to embed\",     endpoint_url = \"https://my-endpoint.huggingface.cloud\"   )   embeddings <- hf_perform_request(req)  } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/json_dump.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert json_schema to API format — json_dump","title":"Convert json_schema to API format — json_dump","text":"Convert json_schema API format","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/json_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create JSON Schema S7 class for structured outputs — json_schema","title":"Create JSON Schema S7 class for structured outputs — json_schema","text":"S7 class JSON Schema definitions used , e.g. OpenAI structured outputs","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/json_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create JSON Schema S7 class for structured outputs — json_schema","text":"","code":"json_schema(   name = character(0),   schema = list(),   strict = TRUE,   description = \"\" )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/json_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create JSON Schema S7 class for structured outputs — json_schema","text":"name Name schema schema JSON schema definition list strict Whether enforce strict mode (default TRUE) description Optional description schema","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Build an OpenAI API Chat Completions request — oai_build_completions_request","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"function constructs httr2 request object specifically tailored interacting OpenAI's Chat Completions API. handles formatting messages, model parameters, optional JSON schema structured responses.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"","code":"oai_build_completions_request(   input,   endpointr_id = NULL,   model = \"gpt-4.1-nano\",   temperature = 0,   max_tokens = 500L,   schema = NULL,   system_prompt = NULL,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\",   timeout = 20,   max_retries = 5 )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"input Text input send model endpointr_id id persist response model OpenAI model use (default: \"gpt-4.1-nano\") temperature Sampling temperature (0-2), higher values = randomness max_tokens Maximum tokens response schema Optional JSON schema structured output (json_schema object list) system_prompt Optional system prompt key_name Environment variable name API key endpoint_url OpenAI API endpoint URL timeout Request timeout seconds max_retries Maximum number retry attempts failed requests","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"httr2 request object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build an OpenAI API Chat Completions request — oai_build_completions_request","text":"function simplifies process making calls OpenAI Chat Completions API assembling request body according API's specifications. input system_prompt (provided) automatically structured required 'messages' array format API. structured outputs (JSON mode), need provide valid JSON schema via schema parameter. can pre-formatted list object class \"json_schema\". schema provided, function automatically set schema$additionalProperties <- FALSE ensure schema$strict <- TRUE (strict already defined schema) encourage predictable reliable structured outputs API. also good idea set temperature 0 extracting structured outputs.","code":""},{"path":[]},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Build OpenAI requests for batch processing — oai_build_completions_request_list","title":"Build OpenAI requests for batch processing — oai_build_completions_request_list","text":"Build OpenAI requests batch processing","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build OpenAI requests for batch processing — oai_build_completions_request_list","text":"","code":"oai_build_completions_request_list(   inputs,   endpointr_ids = NULL,   model = \"gpt-4.1-nano\",   temperature = 0,   max_tokens = 500L,   schema = NULL,   system_prompt = NULL,   max_retries = 5L,   timeout = 30,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\" )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build OpenAI requests for batch processing — oai_build_completions_request_list","text":"inputs Character vector text inputs endpointr_ids vector IDs persist responses model OpenAI model use temperature Sampling temperature max_tokens Maximum tokens per response schema Optional JSON schema structured output system_prompt Optional system prompt max_retries Integer; maximum retry attempts (default: 5) timeout Numeric; request timeout seconds (default: 30) key_name Environment variable name API key endpoint_url OpenAI API endpoint URL","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build OpenAI requests for batch processing — oai_build_completions_request_list","text":"List httr2 request objects","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Build OpenAI embedding API request — oai_build_embedding_request","title":"Build OpenAI embedding API request — oai_build_embedding_request","text":"Creates httr2 request object configured OpenAI's embedding API. lower-level function handles request configuration including authentication, retries, timeouts.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build OpenAI embedding API request — oai_build_embedding_request","text":"","code":"oai_build_embedding_request(   input,   model = \"text-embedding-3-small\",   dimensions = NULL,   max_retries = 5,   timeout = 20,   endpoint_url = \"https://api.openai.com/v1/embeddings\",   key_name = \"OPENAI_API_KEY\",   verbose = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build OpenAI embedding API request — oai_build_embedding_request","text":"input Character vector text(s) embed model OpenAI embedding model use (default: \"text-embedding-3-small\") dimensions Number embedding dimensions (NULL uses model default) max_retries Maximum retry attempts failed requests (default: 5) timeout Request timeout seconds (default: 20) endpoint_url OpenAI API endpoint URL (default: OpenAI's embedding endpoint) key_name Name environment variable containing API key (default: \"OPENAI_API_KEY\") verbose Whether enable verbose request logging (default: FALSE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build OpenAI embedding API request — oai_build_embedding_request","text":"httr2 request object configured OpenAI embedding API. request object includes total_chars attribute containing total character count input texts.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build OpenAI embedding API request — oai_build_embedding_request","text":"function builds HTTP request execute . request can performed using httr2::req_perform() package's hf_perform_request() function. Note OpenAI limits input length - individual inputs exceed model's token limit (typically 8192 tokens embedding models). Empty strings allowed input.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build OpenAI embedding API request — oai_build_embedding_request","text":"","code":"if (FALSE) { # \\dontrun{   # Build a simple request   req <- oai_build_embedding_request(\"Hello world\")    # Build request with custom dimensions   req <- oai_build_embedding_request(     input = \"Hello world\",     dimensions = 512,     model = \"text-embedding-3-large\"   )    # Perform the request   response <- httr2::req_perform(req) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.html","id":null,"dir":"Reference","previous_headings":"","what":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","title":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","text":"function processes large volumes text OpenAI's Chat Completions API configurable chunks, writing results progressively CSV file. handles concurrent requests, automatic retries, structured outputs managing memory efficiently large-scale processing.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","text":"","code":"oai_complete_chunks(   texts,   ids,   chunk_size = 5000L,   model = \"gpt-4.1-nano\",   system_prompt = NULL,   output_file = \"auto\",   schema = NULL,   concurrent_requests = 5L,   temperature = 0L,   max_tokens = 500L,   max_retries = 5L,   timeout = 30L,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\" )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","text":"texts Character vector texts process ids Vector unique identifiers corresponding text (length texts) chunk_size Number texts process batch (default: 5000) model OpenAI model use (default: \"gpt-4.1-nano\") system_prompt Optional system prompt applied requests output_file Path .CSV file results. \"auto\" generates filename, location persistent across sessions. NULL, generates timestamped filename. schema Optional JSON schema structured output (json_schema object list) concurrent_requests Integer; number concurrent requests (default: 5) temperature Sampling temperature (0-2), lower = deterministic (default: 0) max_tokens Maximum tokens per response (default: 500) max_retries Maximum retry attempts per failed request (default: 5) timeout Request timeout seconds (default: 30) key_name Name environment variable containing API key (default: OPENAI_API_KEY) endpoint_url OpenAI API endpoint URL","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","text":"tibble containing results columns: id: Original identifier input content: API response content (text JSON string schema used) .error: Logical indicating request failed .error_msg: Error message failed, NA otherwise .batch: Batch number tracking","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","text":"function designed processing large text datasets may fit comfortably memory. divides input chunks, processes chunk concurrent API requests, writes results immediately disk minimise memory usage. function preserves data integrity matching results source texts ids parameter. chunk processed independently results appended output file, allowing resumable processing interrupted. using structured outputs schema, responses validated JSON schema stored raw JSON strings output file. allows flexible post-processing without memory constraints API calls. chunking strategy balances API efficiency memory management. Larger chunk_size values reduce overhead increase memory usage. Adjust based system resources text sizes.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks","text":"","code":"if (FALSE) { # \\dontrun{ # basic usage with automatic file naming:  # large-scale processing with custom output file: #structured extraction with schema:   # post-process structured results: xx <- xx |>   dplyr::filter(!.error) |>   dplyr::mutate(parsed = map(content, ~jsonlite::fromJSON(.x))) |>   unnest_wider(parsed) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","title":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","text":"function takes data frame text inputs processes row OpenAI's Chat Completions API using efficient chunked processing. handles concurrent requests, automatic retries, structured output validation writing results progressively disk.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","text":"","code":"oai_complete_df(   df,   text_var,   id_var,   model = \"gpt-4.1-nano\",   output_file = \"auto\",   system_prompt = NULL,   schema = NULL,   chunk_size = 1000,   concurrent_requests = 1L,   max_retries = 5L,   timeout = 30,   temperature = 0,   max_tokens = 500L,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\" )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","text":"df Data frame containing text process text_var Column name (unquoted) containing text inputs id_var Column name (unquoted) unique row identifiers model OpenAI model use (default: \"gpt-4.1-nano\") output_file Path .CSV file results. \"auto\" generates filename, location persistent across sessions. NULL, generates timestamped filename. system_prompt Optional system prompt applied requests schema Optional JSON schema structured output (json_schema object list) chunk_size Number texts process batch (default: 5000) concurrent_requests Integer; number concurrent requests (default: 5) max_retries Maximum retry attempts per failed request (default: 5) timeout Request timeout seconds (default: 30) temperature Sampling temperature (0-2), lower = deterministic (default: 0) max_tokens Maximum tokens per response (default: 500) key_name Name environment variable containing API key (default: OPENAI_API_KEY) endpoint_url OpenAI API endpoint URL","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","text":"tibble original id column additional columns: content: API response content (text JSON string schema used) .error: Logical indicating request failed .error_msg: Error message failed, NA otherwise .batch: Batch number tracking","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","text":"function provides data frame interface chunked processing capabilities oai_complete_chunks(). extracts specified text column, processes texts configurable chunks concurrent API requests, returns results matched original data id_var parameter. chunking approach enables processing large data frames without memory constraints. Results written progressively CSV file (either specified auto-generated) read back return value. using structured outputs schema, responses validated JSON schema stored JSON strings. Post-processing may needed unnest separate columns. Failed requests marked .error = TRUE include error messages, allowing easy filtering retry logic failures.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df","text":"","code":"if (FALSE) { # \\dontrun{  } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a completion for a single text using OpenAI's Chat Completions API — oai_complete_text","title":"Generate a completion for a single text using OpenAI's Chat Completions API — oai_complete_text","text":"High-level function generate completion single text string. function handles entire process request creation response processing, optional structured output support.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a completion for a single text using OpenAI's Chat Completions API — oai_complete_text","text":"","code":"oai_complete_text(   text,   model = \"gpt-4.1-nano\",   system_prompt = NULL,   schema = NULL,   temperature = 0,   max_tokens = 500L,   key_name = \"OPENAI_API_KEY\",   endpoint_url = \"https://api.openai.com/v1/chat/completions\",   max_retries = 5L,   timeout = 30,   tidy = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a completion for a single text using OpenAI's Chat Completions API — oai_complete_text","text":"text Character string send model model OpenAI model use (default: \"gpt-4.1-nano\") system_prompt Optional system prompt guide model's behaviour schema Optional JSON schema structured output (json_schema object list) temperature Sampling temperature (0-2), lower = deterministic (default: 0) max_tokens Maximum tokens response (default: 500) key_name Environment variable name API key (default: \"OPENAI_API_KEY\") endpoint_url OpenAI API endpoint URL max_retries Maximum retry attempts failed requests (default: 5) timeout Request timeout seconds (default: 30) tidy Whether attempt parse structured output (default: TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a completion for a single text using OpenAI's Chat Completions API — oai_complete_text","text":"Character string containing model's response, parsed JSON schema provided","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_complete_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a completion for a single text using OpenAI's Chat Completions API — oai_complete_text","text":"","code":"if (FALSE) { # \\dontrun{   # Simple completion   response <- oai_complete_text(     text = \"Explain quantum computing in simple terms\",     temperature = 0.7   )    # With system prompt   response <- oai_complete_text(     text = \"What are the main benefits?\",     system_prompt = \"You are an expert in renewable energy\",     max_tokens = 200   )    # Structured output with schema   sentiment_schema <- create_json_schema(     name = \"sentiment_analysis\",     schema = schema_object(       sentiment = schema_string(\"positive, negative, or neutral\"),       confidence = schema_number(\"confidence score between 0 and 1\"),       required = list(\"sentiment\", \"confidence\")     )   )    result <- oai_complete_text(     text = \"I absolutely loved this product!\",     schema = sentiment_schema,     temperature = 0   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","title":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","text":"High-level function generate embeddings multiple text strings using OpenAI's embedding API. function handles batching, concurrent requests, error handling, provides progress reporting large collections texts.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","text":"","code":"oai_embed_batch(   texts,   model = \"text-embedding-3-small\",   dimensions = 1536,   batch_size = 10,   concurrent_requests = 1,   max_retries = 5,   timeout = 20,   endpoint_url = \"https://api.openai.com/v1/embeddings\",   key_name = \"OPENAI_API_KEY\",   include_texts = TRUE,   relocate_col = 2,   verbose = FALSE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","text":"texts Vector list character strings generate embeddings model OpenAI embedding model use (default: \"text-embedding-3-small\") dimensions Number embedding dimensions (default: 1536 text-embedding-3-small) batch_size Number texts process one API request (default: 10) concurrent_requests Number requests send simultaneously (default: 1) max_retries Maximum retry attempts failed requests (default: 5) timeout Request timeout seconds (default: 20) endpoint_url OpenAI API endpoint URL (default: OpenAI's embedding endpoint) key_name Name environment variable containing API key (default: \"OPENAI_API_KEY\") include_texts Whether include original texts result (default: TRUE) relocate_col Column position place error columns (default: 2) verbose Whether enable verbose request logging (default: FALSE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","text":"tibble containing: Embedding vectors columns (V1, V2, ..., Vn) .error: Logical column indicating embedding failed .error_message: Character column error details text: Original texts (include_texts = TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","text":"function efficiently processes multiple texts : Splitting texts batches specified size Creating concurrent requests (configured) faster processing Handling individual batch failures gracefully Pre-allocating memory embeddings improve performance Providing detailed success/failure reporting batch fails, documents specific batch marked failed, documents across batches. Failed embeddings filled NA values marked error information. function returns tibble embedding columns (V1, V2, ..., Vn), error tracking columns (.error, .error_message), optionally original texts.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate embeddings for multiple texts using OpenAI — oai_embed_batch","text":"","code":"if (FALSE) { # \\dontrun{   # Basic batch embedding   texts <- c(\"First text\", \"Second text\", \"Third text\")   embeddings <- oai_embed_batch(texts)    # Large-scale processing with concurrent requests   large_texts <- rep(\"Sample text\", 100)   embeddings <- oai_embed_batch(     texts = large_texts,     batch_size = 20,     concurrent_requests = 5,     dimensions = 512   )    # Custom model and settings   embeddings <- oai_embed_batch(     texts = texts,     model = \"text-embedding-3-large\",     dimensions = 1024,     include_texts = FALSE,     timeout = 30   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","title":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","text":"High-level function generate embeddings texts data frame using OpenAI's embedding API. function handles entire process request creation response processing, options batching & concurrent requests.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","text":"","code":"oai_embed_df(   df,   text_var,   id_var,   model = \"text-embedding-3-small\",   dimensions = 1536,   key_name = \"OPENAI_API_KEY\",   batch_size = 10,   concurrent_requests = 1,   max_retries = 5,   timeout = 20,   endpoint_url = \"https://api.openai.com/v1/embeddings\",   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","text":"df Data frame containing texts embed text_var Column name (unquoted) containing texts embed id_var Column name (unquoted) unique row identifiers model OpenAI embedding model use (default: \"text-embedding-3-small\") dimensions Number embedding dimensions (NULL uses model default) key_name Name environment variable containing API key batch_size Number texts process one batch (default: 10) concurrent_requests Number concurrent requests (default: 1) max_retries Maximum retry attempts per request (default: 5) timeout Request timeout seconds (default: 20) endpoint_url OpenAI API endpoint URL progress Whether display progress bar (default: TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","text":"Original data frame additional columns embeddings (V1, V2, etc.), plus .error .error_message columns indicating failures","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","text":"function extracts texts specified column, generates embeddings using oai_embed_batch(), joins results back original data frame using specified ID column. function preserves original data frame structure adds new columns embedding dimensions (V1, V2, ..., Vn). number rows match processing (due errors), returns results warning. OpenAI's embedding API allows specify number dimensions output embeddings, can useful reducing memory usage, storage cost,s matching specific downstream requirements. default model-specific (1536 text-embedding-3-small). OpenAI Embedding Updates","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate embeddings for texts in a data frame using OpenAI — oai_embed_df","text":"","code":"if (FALSE) { # \\dontrun{   df <- data.frame(     id = 1:3,     text = c(\"First example\", \"Second example\", \"Third example\")   )    # Generate embeddings with default dimensions   embeddings_df <- oai_embed_df(     df = df,     text_var = text,     id_var = id   )    # Generate embeddings with custom dimensions   embeddings_df <- oai_embed_df(     df = df,     text_var = text,     id_var = id,     dimensions = 360,  # smaller embeddings     batch_size = 5   )    # Use with concurrent requests for faster processing   embeddings_df <- oai_embed_df(     df = df,     text_var = text,     id_var = id,     model = \"text-embedding-3-large\",     concurrent_requests = 3   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate embeddings for a single text using OpenAI — oai_embed_text","title":"Generate embeddings for a single text using OpenAI — oai_embed_text","text":"High-level function generate embeddings single text string using OpenAI's embedding API. function handles entire process request creation response processing.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate embeddings for a single text using OpenAI — oai_embed_text","text":"","code":"oai_embed_text(   text,   model = \"text-embedding-3-small\",   dimensions = NULL,   max_retries = 5,   timeout = 20,   endpoint_url = \"https://api.openai.com/v1/embeddings\",   key_name = \"OPENAI_API_KEY\",   tidy = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate embeddings for a single text using OpenAI — oai_embed_text","text":"text Character string generate embeddings (must non-empty) model OpenAI embedding model use (default: \"text-embedding-3-small\") dimensions Number embedding dimensions (NULL uses model default) max_retries Maximum retry attempts failed requests (default: 5) timeout Request timeout seconds (default: 20) endpoint_url OpenAI API endpoint URL (default: OpenAI's embedding endpoint) key_name Name environment variable containing API key (default: \"OPENAI_API_KEY\") tidy Whether return tidy tibble format (default: TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate embeddings for a single text using OpenAI — oai_embed_text","text":"tidy = TRUE, returns tibble embedding vectors columns (V1, V2, etc.). tidy = FALSE, returns raw httr2 response object.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate embeddings for a single text using OpenAI — oai_embed_text","text":"function designed single text inputs. processing multiple texts, use oai_embed_batch() efficient batch operations. function automatically handles API authentication, request retries, error handling. default, returns tidy tibble embedding vectors columns, can get raw response setting tidy = FALSE.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate embeddings for a single text using OpenAI — oai_embed_text","text":"","code":"if (FALSE) { # \\dontrun{   # Generate embeddings for a single text   embeddings <- oai_embed_text(\"Hello world\")    # Use a different model with custom dimensions   embeddings <- oai_embed_text(     text = \"Hello world\",     model = \"text-embedding-3-large\",     dimensions = 1024   )    # Get raw response instead of tidy format   raw_response <- oai_embed_text(     text = \"Hello world\",     tidy = FALSE   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","title":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","text":"Executes list HTTP requests either sequentially parallel. Automatically chooses sequential processing concurrent_requests = 1 one request.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","text":"","code":"perform_requests_with_strategy(   requests,   concurrent_requests = 1,   progress = TRUE )"},{"path":"https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","text":"requests List httr2_request objects perform concurrent_requests Integer specifying maximum number simultaneous requests (default: 1) progress Logical indicating whether show progress bar (default: TRUE)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","text":"List httr2_response objects error objects failed requests","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","text":"returns responses order requests sent, returns errors predictable format.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform multiple requests with configurable concurrency strategy — perform_requests_with_strategy","text":"","code":"if (FALSE) { # \\dontrun{   # Sequential processing   responses <- perform_requests_with_strategy(     requests = my_requests,     concurrent_requests = 1   )    # Parallel processing with 5 concurrent requests   responses <- perform_requests_with_strategy(     requests = my_requests,     concurrent_requests = 5,     progress = TRUE   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/process_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Process API response with error handling — process_response","title":"Process API response with error handling — process_response","text":"Higher-order function applies tidying function API response. Handles successful responses errors, returning consistent tibble structure. tidy_func parameter allows provide necessary function particular workflow.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/process_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process API response with error handling — process_response","text":"","code":"process_response(resp, indices, tidy_func)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/process_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process API response with error handling — process_response","text":"resp httr2_response object error object failed request indices Vector indices track original position requests tidy_func Function process/tidy successful API responses","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/process_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process API response with error handling — process_response","text":"tibble processed results error information, including: original_index: Position original request batch .error: Logical indicating error occurred .error_message: Character description error Additional columns tidy_func output","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/process_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process API response with error handling — process_response","text":"","code":"if (FALSE) { # \\dontrun{   # Process a response with custom tidying function   result <- process_response(     resp = api_response,     indices = c(1, 2, 3),     tidy_func = function(r) { tibble::tibble(data = httr2::resp_body_json(r)) }   ) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Safely perform an embedding request with error handling — safely_perform_request","title":"Safely perform an embedding request with error handling — safely_perform_request","text":"Wrapper around httr2::req_perform handles errors gracefully.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Safely perform an embedding request with error handling — safely_perform_request","text":"","code":"safely_perform_request(request)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Safely perform an embedding request with error handling — safely_perform_request","text":"request httr2 request object","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Safely perform an embedding request with error handling — safely_perform_request","text":"list components $result $error","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Create array property schema — schema_array","title":"Create array property schema — schema_array","text":"Defines list/array fields containing multiple items type. Use tags, categories, collection data. Can constrain array length.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create array property schema — schema_array","text":"","code":"schema_array(items, description = NULL, min_items = NULL, max_items = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create array property schema — schema_array","text":"items Schema definition array elements (use schema_* helpers) description Human-readable field description min_items Minimum number array elements max_items Maximum number array elements","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_array.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create array property schema — schema_array","text":"","code":"# List of tags schema_array(schema_string(), \"Product tags\", max_items = 10) #> $type #> [1] \"array\" #>  #> $items #> $items$type #> [1] \"string\" #>  #>  #> $description #> [1] \"Product tags\" #>  #> $maxItems #> [1] 10 #>   # Array of objects schema_array(   schema_object(     name = schema_string(\"Ingredient name\"),     amount = schema_number(\"Amount needed\")   ),   \"Recipe ingredients\" ) #> $type #> [1] \"array\" #>  #> $items #> $items$type #> [1] \"object\" #>  #> $items$properties #> $items$properties$name #> $items$properties$name$type #> [1] \"string\" #>  #> $items$properties$name$description #> [1] \"Ingredient name\" #>  #>  #> $items$properties$amount #> $items$properties$amount$type #> [1] \"number\" #>  #> $items$properties$amount$description #> [1] \"Amount needed\" #>  #>  #>  #> $items$additionalProperties #> [1] FALSE #>  #>  #> $description #> [1] \"Recipe ingredients\" #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_boolean.html","id":null,"dir":"Reference","previous_headings":"","what":"Create boolean property schema — schema_boolean","title":"Create boolean property schema — schema_boolean","text":"Defines true/false fields. Use flags, yes/questions, binary states. Ensures LLM returns exactly true false, \"yes\"/\"\" strings.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_boolean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create boolean property schema — schema_boolean","text":"","code":"schema_boolean(description = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_boolean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create boolean property schema — schema_boolean","text":"description Human-readable field description","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_boolean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create boolean property schema — schema_boolean","text":"","code":"# Feature flag schema_boolean(\"Has premium subscription\") #> $type #> [1] \"boolean\" #>  #> $description #> [1] \"Has premium subscription\" #>   # Classification result schema_boolean(\"Contains sensitive information\") #> $type #> [1] \"boolean\" #>  #> $description #> [1] \"Contains sensitive information\" #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_enum.html","id":null,"dir":"Reference","previous_headings":"","what":"Create enumerated property schema — schema_enum","title":"Create enumerated property schema — schema_enum","text":"Defines fields constrained specific allowed values. flexible schema_string enum parameter - supports numeric enums mixed types. Use categories, status codes, multi-choice field.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_enum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create enumerated property schema — schema_enum","text":"","code":"schema_enum(values, description = NULL, type = \"string\")"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_enum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create enumerated property schema — schema_enum","text":"values Vector allowed values description Human-readable field description type Data type enum values (\"string\", \"integer\", \"number\")","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_enum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create enumerated property schema — schema_enum","text":"","code":"# Status categories schema_enum(c(\"draft\", \"published\", \"archived\"), \"Document status\") #> $type #> [1] \"string\" #>  #> $enum #> [1] \"draft\"     \"published\" \"archived\"  #>  #> $description #> [1] \"Document status\" #>   # Priority levels as numbers schema_enum(c(1, 2, 3, 4, 5), \"Priority level\", type = \"integer\") #> $type #> [1] \"integer\" #>  #> $enum #> [1] 1 2 3 4 5 #>  #> $description #> [1] \"Priority level\" #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_integer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create integer property schema — schema_integer","title":"Create integer property schema — schema_integer","text":"Defines whole number fields optional range constraints. Use counts, IDs, quantities, discrete numeric values. restrictive schema_number.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_integer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create integer property schema — schema_integer","text":"","code":"schema_integer(description = NULL, minimum = NULL, maximum = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_integer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create integer property schema — schema_integer","text":"description Human-readable field description minimum Minimum allowed value (inclusive) maximum Maximum allowed value (inclusive)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_integer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create integer property schema — schema_integer","text":"","code":"# Item quantity schema_integer(\"Quantity ordered\", minimum = 1) #> $type #> [1] \"integer\" #>  #> $description #> [1] \"Quantity ordered\" #>  #> $minimum #> [1] 1 #>   # Rating scale schema_integer(\"Star rating\", minimum = 1, maximum = 5) #> $type #> [1] \"integer\" #>  #> $description #> [1] \"Star rating\" #>  #> $minimum #> [1] 1 #>  #> $maximum #> [1] 5 #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_number.html","id":null,"dir":"Reference","previous_headings":"","what":"Create numeric property schema — schema_number","title":"Create numeric property schema — schema_number","text":"Defines decimal number fields optional range constraints. Use prices, percentages, ratings, continuous numeric data.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_number.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create numeric property schema — schema_number","text":"","code":"schema_number(description = NULL, minimum = NULL, maximum = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_number.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create numeric property schema — schema_number","text":"description Human-readable field description minimum Minimum allowed value (inclusive) maximum Maximum allowed value (inclusive)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_number.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create numeric property schema — schema_number","text":"","code":"# Product price schema_number(\"Price in USD\", minimum = 0) #> $type #> [1] \"number\" #>  #> $description #> [1] \"Price in USD\" #>  #> $minimum #> [1] 0 #>   # Percentage score schema_number(\"Confidence score\", minimum = 0, maximum = 100) #> $type #> [1] \"number\" #>  #> $description #> [1] \"Confidence score\" #>  #> $minimum #> [1] 0 #>  #> $maximum #> [1] 100 #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_object.html","id":null,"dir":"Reference","previous_headings":"","what":"Create JSON Schema object definitions — schema_object","title":"Create JSON Schema object definitions — schema_object","text":"helper functions simplify creating JSON Schema definitions structured outputs (e.g. OpenAI). provide type-safe, validated schemas ensure consistent LLM responses matching expected data structure. JSON Schema constrains LLM outputs specific formats, preventing parsing errors ensuring reliable data extraction unstructured text. Create object schema nested properties Defines JSON object typed properties. Use structured data like user profiles, API responses, nested data structure. LLM return JSON matching exactly schema.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_object.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create JSON Schema object definitions — schema_object","text":"","code":"schema_object(..., required = NULL, additional_properties = FALSE)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_object.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create JSON Schema object definitions — schema_object","text":"... Named arguments defining object properties (use schema_* helpers) required Character vector required property names additional_properties Whether allow extra properties beyond defined","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_object.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create JSON Schema object definitions — schema_object","text":"","code":"# User profile with required fields schema_object(   name = schema_string(\"Full name\"),   age = schema_integer(\"Age in years\"),   required = c(\"name\", \"age\") ) #> $type #> [1] \"object\" #>  #> $properties #> $properties$name #> $properties$name$type #> [1] \"string\" #>  #> $properties$name$description #> [1] \"Full name\" #>  #>  #> $properties$age #> $properties$age$type #> [1] \"integer\" #>  #> $properties$age$description #> [1] \"Age in years\" #>  #>  #>  #> $additionalProperties #> [1] FALSE #>  #> $required #> [1] \"name\" \"age\"  #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_string.html","id":null,"dir":"Reference","previous_headings":"","what":"Create string property schema — schema_string","title":"Create string property schema — schema_string","text":"Defines text fields optional constraints. Use names, descriptions, textual data. Can restrict specific values via enum parameter.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_string.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create string property schema — schema_string","text":"","code":"schema_string(description = NULL, enum = NULL)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_string.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create string property schema — schema_string","text":"description Human-readable field description (helps LLM understand context) enum Character vector allowed values (creates dropdown-like constraint)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/schema_string.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create string property schema — schema_string","text":"","code":"# Simple text field schema_string(\"User's email address\") #> $type #> [1] \"string\" #>  #> $description #> [1] \"User's email address\" #>   # Constrained to specific values schema_string(\"Sentiment\", enum = c(\"positive\", \"negative\", \"neutral\")) #> $type #> [1] \"string\" #>  #> $description #> [1] \"Sentiment\" #>  #> $enum #> [1] \"positive\" \"negative\" \"neutral\"  #>"},{"path":"https://jpcompartir.github.io/EndpointR/reference/sentiment_classification_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Single sentiment classification result example — sentiment_classification_example","title":"Single sentiment classification result example — sentiment_classification_example","text":"sample result sentiment classification single text using Hugging Face's classification API. demonstrates structure returned hf_classify_text() function.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/sentiment_classification_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Single sentiment classification result example — sentiment_classification_example","text":"","code":"sentiment_classification_example"},{"path":"https://jpcompartir.github.io/EndpointR/reference/sentiment_classification_example.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Single sentiment classification result example — sentiment_classification_example","text":"data frame 1 row 2 variables: NEGATIVE Numeric; probability score negative sentiment (0-1) POSITIVE Numeric; probability score positive sentiment (0-1)","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/sentiment_classification_example.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Single sentiment classification result example — sentiment_classification_example","text":"Generated using Hugging Face sentiment classification model via EndpointR functions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Set your API keys so they can be accessed by EndpointR — set_api_key","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"Set API key endpoint - endpoint Anthropic, OpenAI, specific Hugging Face Inference endpoint, another supported provider. Add overwrite=TRUE need update existing key. able retrieve key get_api_key() function.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"","code":"set_api_key(key_name, overwrite = FALSE)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"key_name name API format \"ENDPOINT_API_KEY\" -> \"ANTHROPIC_API_KEY\" overwrite Whether overwrite existing value API key.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"Nothing","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/set_api_key.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set your API keys so they can be accessed by EndpointR — set_api_key","text":"","code":"if (FALSE) { # \\dontrun{   # set an Anthropic API key   set_api_key(\"ANTHROPIC_API_KEY\")    # update an existing OpenAI key   set_api_key(\"OPENAI_API_KEY\", overwrite = TRUE) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/single_embedding_hf.html","id":null,"dir":"Reference","previous_headings":"","what":"Single embedding result example from Hugging Face API — single_embedding_hf","title":"Single embedding result example from Hugging Face API — single_embedding_hf","text":"sample embedding vector result processing single text using Hugging Face's embedding API. demonstrates structure returned hf_embed_text() function.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/single_embedding_hf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Single embedding result example from Hugging Face API — single_embedding_hf","text":"","code":"single_embedding_hf"},{"path":"https://jpcompartir.github.io/EndpointR/reference/single_embedding_hf.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Single embedding result example from Hugging Face API — single_embedding_hf","text":"data frame 1 row 768 variables: V1, V2, ..., V768 Numeric; embedding vector dimensions representing semantic encoding input text","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/single_embedding_hf.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Single embedding result example from Hugging Face API — single_embedding_hf","text":"Generated using Hugging Face embedding model via EndpointR functions","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Hugging Face classification response to tidy format — tidy_classification_response","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"Transforms nested JSON response Hugging Face classification endpoint tidy data frame one row columns classification label.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"","code":"tidy_classification_response(response)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"response Either httr2_response object Hugging Face API request parsed JSON object containing classification results","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"data frame one row columns classification label","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"function expects specific structure response, classification result containing 'label' 'score' field. flattens nested structure pivots data create wide-format data frame. function accepts either raw httr2_response object parsed JSON structure, making flexible different workflow patterns.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert Hugging Face classification response to tidy format — tidy_classification_response","text":"","code":"if (FALSE) { # \\dontrun{   # Process response directly from API call   response <- hf_perform_request(req)   tidy_results <- tidy_classification_response(response)    # Or with an already-parsed JSON object   json_data <- httr2::resp_body_json(response)   tidy_results <- tidy_classification_response(json_data)    # Example of expected output structure   # A tibble: 1 × 2   #   positive negative   #      <dbl>    <dbl>   # 1    0.982    0.018 } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Process embedding API response into a tidy format — tidy_embedding_response","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"Converts nested list response Hugging Face Inference API embedding request tidy tibble.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"","code":"tidy_embedding_response(response)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"response httr2 response object parsed JSON response","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"tibble containing embedding vectors","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process embedding API response into a tidy format — tidy_embedding_response","text":"","code":"if (FALSE) { # \\dontrun{   # Process response from httr2 request   req <- hf_build_request(text, endpoint_url, api_key)   resp <- httr2::req_perform(req)   embeddings <- tidy_embedding_response(resp)    # Process already parsed JSON   resp_json <- httr2::resp_body_json(resp)   embeddings <- tidy_embedding_response(resp_json) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","title":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","text":"Converts nested list response OpenAI embedding API request tidy tibble embedding vectors columns.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","text":"","code":"tidy_oai_embedding(response)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","text":"response httr2 response object parsed JSON response OpenAI's embedding API","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","text":"tibble containing embedding vectors columns (V1, V2, etc.) optionally oai_index column present response","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","text":"function handles single document batch embedding responses. extracts embedding vectors converts wide format tibble column (V1, V2, ..., Vn) represents one dimension embedding vector. response includes index information, adds oai_index column preserve ordering.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process OpenAI embedding API response into a tidy format — tidy_oai_embedding","text":"","code":"if (FALSE) { # \\dontrun{   # Process response from httr2 request   req <- oai_build_embedding_request(\"Hello world\")   resp <- httr2::req_perform(req)   embeddings <- tidy_oai_embedding(resp)    # Process already parsed JSON   resp_json <- httr2::resp_body_json(resp)   embeddings <- tidy_oai_embedding(resp_json) } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"Checks endpoint URL valid accessible provided API key. function sends small test request verify endpoint works.","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"","code":"validate_hf_endpoint(endpoint_url, key_name)"},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"endpoint_url URL Hugging Face Inference API endpoint key_name Name environment variable containing API key","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"logical TRUE endpoint valid, otherwise stops error","code":""},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate that a Hugging Face Inference Endpoint is available — validate_hf_endpoint","text":"","code":"if (FALSE) { # \\dontrun{   # Validate endpoint retrieving API key from environment   validate_hf_endpoint(     endpoint_url = \"https://my-endpoint.huggingface.cloud\",     key_name = \"HF_API_KEY\"   )    # Using default key name   validate_hf_endpoint(\"https://my-endpoint.huggingface.cloud\") } # }"},{"path":"https://jpcompartir.github.io/EndpointR/reference/validate_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate response data against schema — validate_response","title":"Validate response data against schema — validate_response","text":"Validate response data schema","code":""},{"path":"https://jpcompartir.github.io/EndpointR/news/index.html","id":"endpointr-011","dir":"Changelog","previous_headings":"","what":"EndpointR 0.1.1","title":"EndpointR 0.1.1","text":"oai_complete_chunks() function better support chunking/batching oai_complete_df() oai_complete_df() now writes file mitigate chance completely lost data","code":""},{"path":"https://jpcompartir.github.io/EndpointR/news/index.html","id":"endpointr-010","dir":"Changelog","previous_headings":"","what":"EndpointR 0.1.0","title":"EndpointR 0.1.0","text":"Initial BETA release, ships : Support embeddings classification Hugging Face Inference API & Dedicated Inference Endpoints Support text completion using OpenAI models via Chat Completions API Support embeddings OpenAI Embeddings API Structured outputs via JSON schemas validators","code":""}]
