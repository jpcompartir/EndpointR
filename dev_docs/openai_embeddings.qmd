---
title: "openai_embeddings"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: flatly
    number_sections: true
    fig_caption: true
    df_print: paged
    highlight: tango
    code_folding: show
    anchor_sections: true
---

```{r, setup}
library(httr2)
library(dplyr)
library(tidyr)
library(purrr)
library(tibble)
```

# Embed Text

## Single Req

Quite similar to hugging face embeddings, except the part of the response we are interested in is an object with elements object, index, embedding. Whether batch or single embeddings we get this object - for single embeddings the index is 0

```{r, build_single_req}
x <- oai_build_embedding_request("hello")

res <- httr2::req_perform(x)
```

```{r, single_text_hardcode}
res_json <- res |> 
  resp_body_json()

col_names <- paste("V", seq_along(res_json$data[[1]][["embedding"]] ), sep = "")

res_json$data[[1]][["index"]]

res_json$data[[1]][["embedding"]] |> 
  as.data.frame(col.names = col_names) |> 
  as_tibble()

# or in tidyv
res_json |> 
  pluck('data', 1, "embedding") |>
  as.data.frame(col.names = col_names) |> 
  as_tibble()
```

Test our function:

```{r, test_embed_text_function}
x <- oai_embed_text("hello")

low_dim_x <- oai_embed_text("Embed me", dimensions = 300)

```

## Batch Texts

```{r, batch_hardcode}
batch_req <- oai_build_embedding_request(list("hello", "goodbye"))

batch_res <- batch_req$request |>  httr2::req_perform()

batch_res_json <- batch_res |>resp_body_json()

batch_res_json |>
  pluck('data') |>
  map(~{
    x <- .x # for the environment stuff inside map/anonymous func
    
    browser()
    
    index <- x$index
    embeddings <- x$embedding
    
    col_names <- paste("V", seq_along(embeddings), sep = "")
    
    setNames(embeddings, col_names)
    
    id_df <- data.frame(batch_id = index)
    
    embedding_df <- as.data.frame(embeddings, col.names = col_names)
    
    bind_cols(id_df, embedding_df) |> tibble()
   }) 

```

# Batch Function

## Generate Error Length

```{r, error_length}
long_text <- paste(rep("hello", 5000), collapse = " ")
long_batch <- list(
  long_text,
  long_text
)

ex_long_batch<- oai_build_embedding_request_batch(
  long_batch
)

attributes(ex_long_batch)
```

```{r, check_attributes}
smol_batch <- paste(rep("hello", 20), collapse = " ")
smol_batch <- rep(smol_batch, 10)

ex_smol_batch <- oai_build_embedding_request_batch(
  smol_batch
)

attributes(ex_smol_batch)
```

```{r, test_batch_embed}
batch_resps <- oai_embed_batch(
  smol_batch,
  batch_size = 10,
  concurrent_requests = 1
)

single_batch_embeddings <- oai_embed_batch(smol_batch, batch_size = 10)
```

```{r, larger_batch_test}
bigger_batch <- rep(smol_batch, 10)

bigger_batch_embeddings <- oai_embed_batch(bigger_batch, dimensions = 512, concurrent_requests = 10)
```

## Generate Error Empty String

```{r, empty_errror}
texts_with_empty <- c("hello", "goodbye", "nice", "")
oai_embed_batch(texts_with_empty, batch_size = 2) # Error
```

## DF Trust Test Batch - Informal tests

```{r, get_real_data}
trust <- readr::read_csv("~/data/trust/trust_slice_spam_classification.csv")

trust <- trust |> filter(text != "")
trust_100 <- trust |>  slice(1:100)
trust_100_2 <- trust |>  slice(20001:20100)
trust_1k <- trust |>  slice(1:1000)
trust_10k <- trust |>  slice(1:10000)

```

```{r, test_with_concurrency}
trust_100_embeddings <- oai_embed_batch(trust_100$text, batch_size = 100, concurrent_requests = 2)
```

```{r, test_with_dimensions}
trust_100_2_embeddings <- oai_embed_batch(trust_100_2$text, batch_size = 20, concurrent_requests = 2, dimensions = 360)
trust_100_2_embeddings |>  count(.error)
```

```{r, test_with_verbose}
oai_embed_batch(trust_100$text, batch_size = 10, concurrent_requests = 5, verbose = TRUE)
```

```{r, test_1k}
trust_1k_embeddings <- oai_embed_batch(trust_1k$text, batch_size =20, concurrent_requests = 50)
```

```{r, memory_profiling}
library(profvis)
mem_profile <- profvis({
  trust_10k_embeddings <-  oai_embed_batch(trust_10k$text, batch_size = 10, concurrent_requests = 20)
})
```

```{r, test_10k}
trust_10k_embeddings <-  oai_embed_batch(trust_10k$text, batch_size = 10, concurrent_requests = 50, verbose = FALSE)
trust_10k_embeddings |>  count(.error_message)
```

```{r, test_high_concurrency}
trust_10k_embeddings_1_batch <-  oai_embed_batch(trust_10k$text, batch_size = 10, concurrent_requests = 100)

trust_10k_embeddings_1_batch |>  count(.error)
```

```{r, inspect_10k_embeddings}
trust_10k_embeddings
```

```{r, trust_10k_2_time}
trust_10k_2 <- trust |>
  filter(text != "", !is.null(text))|>  slice(10001:20000)

start <- Sys.time()
trust_10k_2_embeddings <-
  oai_embed_batch(trust_10k_2$text,
                  batch_size = 5, concurrent_requests = 80)
end <- Sys.time()

start - end
```

# Efficiency of batching

do.call vs pre-filling the matrix and iterating

```{r}
trust_2k_do_call <- trust |>  slice_sample(n = 20000)
trust_2k_pre_fill <- trust |>  slice_sample(n = 20000)

trust_2k_do_call |>  reframe(sl = mean(stringr::str_length(text)),
                             n = n())
trust_2k_pre_fill |>  reframe(sl = mean(stringr::str_length(text)),
                              n = n())
```

When profiling it's *quite clear* that the choice of prefilling vs do.call isn't particularly relevant. The memory usage is almost exclusively in creating the reqeusts, performing them, and getting the embeddings out of the response's JSON. So we'll go with the do.call approach as it's simpler to understand and debug (IMO)

```{r, do_call_profile}
library(profvis)

do_call_profile <- profvis({
  do_call_embeddings <-oai_embed_batch_do_call(
    trust_2k_do_call$text, 
    batch_size = 10,
    concurrent_requests = 50
  )
})


```

```{r, pre_fill_profile}
pre_fill_profile <- profvis({
  pre_filll_embeddings <-oai_embed_batch_prefill(
    trust_2k_pre_fill$text, 
    batch_size = 10,
    concurrent_requests = 50
  )
})
```
