---
title: "initial_release"
format: 
  html:
    embed-resources: true
    code-tools: true
    code-block-bg: true
    code-block-border-left: true
    toc: true
---

```{r}
library(httr2)
library(webfakes)
library(tibble)
library(dplyr)
library(tibble)
library(readr)
```

```{r}
trust <- read_csv("~/data/trust/trust_slice.csv") #|> 
  # slice(1:1000)
```

# Use Cases

Primary use cases are:

-   embedding a dataset - via HF Inference Endpoint, or OpenAI
-   classifying over a dataset with a pre-defined task
-   classifying over a dataset on a task which is not pre-defined, using structured outputs

For API consistency, the user should be able to set max_retries, max_requests

# Todos

-   \[ \]

# OpenAI

# httr2 notes

req\_\* functions, like `req_perform_parallel`, `req_body_json`, `req_retry`, `req_throttle` to take care of some key processes for efficiently hitting APIs and handling requests.

Experimental req_perform_promise()

resp\_\* functions, like `resp_`

## Parallel Requests

With example_url() we can hit a web-api to test throttling, parallel requests etc. pretty noicely

Copy the URL http://127.0.0.1:58524/#tag/HTTP-methods/paths/\~1put/put and you can explore the web app (local host) it creates

```{r}
# req_dry_run(req)
req <- 
  request(example_url()) |> 
  req_throttle(capacity = 5)
  
reqs <- rep(
  list(
    request(example_url()) |> 
      req_throttle(capacity = 5, fill_time_s = 3)
    ), 50)

req_perform_parallel(reqs, 
                     progress = TRUE, 
                     max_active = 10)
```

## Batching

We need to figure out how is best to do the batching of requests - how we make sure all of the results are returned in order. How we track the progress of the Inference Endpoint, to understand whether it can handle the load or not.

Whether we can do this nicely without locking the R session, or if that adds too much complexity... could see that being a big issue in CI/CD

```{r}
library(webfakes)
app <- new_app()
app$get("/test", function(req, res) {
  Sys.sleep(0.1)
  res$send("OK")
})
server <- new_app_process(app)

reqs <- rep(
  list(
    request(server$url("/test")) |> 
      req_throttle(capacity = 5, fill_time_s = 10)
    ), 50)

req_perform_parallel(reqs, 
                     progress = TRUE, 
                     max_active = 5)
```

# HF inference endpoint

-   We can't easily optimise the memory usage within a server - or at least not a CPU-based server. Though it did seem to scale up when we use req_perform_parallel.

```{r, endpoint_data}
api_key <- Sys.getenv("HF_TEST_API_KEY")
hf_test_api_url <- "https://o1stb590fw4ortu4.us-east-1.aws.endpoints.huggingface.cloud"
sentences <- sentences # data for some basic checking
```

## Building a request

```{r}
req <- request(hf_test_api_url)
req <- req |>
  req_user_agent(string = "EndpointR") |>
  req_method("POST") |>
  req_headers("Content-Type" = "application/json") |>
  req_auth_bearer_token(token = api_key) |>
  req_progress() |>
  req_body_json(list(inputs = sentences[31:60])) |>
  req_progress() |> 
  req_retry(
    max_tries = 10,
    retry_on_failure = TRUE)

# perform req -> tidy response
resp <- req |>
  httr2::req_perform(verbosity = 1)

resp_json <- resp |>
  resp_body_json()

tidy_nested_embedding_list(resp_json)
```

## Converting to a function

User needs to: 1. select the text column 2. input the endpoint url 3. input the API key

```{r, functions}
build_hf_embed_request <- function(text, endpoint_url, api_key) {
  req <- httr2::request(base_url = hf_test_api_url)
  
  req <- req |> 
    req_user_agent(string = "EndpointR") |>
    req_method("POST") |>
    req_headers("Content-Type" = "application/json") |>
    req_auth_bearer_token(token = api_key) |> 
    req_body_json( list(inputs = {{text}})) |>
    # req_progress() |> 
    req_retry(max_tries = 10,
              retry_on_failure = TRUE)
  
  return(req)
}

# basic func for getting the list tidied.
tidy_nested_embedding_list <- function(nested_list) {
  tib <- sapply(nested_list, unlist) |>  # first conv to matrix, but this is long format
    t() |> # transpose to wide form
    as.data.frame.matrix() |>  # conv to df, has grim column names
    tibble::as_tibble()

  return(tib)
}
```

## Sentences Test

```{r}
id <-paste0("uid_", 1:length(sentences))
request_df <- tibble(
  id = id,
  text = sentences) |>
  mutate(request =
           map(text, 
               ~ build_hf_embed_request(
                 .x,
                 hf_test_api_url,
                 api_key = api_key)
           )
  )


request_df |>  
  pluck('request', 1) |>  
  req_dry_run() # check everything looks fine before sending off all requests.

response_df <- request_df |> 
  mutate(response = map(request, ~ req_perform(.x)))

response_df |>  
  pluck('response', 1) |>  
  resp_body_json() |>  
  tidy_nested_embedding_list()
```

## Testing with progress in sequential requests

predictably, doesn't work as each request is too fast - latency tends to be \< 0.1s. Also, with a larger DF there is quite a lot of start up time to create each request

```{r}
safe_perform <- safely(req_perform)
start <- Sys.time()
trust_id <- paste0("uid_", 1:nrow(trust))

trust_request_df <- tibble(
  id = trust_id,
  text = trust$text
) |>
  filter(text != "", !is.na(text)) |> 
  slice(1:5000)

map_start <- Sys.time()
trust_request_df <- trust_request_df |>  
  # slice(1:100) |> 
  mutate(request = map(text, ~build_hf_embed_request(.x, hf_test_api_url, api_key = api_key)))
map_end <- Sys.time() - map_start
map_end


resp_progress  <- trust_request_df |> 
  # slice(1:100) |> 
  mutate(response = map(request, 
                        ~ safe_perform(.x, verbosity = 0)),
         .progress = TRUE)

end <- Sys.time() - start
print(end)
```

Building the requests like this takes \~30 seconds via map, and \~29 seconds via adding a `Vectorize()` to build_hf_embed_request, so there's no real need to vectorise independently (mutate is vectorised).

```{r}
vec_build_req <- Vectorize(build_hf_embed_request, SIMPLIFY = FALSE)
vec_start <- Sys.time()
batch_requests <- vec_build_req(trust_docs, hf_test_api_url, api_key = api_key)
vec_end <- Sys.time() - vec_start
vec_end
```

```{r}
jsonlite::toJSON
```

# Developing the batch workflow

Whether we eventually hand over a list of inputs, or a chunked data frame of inputs,we want the requests to be packaged together, using something like `req_body_json( list(inputs = {{text}})`. This should be able to build on hf_build_request, but have some additional checks for the input being a list.

If we're trying to batch inference a list of chunked data frames, we need to get ids and texts, and then re-package them up from the response. httr2 guarantees we get our results back sorted in the order we requested them, within batches and across batches - we just need to take care of getting the list of inputs together.

```{r}
hf_build_request(input = embedding_sentences, endpoint_url = endpoint_url)
```

```{r}
embedding_sentences <- c(
  "Text embedding models compress your rambling prose into compact vectors, much like how British commuters squeeze themselves into tube carriages during rush hour.",
  "Setting up an inference endpoint without proper documentation is akin to attempting to navigate London using a map of Birmingham.",
  "When your embedding model starts hallucinating, it's rather like watching Parliament during Question Time—entertaining but utterly unpredictable.",
  "Optimising your inference endpoint is essential unless you fancy your users growing old whilst waiting for a response, perhaps even ageing enough to collect their pension.",
  "The distance between word embeddings reveals semantic relationships, though sadly not the distance between what your client requested and what they actually wanted.",
  "Creating multilingual embeddings is a bit like attempting to order tea properly across Europe—technically possible but fraught with cultural misunderstandings.",
  "Batch processing through inference endpoints saves computing resources, much like how the British save conversation topics by discussing the weather exclusively.",
  "Token limits on embedding APIs are the digital equivalent of a queue at a British post office—inevitably, you'll reach the front just as they close for lunch.",
  "Fine-tuning embedding models on domain-specific corpora is rather like training a British child to apologise—it requires repetition, patience, and considerable examples.",
  "When your inference endpoint crashes under load, it maintains that quintessentially British trait of breaking down precisely when most inconvenient."
)
```

```{r}
rep_embedding_sentences <- rep(embedding_sentences, 10)
rep_ids <- 1: length(rep_embedding_sentences)

rep_embed_df <- tibble(
  id = rep_ids,
  sentence = rep_embedding_sentences
)
rep_chunks <- chunk_dataframe(rep_embed_df, 8)

batch_rep_reqs <- map(rep_chunks, ~ hf_build_request_batch(inputs =.x$sentence,endpoint_url = endpoint_url, key_name = "HF_TEST_API_KEY"))

map(batch_rep_reqs, hf_perform_request, path = "test.jsonl", .progress = TRUE) # doesn't quite work out of the box...

tmp_test_df <- jsonlite::stream_in(file("test.jsonl"))  #readLines(con, n = pagesize, encoding = "UTF-8") : incomplete final line found on 'test.jsonl'
lines <- readLines(con = file("test.jsonl"))


map(batch_rep_reqs, hf_perform_request, path = "test.txt", .progress = TRUE) # doesn't quite work out of the box...
lines <- readLines(con = file("test.txt")) # same issue

```

## Refactoring hf_embed_df

```{r}
embedding_sentences <- c(
  "Text embedding models compress your rambling prose into compact vectors, much like how British commuters squeeze themselves into tube carriages during rush hour.",
  "Setting up an inference endpoint without proper documentation is akin to attempting to navigate London using a map of Birmingham.",
  "When your embedding model starts hallucinating, it's rather like watching Parliament during Question Time—entertaining but utterly unpredictable.",
  "Optimising your inference endpoint is essential unless you fancy your users growing old whilst waiting for a response, perhaps even ageing enough to collect their pension.",
  "The distance between word embeddings reveals semantic relationships, though sadly not the distance between what your client requested and what they actually wanted.",
  "Creating multilingual embeddings is a bit like attempting to order tea properly across Europe—technically possible but fraught with cultural misunderstandings.",
  "Batch processing through inference endpoints saves computing resources, much like how the British save conversation topics by discussing the weather exclusively.",
  "Token limits on embedding APIs are the digital equivalent of a queue at a British post office—inevitably, you'll reach the front just as they close for lunch.",
  "Fine-tuning embedding models on domain-specific corpora is rather like training a British child to apologise—it requires repetition, patience, and considerable examples.",
  "When your inference endpoint crashes under load, it maintains that quintessentially British trait of breaking down precisely when most inconvenient."
)
```

```{r}
id_1000 <- 1:1000
sentences_1000 <- rep(embedding_df$sentence, 100)
embedding_df_1000 <- tibble(id = id_1000, sentence = sentences_1000)
```

On batching new is much better!

```{r}
new_start <- Sys.time()

 new_result <- hf_embed_df_new(
  df = embedding_df_1000,
  text_var = sentence,
  id_var = id,
  key_name = "HF_TEST_API_KEY",
  endpoint_url = endpoint_url,
  batch_size = 20, 
  concurrent_requests = 5,
  progress = TRUE,
  include_errors = TRUE,
  max_retries = 5)
 
new_end <- Sys.time() - new_start
glue::glue("Processing time for new func version: {new_end}") 
```

```{r}
new_start <- Sys.time()

 new_result <- hf_embed_df_new(
  df = embedding_df_1000 |>  slice(1:100),
  text_var = sentence,
  id_var = id,
  key_name = "HF_TEST_API_KEY",
  endpoint_url = endpoint_url,
  batch_size = 1, 
  concurrent_requests = 10,
  progress = TRUE,
  include_errors = TRUE,
  max_retries = 5)
 
new_end <- Sys.time() - new_start
glue::glue("Processing time for new func version unbatched: {new_end}") 
```

Unbatched

```{r}
original_start <- Sys.time()

 original_result <- hf_embed_df(
  df = embedding_df_1000 |>  slice(1:100),
  text_var = sentence,
  id_var = id,
  key_name = "HF_TEST_API_KEY",
  endpoint_url = endpoint_url,
  batch_size = 1, concurrent_requests = 10,
  progress = TRUE,
  include_errors = TRUE,
  max_retries = 5)
 
original_end <- Sys.time() - original_start
glue::glue("Processing time for original version Unbatched: {original_end}") 
```

# Hf Inference Vignette Drafts

## Request Preparation

TODO: fix following all the re-factoring Prepare requests without executing theme, useful for inspection or custom processing:

```{r}
is_endpoint_active <- validate_hf_endpoint(endpoint_url, "HF_TEST_API_KEY")

req <- hf_embed_request_single(
  text = "Text for custom request",
  endpoint_url = endpoint_url,
  max_retries = 5,       # retry failed requests
  timeout = 30,          
  validate = FALSE,     # skip validation if endpoint is up (we just checked so it will be)
  key_name = "HF_TEST_API_KEY"
)

req # check it before you run it
```

## Manual Request Execution

If you've checked the request and everything looks ok, go ahead and execute it. We'll set tidy = FALSE first so that we can see what the raw request comes back like, then we'll tidy it up:

```{r}
response_raw <- hf_perform_single(req, tidy = FALSE)

resp_status <- httr2::resp_status(response_raw)
resp_headers <- httr2::resp_headers(response_raw)
```

We should now end up with our tidied data frame where each column is an embedding dimension.

```{r}
(embeddings <- tidy_embedding_response(response_raw))
```

## Batch Processing with Data Frames

Work directly with the component functions for custom batch workflows.

### Batch Requests

Sending concurrent requests is one way to be more efficient - mainly through increasing throughput. Another way is to send more data inside each request, thus reducing total number of requests, and time spent sending requests to the endpoint.

To be concrete, this means sending multiple rows of data inside each request. If we have 1,000 rows of data, and we create batches of size 50, we'll have $\frac{1000}{50}= 20$ total requests to send, rather than the 1,000 requests we have without batching. If our endpoint is powered by GPU(s) or other hardware accelerators, it will take care of processing the data using parallel computation, increasing speed up.

TODO: come back to this when batching is fully fleshed out.

Going back to our list of sentences which we aved in the list - `embedding_sentences` above, we can create a single request with all 10 sentences batched in using `hf_build_request_batch()`

```{r}
batch_request <- hf_build_request_batch(
  embedding_sentences, endpoint_url, key_name = "HF_TEST_API_KEY")
```

Now if you inspect the body of the request, and look at data -\> inputs, you'll see the list of 10 sentences.

```{r}
batch_request$body$data
```

You can use `hf_perform_request()`to send the request. To the endpoint it will look like a single request, and it will return a single response. Each individual item in our requests `body$data$inputs` will be embedded separately.

```{r}
batch_result <- hf_perform_request(batch_request)
```

### Testing Batching

TODO Fix following re-factoring

grab a random id, re-embed it, chekc they're identical. this should go in tests somewhwre.

```{r}
batched_embeddings <- hf_embed_batch(sentences_1000, endpoint_url, "HF_TEST_API_KEY", concurrent_requests = 5)

test_index <- batched_embeddings |>  slice(157)
test_embed <- test_index |>  select(contains("V"))
test_repro_embed <- hf_embed_text(test_index$text, endpoint_url = endpoint_url, "HF_TEST_API_KEY")

all(test_embed[, 1:384] |> unlist()-test_repro_embed[, 1:384] |>  unlist() == 0) 
```

### Chunking Data Frames

When we have a lot of data - 100,000s or 1,000,000s of rows - we may want (or need!) to break the task down into manageable chunks.

### Writing to Files

Embeddings are returned to us as data frames of floating point numbers, where each column is an embedding dimension. The number of dimensions will usually fall in the range of \$\approx\$300-3000 (and some LLMs have internal represenations in the 10,000s of dimensions range!). That's a lot of data, and our computers only have so much RAM available. If we try to embed 100,000s - 1,000,000 rows at a time, there's a heightened chance of our R session crashing, meaning we lose the results. This wastes time, API credits, energy, and water.

It would be better to chunk our data up, create batches of requests, and write each batched response to a file. The file will persist if our R session crashes.

> At this stage it would normally be advisable to build up your own request directly with httr2 - you'll have full control over batching, concurrent requests, exponential back-off, time-outs and retries, error handling.

However, because it seems quite likely people will run into trouble, and there's a lot at stake if we're sending 100,000s-1,000,000s of requests, we've created a high-level function for chunking a data frame, batching requests, sending concurrent batches, and appending the batch to a file.

## Error Handling

The low-level API allows more detailed error inspection and recovery. We just wrap `req_perform` in `purrr::safely`. This changes the return object slightly, we now have a list with `$result` and `$error`

```{r}
result <- safely_perform_request(req)

if (!is.null(result$error)) {
  print(result$error)
} else {
  embeddings <- tidy_embedding_response(result$result)
}
```

## Testing Responses

If you are testing an API request with `hf_embed_text()`, you can pass additional arguments into `httr2::req_perform()`. For example, you can pass in a value for 'verbosity' to get more information on the request.

How much information to print?

From {httr}'s docs: \> This is a wrapper around req_verbose() that uses an integer to control verbosity: - 0: no output - 1: show headers - 2: show headers and bodies - 3: show headers, bodies, and curl status messages

You can also pass in a value for 'path', which will save the response to a file, we'll look more at how to manage this later.
