---
title: "performance_testing_memory"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: flatly
    number_sections: true
    fig_caption: true
    df_print: paged
    highlight: tango
    code_folding: show
    anchor_sections: true
---

```{r, setup}
library(EndpointR)
library(tidyverse)
library(here)
library(httr2)
library(furrr)
library(purrr)
library(profvis)
library(jsonlite)
```

```{r}
review_df <- get_review_df()
responses_tibble <- readRDS("~/dev/projects/copilot_endpointr_trial_run/all_responses.Rdata")
# responses_tibble <- tibble(response = httr2::resps_successes(responses_tibble$response)) # just successes

```

# oai_complete_df

Solution(s):

-   We want to make sure that we batch things up properly. That likely means sacrificing some time for stability. For example, we won't be able to just fire off all of the requests in parallel. We'll need to prepare batches, deal with that batch's requests/responses in parallel, tidy/validate them, and *then* move to the next batch. Each batch's
-   We don't need all of the headers and additional information in any response list/data frame, we should extract just the message/content that contains the information we're after
-   Take validaton logic out to its own function, and run that after the responses have been received? Still implement chunking etc. or actually have the user decide to do that specifically, after processing of responses is done.
-   Add an endpointr_id to each request as a header so that it persists with the response. Then we don't need to do the match or a look-up. We can uses resps_succeses(), convert to data frame. Perhaps endpointr_id should be a mandatory argument. âœ…
-   Now that we're separating successes and failures, we can make .extract_response_fields simpler too.
-   

# Hugging Face

## Embeddings

With \~210k documents we did get fairly high memory usage (peaking at \~12-15gb) which does hint at some inefficiency, as the all-mini-lm embeddings are quite small

# Refactoring

## Schema Type Checking

```{r, schema_type_check}
  if (!is.null(schema)) {
    if (inherits(schema, "EndpointR::json_schema") || inherits(schema, "json_schema") || inherits(schema, "S7_object")){
      schema_dump <- json_dump(schema)

      if (!is.null(schema_dump$json_schema$schema)) {
        schema_dump$json_schema$schema$additionalProperties <- FALSE # must be the case for OAI structured outputs
      }

      body$response_format <- schema_dump
    } else {
      body$response_format <- schema
    }
  }

```

```{r, schema_type_check_refactor}
body <- list()

if (!is.null(schema)) {
  if (inherits(schema, "EndpointR::json_schema")){
    schema <- json_dump(schema)
  }
  body$response_format <- schema
}

body$response_format
```

```{r}
schema <- create_json_schema(
  name = "test_json_schema",
  schema = schema_object(name = "hello"),
  strict = TRUE
  )

non_schema <- list("hello")
inherits(schema, c("EndpointR::json_schema"))
inherits(non_schema, c("EndpointR::json_schema"))

json_dump(schema) |>
  jsonlite::toJSON()
```

## IDs as metadata?

to make sure we keep track of the request, and the ID comes back with the response, we can add a header - this will get returned in the `response$request$headers`

```{r}
oai_base_req <- EndpointR::oai_build_completions_request(input = "hello")
oai_base_req <- oai_base_req |>
  httr2::req_headers(EndpointR_id = "id_101")
response <- httr2::req_perform(oai_base_req)
response$request$headers$EndpointR_id

response |> 
  httr2::resp_body_json() |>
  purrr::pluck("choices", 1, "message", "content")
```

## Single-dump schema

in oai_complete_df, we want to check for a schema, and if we have one dump it just the once. If we don't copy the schema then later, in validation, the schema isn't available. But currently I'm thinking validation *should not* take place in the oai_complete_df function but outside of it, in a separate function (which means re-ordering the vignette somewhat)

```{r}
  # we need to dump the schema just once
  if(!is.null(schema)) {
    # we want dump the schema once here. This way it will not be dumped in each individual request because it's being dumped 
    if(inherits(schema, "EndpointR::json_schema")) { 
      schema <- json_dump(schema) }
    
  }
  
  # cleaner way:
  if(!is.null(schema) && inherits(schema, "EndpointR::json_schema")) {
      schema <- json_dump(schema)
  }
```

add browser() to oai_build_completions_request to check if we trigger the call

```{r}
sentiment_schema <- create_json_schema(
    name = "sentiment_test",
    schema = schema_object(
      sentiment = schema_enum(
        values = c("positive", "negative", "neutral"),
        description = "Sentiment classification for the document",
        type = "string"
      ),
      required = list("sentiment"),
      additional_properties = FALSE
    )
  )

# review_df <- get_review_df()  # in testthat helper
results <- oai_complete_df(df = review_df, text_var = review_text, id_var = id, schema = sentiment_schema, concurrent_requests = 5) # ok, doesn't trigger. - tested with browser()
```

```{r}
library(profvis)
results_profviz <-
  profvis({
    oai_complete_df(df = review_df, text_var = review_text, id_var = id, schema = sentiment_schema, concurrent_requests = 5)   
  })

lobstr::mem_used()

```

## Validation Function

Should pull this out into its own function, and reduce the complaxity significantly. Currently we were validating twice - once to check validation would be ok for all, and then again if it was. This is doubly inefficient obviously, but as validation seemed very cheap compared to performing the requests it didn't seem so bad.

Current thinking is that responses should be handed back to the user in a data frame and the user should decide whether to validate, as at large-ish scale processing, validating everything is costly, and more likely the user ends the session or similar.

```{r, validate_schema_chunk}
  # validate against schema ----
  # in the old implementation, if the schema didn't validate we'd error - and lose any susccessful requests too. This is more complex, but it handles the edge cases better.

  if (!is.null(schema)) {
    # validate and track errors to make sure we don't try to unnest data that doesn't exist, and maintain homogeneity in output shape
    validation_errors <- purrr::map_chr(results_df$content, ~{
      if (!is.na(.x)) {
        tryCatch({
          validate_response(schema, .x)
          NA_character_
        }, error = function(e) as.character(e$message))
      } else {
        NA_character_
      }
    })

    results_df <- results_df |>
      dplyr::mutate(.error_msg = dplyr::coalesce(.error_msg, validation_errors))


    n_validation_errors <- sum(!is.na(validation_errors))

    if (n_validation_errors > 0) {
      cli::cli_warn(c(
        "{n_validation_errors} response{?s} failed schema validation",
        "i" = "Returning raw JSON in 'content' column for all rows"
      ))
    } else {
      # can only unnest safely if ALL validations passed, or we'd want to create a 'content' column for unsuccessful validations, and, e.g. 'sentiment' for successful validatons in the simple sentiment case...
      results_df <- results_df |>
        dplyr::mutate(
          content = purrr::map(content, ~validate_response(schema, .x))
        ) |>
        tidyr::unnest_wider(content)
    }
  }
```

```{r}
profvis({
  resps_successes(responses_5k$response) |> 
  map( ~ .x |> resp_body_json())   
})
responses |> 
  mutate(content = map( content, ~ jsonlite::fromJSON(.x)))
```

Flow:

Make all requests (in batch or altogether?)

## chunks developmet

```{r}
review_df <- get_review_df()
ids <- review_df$id
texts <- review_df$review_text

oai_complete_chunks(
  texts,
  ids,
  chunk_size = 2
)

oai_complete_df(review_df,
                review_text,
                id,
                chunk_size = 1,
                output_file = NULL)
```

## handle output_file

Utils

```{r}
.handle_output_filename <- function(x, base_file_name = "batch_processing_") {
  if (is.null(x)) {
    return(tempfile(pattern = base_file_name, fileext = ".csv"))
  }

  if(identical(x, "auto")) {
    timestamp <- format(Sys.time(), "%d%m%Y_%H%M%S")
    output_file <- paste0(base_file_name, "_", timestamp, ".csv")

    return(output_file)
  }

  if (!endsWith(tolower(x), ".csv")) {
    cli::cli_abort("`output_file` must have a .csv extension")
  }

  return(x)
}

```

```{r}

data <- read_csv("~/Google Drive/My Drive/data_science_project_work/microsoft/project_work/823_copilot_associations/data/wrangled_data/flagged_df.csv", n_max = 1000)

data <- data |> 
  select(universal_message_id, clean_message)

responses_1k_writes <- data |>
  # slice(1:10) |>
  oai_complete_df(
    text_var = clean_message, # Change as needed
    id_var = universal_message_id, # Change as needed
    output_file = "responses_1k.csv",
    model = "gpt-4.1-nano",
    chunk_size = 200,
    system_prompt = copilot_prompt,
    schema = copilot_schema,
    concurrent_requests = 50,
    max_retries = 10,
    timeout = 50,
    temperature = 0,
    max_tokens = 100L
  )
```

```{r}
progress_test <- data |> 
  slice(1:40) |> 
  oai_complete_df(
    text_var = clean_message, # Change as needed
    id_var = universal_message_id, # Change as needed
    output_file = "responses_1k.csv",
    model = "gpt-4.1-nano",
    system_prompt = "classify sentiment",
    chunk_size = 20,
    concurrent_requests = 1,
    max_retries = 10,
    timeout = 50,
    temperature = 0,
    max_tokens = 100L
  )
```

# Validate DF Against Schema

For validation purposes, we were paying a fairly disgusting overhead when validating the schema.

When we want to validate all of the rows of a data frame, we want to convert the schema to JSON just once.

The data will have been converted from JSON -\> string. So we want to maybe skip that step. Then we want to return the validated.

use the sentiment_schema we've been playing around with

```{r}
sentiment_schema <- create_json_schema(
    name = "sentiment_test",
    schema = schema_object(
      sentiment = schema_enum(
        values = c("positive", "negative", "neutral"),
        description = "Sentiment classification for the document",
        type = "string"
      ),
      required = list("sentiment"),
      additional_properties = FALSE
    )
  )

```

```{r}
validate_df_against_schema <- function(df, x, schema) {
  
  stopifnot(inherits(schema, "EndpointR::json_schema"))
  
  schema_json <- jsonlite::toJSON(schema@schema)
  
}
```

```{r}
copilot_schema <- create_json_schema(
  name = "copilot_sentiment",  
  schema = schema_object(
    entity = schema_string(
      description = "Mention of Copilot"),
    sentiment = schema_enum(
      c("positive", "negative", "neutral", "mixed"),
      "sentiment of copilot mention"
    ),
    required = list("entity", "sentiment")
    ),
  )

responses_tibble <- responses_tibble |> 
  slice(1:1000)
```

```{r}
copilot_schema_json <- jsonlite::toJSON(copilot_schema@schema, auto_unbox = TRUE)

response_string <- responses_tibble$response[[1]] |> 
  resp_body_json() |> 
  pluck('choices', 1, "message", "content")

response_parsed_list <- jsonlite::fromJSON(response_string, simplifyVector = FALSE) 


jsonvalidate::json_validate(response_string, copilot_schema_json, engine = "ajv", verbose = TRUE)
```

```{r}
Rprof("profile.out", memory.profiling = TRUE)
jsonvalidate::json_validate(response_string, copilot_schema_json, engine = "ajv", verbose = TRUE)
Rprof(NULL)
summaryRprof("profile.out", memory = "both")
```

memory profiling: sample.interval=20000 :3560172:4418320:192387552:4: :3560618:4419492:192497032:54:"context_eval" "get_str_output" "evaluate_js" "ct$source" "jsonvalidate_js" "initialize" "json_schema$new" "json_validator" "jsonvalidate::json_validate" :3561985:4419521:192855040:189:"match.arg" "toJSON" "FUN" "vapply" "v8$call" "private$do_validate" "validator" "jsonvalidate::json_validate"

```{r}
responses_tibble |> 
  slice(1) |>  
  mutate(content = map(response, resp_body_json))  |>
  pull(content) |> 
  pluck(1, 'choices', 1, "message", "content") |> 
  jsonlite::fromJSON() |> 
  as_tibble()
```

```{r}
'{
  "entities": [
    {"entity": "Copilot", "sentiment": "negative"},
    {"entity": "Microsoft", "sentiment": "neutral"},
    {"entity": "AI assistance", "sentiment": "positive"}
  ]
}' |> 
  jsonlite::fromJSON() |> 
  as_tibble()
```

```{r}
'{
  "documents": [
    {
      "document_id": "review_001",
      "entities": [
        {"entity": "GitHub Copilot", "sentiment": "positive"},
        {"entity": "pricing", "sentiment": "negative"}
      ]
    },
    {
      "document_id": "review_002", 
      "entities": [
        {"entity": "code completion", "sentiment": "positive"},
        {"entity": "Microsoft", "sentiment": "neutral"},
        {"entity": "VS Code integration", "sentiment": "positive"}
      ]
    }
  ]
}
' |> 
  jsonlite::fromJSON() |> 
  pluck("documents") |> 
  unnest(entities) |> 
  as_tibble()
```

## profiling

old way:

new way:

memory comparison/opps for improvement

```{r}
start <- Sys.time()
responses_tibble |> 
  # slice(1:10) |>
  mutate(content = 
           map(response, ~ .x |>
                 .extract_successful_completion_content() |> 
                 fromJSON()
           ))

end <- Sys.time() - start
end
```

```{r}
library(furrr)
future::plan(future::multisession(workers = future::availableCores() - 1))
safe_pluck <- purrr::safely(purrr::pluck)

profvis({
  responses_tibble |> 
  group_split(row_number() %% 15) |>
  map( ~ .x |>
         # slice(1:10) |>
          # mutate(response = future_map(
          mutate(response = map(
            response, ~ .x |>
              httr2::resp_body_json() |>
              safe_pluck('choices', 1, "message", "content")
          ))
  )  
})


future::plan(future::sequential())


future::plan(future::multisession(workers = future::availableCores() - 1))
responses_tibble |>
  mutate(response = 
           future_map(
             response, ~ .x |>
               httr2::resp_body_json() |> 
               purrr::pluck('choices', 1, "message", "content"))
  )
future::plan(future::sequential())
```

## Dealing with refusals

If OpenAI return a valid response, but with a refusal. Our normal method of plucking will not wor unless we set the .default = NA or .defualt = NA_character\_

```{r}
responses_tibble |>  
  slice(37211) |>  
  pull(response) |> 
  pluck(1) |>  
  resp_body_json() |>  
  pluck("choices", 1, "message", "content") |> 
  fromJSON()
  # pluck("choices", 1, "message", "content", .default = NA)




responses_tibble |> 
  slice(1000:1100) |>
  mutate(content = 
           map(response,
               .x |>
                 safely_parse_completion()))

responses_tibble |> 
  slice(37200:37215) |>
  mutate(content = 
           map(response, {
             response_parsing <- ~ .x |>
               extract_and_parse_completion()
           })) |> 
  unnest_wider(content)



responses_tibble |> 
  slice(1000:10000) |>
  mutate(content = 
           map(response, {
             response_parsing <- ~ .x |>
               parse_safely()
           }))  |> 
  unnest_wider(content)  

responses_tibble |>  
  slice(37210:37215) |>  
  # pull(response) |> 
  # pluck(1)  |>
  mutate(content = map(response, ~ {
    content <- .x |> 
      resp_body_json() |> 
      pluck("choices", 1, "message", "content", .default = NA_character_)
    
    if (is.na(content)) return(NA)
    fromJSON(content)
  })) |> 
  unnest_wider(content)
```

```{r}
responses_tibble |>  
  slice(37210:37215) |> 
  mutate(content = map(
    response, ~ resp_body_json(.x) |> 
      pluck("choices", 1, "message", "content", .default = NA_character_) |>
      safely_from_json() 
                       )) |> 
  unnest_wider(content)
```

potential routes:

```{r}
is_refusal <- function(oai_response) {
  
}

.parse_safely <- purrr::possibly(~ .x |> 
                         .extract_successful_completion_content() |> 
                         jsonlite::fromJSON(), 
                       otherwise = list())


.safely_from_json <- purrr::possibly(.f = jsonlite::fromJSON,
                                     otherwise = list())


inspect_content_names <- function(responses_df, content_var = content, n = 5) {
  
  content_sym <- rlang::ensym(content_var)
  
  content_names <- responses_df |>
    dplyr::slice_head(n = n) |>
    dplyr::pull(content) |>
    purrr::map(names)
  
  return(content_names)
  
}
```

# Real Data - 11th July 2025

```{r, setup_data_copilot_prompt_schema}
data <- read_csv("~/Google Drive/My Drive/data_science_project_work/microsoft/project_work/823_copilot_associations/data/wrangled_data/df_all_flags.csv", n_max = 100)
responses_slice <- responses_tibble |>  slice(37210:37225)

data <- data |> 
  select(universal_message_id, clean_message)

copilot_prompt <- "
You are an emotionally-intelligent assistant who focuses solely on the brand \"Microsoft Copilot\" in social media posts. 
I want you to examine a given document and extract the sentiment specifically attributed towards Copilot, regardless of the document's overall sentiment.
- I only care about mentions of \"Microsoft Copilot\" - no other brands or entities should be considered, such as GitHub Copilot.
- For every mention of Copilot, determine the sentiment in the context where it appears, categorising it as \"positive\", \"negative\", \"neutral\", or \"mixed\" (if multiple sentiments are expressed regarding Copilot).
- If Microsoft Copilot is mentioned only in a neutral context (e.g., general announcements, greetings, or messages not directed specifically at Copilot's attributes or actions), classify the sentiment as neutral.
- If Copilot is mentioned in a URL or in a context that does not express any clear sentiment (e.g., mentions in job postings or generic references), classify the sentiment as neutral.
- If the document contains no mention of Copilot, output should indicate that with \"NO_ENTITIES\".
Example:
Document: \"Taking the test has been really hard and burdensome. What I've found to help me prepare is Microsoft Copilot's study materials\"
Output:
{
  \"entities\": [
    {
      \"entity\": \"Copilot\",
      \"sentiment\": \"positive\"
    }
  ]
}
Document: \"I was really disappointed with how Copilot integrated with powerpoint, especially when other brands like OpenAI seem to offer much better integration.
Output:
{
  \"entities\": [
    {
      \"entity\": \"Copilot\",
      \"sentiment\": \"negative\"
    }
  ]
}
Document: \"happy holidays from Github Copilot! wishing you a wonderful season and plenty of rest.\"
Output:
{
  \"entities\": [
    {
      \"entity\": \"NO_ENTITIES\",
      \"sentiment\": \"neutral\"
    }
  ]
}
Please respond to the given document by following these instructions exactly.
"

copilot_schema <- create_json_schema(
  name = "copilot_sentiment",  
  schema = schema_object(
    entity = schema_string(
      description = "Mention of Copilot"),
    sentiment = schema_enum(
      c("positive", "negative", "neutral", "mixed"),
      "sentiment of copilot mention"
    ),
    required = list("entity", "sentiment")
    ),
  )
```

```{r}
small_trial <- data |> 
  slice(1:5) |> 
    oai_complete_df(
      text_var = clean_message, # Change as needed
      id_var = universal_message_id, # Change as needed
      model = "gpt-4.1-nano",
      system_prompt = copilot_prompt,
      schema = copilot_schema,
      concurrent_requests = 5,
      max_retries = 10,
      timeout = 50,
      temperature = 0,
      max_tokens = 30L
  )

small_trial |>
  mutate(content = map(content, safely_from_json)) |> 
  unnest_wider(content)
```

```{r}
nchar(
'))
```
