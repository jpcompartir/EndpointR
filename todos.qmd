# EndpointR Hugging Face Embeddings Implementation Checklist

## Prios:

First: separation of concerns inside perform_requests_with_strategy - function shouldn't perform the requests and tidy them. Tidying should be done on the responses, but outside of the perform_requests_with_strategy.

Once the separation of concerns is achieved: Second prio: Fix tidying functions for the case where `return_all_scores = FALSE`

As in this: \> single_one_score \|\> + resp_body_json() \|\> + flatten() \|\> + as.data.frame() label score 1 not_spam 0.9985068

Need to add parameters to the request here, really. And also need to add to batch...

## Fixing Tidying

Fixing hf_classify.R and perform_requests_with_strategy:

Question whether the funcs should try to take care of `return_all_scores = FALSE` as well as `return_all_scores = TRUE`, not clear how much benefit there is to allowing the FALSE option, and unclear how we'd track. With an individual parameter, or extracting from parameters, seems messy.

Should the user provide the cleaning function for the `hf_classify_batch()` and `hf_classify_df()` or should we try to do it for them, in all cases? Should this be an argument? There are already quite a lot of arguments. I'd be tempted to just go with `return_all_scores = TRUE` by default, and have other people deal with them separately if they don't want to do this.

`hf_classify_batch` is a bit of a smell atm, as it has quite a few args and is fairly sizeable. The high-level functions have to take care of: - retries - batching - concurrent_requests - handle errors - ordering of outputs/batches

## Re-factor & Decouple inference and embed files

Decouple hf_inference.R from hf_embed.R

-   [x] remove tidy option from request (leave to hf_embed.R)
-   [x] update batching with changes to hf_perform_request
    -   [x] separate concerns & make re-usable
        -   break batching logic into a utility function
        -   [x] batch_vector
        -   [x] tidy_batches (wasn't necessary)

## Core Features

-   [x] Embed a text
-   [x] Embed a batch of texts
-   [x] Embed a data frame of texts

### Testing

-   [ ] Error handling & Reporting
    -   [ ] All errors
    -   [ ] Some errors

## Core Functions

-   [x] `base_request()` - Base POST request other functions will use, headers, api key, method
-   [x] `hf_build_request()` - Prepare a single text embedding request
-   [x] `hf_embed_batch()` - Prepare batched requests for multiple texts
-   [x] `hf_embed_text()` - High-level function for embedding single texts
-   [x] `hf_embed_df()` - High-level function for embedding texts in a data frame

## Utility Functions

-   [x] `tidy_embedding_response()` - Process API responses into tidy format
-   [x] `safely_perform_request()` - Error handling wrapper for requests
-   [x] `validate_hf_endpoint()` - Validate endpoint before sending requests
-   [x] `batch_vector`

## Implementation Order

1.  Start with single text embedding functionality
2.  Add data frame and batch processing capabilities
3.  Implement parallel processing with proper error handling
4.  Add caching for improved efficiency
5.  Create high-level user-friendly functions
6.  Write/update vignette

## HF Batch & Parallel implementation

-   at first glance it seems more straightforward to implement:
    -   batch the df
    -   deal with the requests *within the batch* in parallel
    -   package the batches back up into the df Because we can easily keep track of the id column and the text. But, httr2 req_perform_parallel should return concurrent requests in order they arrived. So we can actually batch up the requests to reduce the number of requests being sent, which reduces some of the overhead.

From a design POV, with batch and parallel in the same function, there are too many arguments and there are dependencies within arguments, which are difficult to reason about. Plus, there are all the different ways progress can be reported, which makes it not simple (by httr2 for lists of requests, for parallel requests, by cli:: for batches, etc.).

To make things slightly more complex, from the httr2 docs we see that req_perform_parallel doesn't respect max_retries...

> Additionally, it does not respect the max_tries argument to req_retry() because if you have five requests in flight and the first one gets rate limited, it's likely that all the others do too. This also means that the circuit breaker is never triggered.

Given we're sending to the same API, it should respect req_throttle and req_retry:

> The main limitation of req_perform_parallel() is that it assumes applies req_throttle() and req_retry() are across all requests. This means, for example, that if request 1 is throttled, but request 2 is not, req_perform_parallel() will wait for request 1 before performing request 2. This makes it most suitable for performing many parallel requests to the same host, rather than a mix of different hosts. It's probably possible to remove these limitation, but it's enough work that I'm unlikely to do it unless I know that people would fine it useful: so please let me know!

Batching multiple texts within a request is handled quite straightforwardly with: `req_body_json(list(inputs = sentences[31:60]))`

It's just we have to do more work the other end with the response handling - particularly if we're using safely and parallel etc. Is it worth it?

### Writing to files

We can do it simply with the `path =` argument in `httr2::req_perform()`, but the response

### Hugging Face Classification checklist

-   [x] `hf_classify_text()`
-   \[WIP\] `hf_classify_batch()`
-   [ ] `hf_classify_df()`
-   [x] `tidy_classification_response()` - deal with label/score
-   [x] `tidy_batch_classification_response()`
-   \[WIP\] TESTS
-   [ ] S7 models for Peaks & Pits, Sentiment?

# OpenAI

Headliners:

-   [x] Completions API Requests & Responses
-   [ ] Responses API Requests & Responses
-   [ ] Name Functions - don't have as clear a naming structure as `hf_classify/hf_embed`

## Vignette

-   [x] Requests & Responses
    -   [x] Single
    -   [x] List
        -   [x] Sequential
        -   [x] Parallel
    -   [ ] DF
    -   Structured Output
-   [ ] Responses

## Completions API

-   [ ] Base Requests
    -   [x] Single Req
    -   [x] List of Reqs (can't send batches like we can with HF Inf)
    -   [ ] Refactor this to get the API just right...
    -   [ ] Data Frame of Reqs
-   [ ] Responses
-   [x] Structured Outputs
-   [x] JSON Schema
-   [x] S7 Objects (Like Pydantic)
-   [ ] Dynamic Rate Limit Management (From info in response headers)

## Responses API

## Embeddings API

for the classifier use-case / just generally accessing the v powerful (but large) embeddings OAI offer.

# JSON Schema

## Core S7 Class Architecture

-   [x] Define `json_schema` S7 class
-   [x] Add validator for class constraints/type checking
-   [x] Implement constructor function `create_json_schema()`

## Generic Methods System

-   [x] Create S7 generics for schema operations
-   [x] Implement `json_dump()` method for API serialisation
-   [x] Build `validate_response()` method with jsonvalidate integration

## Schema Builder Functions

-   [x] Core object schema with `schema_object()`
-   [x] Primitive type helpers:
-   [x] `schema_string()` with enum support
-   [x] `schema_number()` with min/max constraints
-   [x] `schema_integer()` with range validation
-   [x] `schema_boolean()` for binary fields
-   [x] Advanced type helpers:
-   [x] `schema_enum()` with mixed type support
-   [x] `schema_array()` with item constraints
-   [x] Documentation with practical examples

Schema -\> Type relationship? e.g. schema_number has its own method which coerces to numeric, or let user?

## Validation & Error Handling

-   [x] JSON response validation against schema
-   [x] Informative error messages via cli package
-   [x] Support for both list and JSON string inputs Check out [convert](https://rconsortium.github.io/S7/reference/convert.html) for type conversion Check out the various S7 types

## API Integration

-   [x] Format output for OpenAI structured outputs
-   [x] Handle optional description fields
-   [x] Strict mode configuration

## Testing & Polish

-   [x] Unit tests for all schema builders
-   [ ] Integration tests with actual LLM APIs
-   [ ] Performance benchmarks for large schemas
-   [ ] Vignettes with real-world examples
-   [ ] Vignette re-working/refining based on JH feedback
-   [ ] Optional Fields and Schemas
