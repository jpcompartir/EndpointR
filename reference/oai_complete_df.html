<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df • EndpointR</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Process a data frame through OpenAI's Chat Completions API with chunked processing — oai_complete_df"><meta name="description" content="This function takes a data frame with text inputs and processes each row through
OpenAI's Chat Completions API using efficient chunked processing. It handles
concurrent requests, automatic retries, and structured output validation while
writing results progressively to disk."><meta property="og:description" content="This function takes a data frame with text inputs and processes each row through
OpenAI's Chat Completions API using efficient chunked processing. It handles
concurrent requests, automatic retries, and structured output validation while
writing results progressively to disk."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">EndpointR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.1.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><h6 class="dropdown-header" data-toc-skip>Getting Started</h6></li>
    <li><a class="dropdown-item" href="../articles/api_keys.html">API Key Management</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Hugging Face</h6></li>
    <li><a class="dropdown-item" href="../articles/hugging_face_inference.html">Hugging Face Inference Endpoints</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>LLM Providers (OpenAI, Anthropic)</h6></li>
    <li><a class="dropdown-item" href="../articles/llm_providers.html">Working with Major LLM Providers</a></li>
    <li><a class="dropdown-item" href="../articles/structured_outputs_json_schema.html">Structured Outputs with JSON Schema</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Embeddings</h6></li>
    <li><a class="dropdown-item" href="../articles/embeddings_providers.html">Embeddings Providers</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Advanced Topics</h6></li>
    <li><a class="dropdown-item" href="../articles/improving_performance.html">Improving Performance</a></li>
  </ul></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-news" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">News</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-news"><li><h6 class="dropdown-header" data-toc-skip>Releases</h6></li>
    <li><a class="dropdown-item" href="../news/index.html">Version 0.1.0</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><a class="dropdown-item" href="../news/index.html">Changelog</a></li>
  </ul></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/jpcompartir/EndpointR" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Process a data frame through OpenAI's Chat Completions API with chunked processing</h1>
      <small class="dont-index">Source: <a href="https://github.com/jpcompartir/EndpointR/tree/main/R/openai_completions.R" class="external-link"><code>R/openai_completions.R</code></a></small>
      <div class="d-none name"><code>oai_complete_df.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>This function takes a data frame with text inputs and processes each row through
OpenAI's Chat Completions API using efficient chunked processing. It handles
concurrent requests, automatic retries, and structured output validation while
writing results progressively to disk.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">oai_complete_df</span><span class="op">(</span></span>
<span>  <span class="va">df</span>,</span>
<span>  <span class="va">text_var</span>,</span>
<span>  <span class="va">id_var</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"gpt-4.1-nano"</span>,</span>
<span>  output_file <span class="op">=</span> <span class="st">"auto"</span>,</span>
<span>  system_prompt <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  schema <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  chunk_size <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>  concurrent_requests <span class="op">=</span> <span class="fl">1L</span>,</span>
<span>  max_retries <span class="op">=</span> <span class="fl">5L</span>,</span>
<span>  timeout <span class="op">=</span> <span class="fl">30</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  max_tokens <span class="op">=</span> <span class="fl">500L</span>,</span>
<span>  key_name <span class="op">=</span> <span class="st">"OPENAI_API_KEY"</span>,</span>
<span>  endpoint_url <span class="op">=</span> <span class="st">"https://api.openai.com/v1/chat/completions"</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-df">df<a class="anchor" aria-label="anchor" href="#arg-df"></a></dt>
<dd><p>Data frame containing text to process</p></dd>


<dt id="arg-text-var">text_var<a class="anchor" aria-label="anchor" href="#arg-text-var"></a></dt>
<dd><p>Column name (unquoted) containing text inputs</p></dd>


<dt id="arg-id-var">id_var<a class="anchor" aria-label="anchor" href="#arg-id-var"></a></dt>
<dd><p>Column name (unquoted) for unique row identifiers</p></dd>


<dt id="arg-model">model<a class="anchor" aria-label="anchor" href="#arg-model"></a></dt>
<dd><p>OpenAI model to use (default: "gpt-4.1-nano")</p></dd>


<dt id="arg-output-file">output_file<a class="anchor" aria-label="anchor" href="#arg-output-file"></a></dt>
<dd><p>Path to .CSV file for results. "auto" generates the filename, location and is persistent across sessions. If NULL, generates timestamped filename.</p></dd>


<dt id="arg-system-prompt">system_prompt<a class="anchor" aria-label="anchor" href="#arg-system-prompt"></a></dt>
<dd><p>Optional system prompt applied to all requests</p></dd>


<dt id="arg-schema">schema<a class="anchor" aria-label="anchor" href="#arg-schema"></a></dt>
<dd><p>Optional JSON schema for structured output (json_schema object or list)</p></dd>


<dt id="arg-chunk-size">chunk_size<a class="anchor" aria-label="anchor" href="#arg-chunk-size"></a></dt>
<dd><p>Number of texts to process in each batch (default: 5000)</p></dd>


<dt id="arg-concurrent-requests">concurrent_requests<a class="anchor" aria-label="anchor" href="#arg-concurrent-requests"></a></dt>
<dd><p>Integer; number of concurrent requests (default: 5)</p></dd>


<dt id="arg-max-retries">max_retries<a class="anchor" aria-label="anchor" href="#arg-max-retries"></a></dt>
<dd><p>Maximum retry attempts per failed request (default: 5)</p></dd>


<dt id="arg-timeout">timeout<a class="anchor" aria-label="anchor" href="#arg-timeout"></a></dt>
<dd><p>Request timeout in seconds (default: 30)</p></dd>


<dt id="arg-temperature">temperature<a class="anchor" aria-label="anchor" href="#arg-temperature"></a></dt>
<dd><p>Sampling temperature (0-2), lower = more deterministic (default: 0)</p></dd>


<dt id="arg-max-tokens">max_tokens<a class="anchor" aria-label="anchor" href="#arg-max-tokens"></a></dt>
<dd><p>Maximum tokens per response (default: 500)</p></dd>


<dt id="arg-key-name">key_name<a class="anchor" aria-label="anchor" href="#arg-key-name"></a></dt>
<dd><p>Name of environment variable containing the API key (default: OPENAI_API_KEY)</p></dd>


<dt id="arg-endpoint-url">endpoint_url<a class="anchor" aria-label="anchor" href="#arg-endpoint-url"></a></dt>
<dd><p>OpenAI API endpoint URL</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A tibble with the original id column and additional columns:</p><ul><li><p><code>content</code>: API response content (text or JSON string if schema used)</p></li>
<li><p><code>.error</code>: Logical indicating if request failed</p></li>
<li><p><code>.error_msg</code>: Error message if failed, NA otherwise</p></li>
<li><p><code>.batch</code>: Batch number for tracking</p></li>
</ul></div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>This function provides a data frame interface to the chunked processing
capabilities of <code><a href="oai_complete_chunks.html">oai_complete_chunks()</a></code>. It extracts the specified text column,
processes texts in configurable chunks with concurrent API requests, and returns
results matched to the original data through the <code>id_var</code> parameter.</p>
<p>The chunking approach enables processing of large data frames without memory
constraints. Results are written progressively to a CSV file (either specified
or auto-generated) and then read back as the return value.</p>
<p>When using structured outputs with a <code>schema</code>, responses are validated against
the JSON schema and stored as JSON strings. Post-processing may be needed to
unnest these into separate columns.</p>
<p>Failed requests are marked with <code>.error = TRUE</code> and include error messages,
allowing for easy filtering and retry logic on failures.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://github.com/jpcompartir" class="external-link">Jack Penzer</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

