<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks • EndpointR</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Process text chunks through OpenAI's Chat Completions API with batch file output — oai_complete_chunks"><meta name="description" content="This function processes large volumes of text through OpenAI's Chat Completions API
in configurable chunks, writing results progressively to a CSV file. It handles
concurrent requests, automatic retries, and structured outputs while
managing memory efficiently for large-scale processing."><meta property="og:description" content="This function processes large volumes of text through OpenAI's Chat Completions API
in configurable chunks, writing results progressively to a CSV file. It handles
concurrent requests, automatic retries, and structured outputs while
managing memory efficiently for large-scale processing."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">EndpointR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><h6 class="dropdown-header" data-toc-skip>Getting Started</h6></li>
    <li><a class="dropdown-item" href="../articles/api_keys.html">API Key Management</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Hugging Face</h6></li>
    <li><a class="dropdown-item" href="../articles/hugging_face_inference.html">Hugging Face Inference Endpoints</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>LLM Providers (OpenAI, Anthropic)</h6></li>
    <li><a class="dropdown-item" href="../articles/llm_providers.html">Working with Major LLM Providers</a></li>
    <li><a class="dropdown-item" href="../articles/structured_outputs_json_schema.html">Structured Outputs with JSON Schema</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Embeddings</h6></li>
    <li><a class="dropdown-item" href="../articles/embeddings_providers.html">Embeddings Providers</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Advanced Topics</h6></li>
    <li><a class="dropdown-item" href="../articles/improving_performance.html">Improving Performance</a></li>
  </ul></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-news" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">News</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-news"><li><h6 class="dropdown-header" data-toc-skip>Releases</h6></li>
    <li><a class="dropdown-item" href="../news/index.html">Version 0.1.0</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><a class="dropdown-item" href="../news/index.html">Changelog</a></li>
  </ul></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/jpcompartir/EndpointR" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Process text chunks through OpenAI's Chat Completions API with batch file output</h1>
      <small class="dont-index">Source: <a href="https://github.com/jpcompartir/EndpointR/tree/main/R/openai_completions.R" class="external-link"><code>R/openai_completions.R</code></a></small>
      <div class="d-none name"><code>oai_complete_chunks.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>This function processes large volumes of text through OpenAI's Chat Completions API
in configurable chunks, writing results progressively to a CSV file. It handles
concurrent requests, automatic retries, and structured outputs while
managing memory efficiently for large-scale processing.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">oai_complete_chunks</span><span class="op">(</span></span>
<span>  <span class="va">texts</span>,</span>
<span>  <span class="va">ids</span>,</span>
<span>  chunk_size <span class="op">=</span> <span class="fl">5000L</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"gpt-4.1-nano"</span>,</span>
<span>  system_prompt <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  output_file <span class="op">=</span> <span class="st">"auto"</span>,</span>
<span>  schema <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  concurrent_requests <span class="op">=</span> <span class="fl">5L</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fl">0L</span>,</span>
<span>  max_tokens <span class="op">=</span> <span class="fl">500L</span>,</span>
<span>  max_retries <span class="op">=</span> <span class="fl">5L</span>,</span>
<span>  timeout <span class="op">=</span> <span class="fl">30L</span>,</span>
<span>  progress <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  key_name <span class="op">=</span> <span class="st">"OPENAI_API_KEY"</span>,</span>
<span>  endpoint_url <span class="op">=</span> <span class="st">"https://api.openai.com/v1/chat/completions"</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-texts">texts<a class="anchor" aria-label="anchor" href="#arg-texts"></a></dt>
<dd><p>Character vector of texts to process</p></dd>


<dt id="arg-ids">ids<a class="anchor" aria-label="anchor" href="#arg-ids"></a></dt>
<dd><p>Vector of unique identifiers corresponding to each text (same length as texts)</p></dd>


<dt id="arg-chunk-size">chunk_size<a class="anchor" aria-label="anchor" href="#arg-chunk-size"></a></dt>
<dd><p>Number of texts to process in each batch (default: 5000)</p></dd>


<dt id="arg-model">model<a class="anchor" aria-label="anchor" href="#arg-model"></a></dt>
<dd><p>OpenAI model to use (default: "gpt-4.1-nano")</p></dd>


<dt id="arg-system-prompt">system_prompt<a class="anchor" aria-label="anchor" href="#arg-system-prompt"></a></dt>
<dd><p>Optional system prompt applied to all requests</p></dd>


<dt id="arg-output-file">output_file<a class="anchor" aria-label="anchor" href="#arg-output-file"></a></dt>
<dd><p>Path to .CSV file for results. "auto" generates the filename, location and is persistent across sessions. If NULL, generates timestamped filename.</p></dd>


<dt id="arg-schema">schema<a class="anchor" aria-label="anchor" href="#arg-schema"></a></dt>
<dd><p>Optional JSON schema for structured output (json_schema object or list)</p></dd>


<dt id="arg-concurrent-requests">concurrent_requests<a class="anchor" aria-label="anchor" href="#arg-concurrent-requests"></a></dt>
<dd><p>Integer; number of concurrent requests (default: 5)</p></dd>


<dt id="arg-temperature">temperature<a class="anchor" aria-label="anchor" href="#arg-temperature"></a></dt>
<dd><p>Sampling temperature (0-2), lower = more deterministic (default: 0)</p></dd>


<dt id="arg-max-tokens">max_tokens<a class="anchor" aria-label="anchor" href="#arg-max-tokens"></a></dt>
<dd><p>Maximum tokens per response (default: 500)</p></dd>


<dt id="arg-max-retries">max_retries<a class="anchor" aria-label="anchor" href="#arg-max-retries"></a></dt>
<dd><p>Maximum retry attempts per failed request (default: 5)</p></dd>


<dt id="arg-timeout">timeout<a class="anchor" aria-label="anchor" href="#arg-timeout"></a></dt>
<dd><p>Request timeout in seconds (default: 30)</p></dd>


<dt id="arg-progress">progress<a class="anchor" aria-label="anchor" href="#arg-progress"></a></dt>
<dd><p>Logical; whether to show progress bar (default: TRUE)</p></dd>


<dt id="arg-key-name">key_name<a class="anchor" aria-label="anchor" href="#arg-key-name"></a></dt>
<dd><p>Name of environment variable containing the API key (default: OPENAI_API_KEY)</p></dd>


<dt id="arg-endpoint-url">endpoint_url<a class="anchor" aria-label="anchor" href="#arg-endpoint-url"></a></dt>
<dd><p>OpenAI API endpoint URL</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A tibble containing all results with columns:</p><ul><li><p><code>id</code>: Original identifier from input</p></li>
<li><p><code>content</code>: API response content (text or JSON string if schema used)</p></li>
<li><p><code>.error</code>: Logical indicating if request failed</p></li>
<li><p><code>.error_msg</code>: Error message if failed, NA otherwise</p></li>
<li><p><code>.batch</code>: Batch number for tracking</p></li>
</ul></div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>This function is designed for processing large text datasets that may not
fit comfortably in memory. It divides the input into chunks, processes each chunk
with concurrent API requests, and writes results immediately to disk to minimise
memory usage.</p>
<p>The function preserves data integrity by matching results to source texts through
the <code>ids</code> parameter. Each chunk is processed independently with results appended
to the output file, allowing for resumable processing if interrupted.</p>
<p>When using structured outputs with a <code>schema</code>, responses are validated against
the JSON schema but stored as raw JSON strings in the output file. This allows
for flexible post-processing without memory constraints during the API calls.</p>
<p>The chunking strategy balances API efficiency with memory management. Larger
<code>chunk_size</code> values reduce overhead but increase memory usage. Adjust based on
your system resources and text sizes.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="co"># basic usage with automatic file naming:</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># large-scale processing with custom output file:</span></span></span>
<span class="r-in"><span><span class="co">#structured extraction with schema:</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># post-process structured results:</span></span></span>
<span class="r-in"><span><span class="va">xx</span> <span class="op">&lt;-</span> <span class="va">xx</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="op">!</span><span class="va">.error</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>parsed <span class="op">=</span> <span class="fu">map</span><span class="op">(</span><span class="va">content</span>, <span class="op">~</span><span class="fu">jsonlite</span><span class="fu">::</span><span class="fu"><a href="https://jeroen.r-universe.dev/jsonlite/reference/fromJSON.html" class="external-link">fromJSON</a></span><span class="op">(</span><span class="va">.x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span></span>
<span class="r-in"><span>  <span class="fu">unnest_wider</span><span class="op">(</span><span class="va">parsed</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://github.com/jpcompartir" class="external-link">Jack Penzer</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

