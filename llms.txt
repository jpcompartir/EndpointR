# EndpointR

EndpointR is a ‘batteries included’, open-source R package for
connecting to various Application Programming Interfaces
([APIs](https://en.wikipedia.org/wiki/API)) for Machine Learning model
predictions.

> **TIP:** If you are an experienced programmer, or have experience with
> hitting APIs, consider going directly to
> [httr2](https://httr2.r-lib.org/reference/index.html)

# Installation

EndpointR will not be put on CRAN, so you can download and install the
latest development version with the following code:

``` r
library(EndpointR)
remotes::install_github("jpcompartir/EndpointR")
```

# Quick Starts

## Hugging Face - embeddings

Securely set your API key

``` r
set_api_key("HF_API_KEY")
```

Point to an endpoint - this is for the ‘all-mpnet-base-v2’ model with
feature extraction (embeddings)

``` r
endpoint_url <- "https://router.huggingface.co/hf-inference/models/sentence-transformers/all-mpnet-base-v2/pipeline/feature-extraction" 
```

Embed a single text:

``` r
 hf_embed_text(
  text = "Convert this text to embeddings",
  endpoint_url = endpoint_url,
  key_name = "HF_API_KEY"
)
```

Embed a list of texts in batches:

``` r
review_texts <-c(
    "Absolutely fantastic service! The staff were incredibly helpful and friendly.",
    "Terrible experience. Food was cold and the waiter was rude.",
    "Pretty good overall, but nothing special. Average food and service.",
    "Outstanding meal! Best restaurant I've been to in years. Highly recommend!",
    "Disappointed with the long wait times. Food was okay when it finally arrived."
  )

hf_embed_batch(
  texts = review_texts,
  endpoint_url = endpoint_url,
  key_name = "HF_API_KEY",
  batch_size = 3,
  concurrent_requests = 2
)
```

Embed a data frame of texts:

``` r
review_data <- tibble::tibble(
  review_id = 1:5,
  review_text = review_texts
)
```

``` r
hf_embed_df(
  df = review_data,
  text_var = review_text,
  id_var = review_id,
  endpoint_url = endpoint_url,
  key_name = "HF_API_KEY",
  output_dir = "embeddings_output",  # writes .parquet chunks to this directory
  chunk_size = 5000,  # process 5000 rows per chunk
  concurrent_requests = 2,
  max_retries = 5,
  timeout = 15
)
```

## Hugging Face - Classification

Select a Classification Endpoint URL

``` r
sentiment_endpoint <- "https://router.huggingface.co/hf-inference/models/cardiffnlp/twitter-roberta-base-sentiment"
```

Classify a single text:

You’ll need to grab the label2id mapping from the model’s card: [Cardiff
NLP model
info](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/blob/main/README.md)

``` r
labelid_2class <- function() {
  return(list(negative = "LABEL_0",
              neutral = "LABEL_1",
              positive = "LABEL_2"))
}

 hf_classify_text(
  text = review_texts[[1]],
  endpoint_url = sentiment_endpoint,
  key_name = "HF_API_KEY"
) |> 
   dplyr::rename(!!!labelid_2class())
```

Classify a data frame:

``` r
hf_classify_df(
  df = review_data,
  text_var = review_text,
  id_var = review_id,
  endpoint_url = sentiment_endpoint,
  key_name = "HF_API_KEY",
  max_length = 512,  # truncate texts longer than 512 tokens
  output_dir = "classification_output",  # writes .parquet chunks to this directory
  chunk_size = 2500,  # process 2500 rows per chunk
  concurrent_requests = 3,
  max_retries = 5,
  timeout = 60
) |>
  dplyr::rename(!!!labelid_2class())
```

Read the [Hugging Face Inference
Vignette](https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.md)
for more information on embedding and classifying using Dedicated
Inference Endpoints and the Inference API from Hugging Face.

## OpenAI - Chat Completions API

Make sure you’ve set your API key:

``` r
set_api_key("OPENAI_API_KEY")
```

Complete a single text:

``` r
oai_complete_text(
  text = review_texts[[2]],
  system_prompt = "Classify the sentiment of the following text: "
)
```

Complete a single text with a schema and tidy:

``` r
sentiment_schema <- create_json_schema(
  name = "sentiment_analysis",
  schema = schema_object(
    sentiment = schema_string("positive, negative, or neutral"),
    confidence = schema_number("confidence score between 0 and 1"), # we don't necessarily recommend asking a model for its confidence score, this is mainly a schema-construction demo!
    required = list("sentiment", "confidence")
  )
)

oai_complete_text(
  text = review_texts[[2]],
  system_prompt = "Classify the sentiment of the following text: ",
  schema = sentiment_schema,
  tidy = TRUE
) |> 
  tibble::as_tibble()
```

Complete a Data Frame of texts:

``` r
oai_complete_df(
  df = review_data,
  text_var = review_text,
  id_var = review_id,
  system_prompt = "Classify the following review:",
  key_name = "OPENAI_API_KEY",
  output_dir = "completions_output",  # writes .parquet chunks to this directory
  chunk_size = 1000,  # process 1000 rows per chunk
  concurrent_requests = 5,  # send 5 rows of data simultaneously
  max_retries = 5,
  timeout = 30
)
```

Complete a Data Frame of texts with schema:

``` r
df_output_w_schema <- oai_complete_df(
  df = review_data,
  text_var = review_text,
  id_var = review_id,
  system_prompt = "Classify the following review:",
  schema = sentiment_schema,
  key_name = "OPENAI_API_KEY",
  output_dir = NULL,
  # output_dir = "completions_output",
  chunk_size = 1000,
  concurrent_requests = 5
)

df_output_w_schema |> 
  dplyr::mutate(content = purrr::map(content, safely_from_json)) |>
  tidyr::unnest_wider(content)
```

# Working with Output Files

## Reading Results from Disk

Hugging Face functions
([`hf_embed_df()`](https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.md),
[`hf_classify_df()`](https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.md))
write intermediate results as `.parquet` files in the specified
`output_dir`. To read all results back:

``` r
# List all parquet files (excludes metadata.json automatically)
parquet_files <- list.files("embeddings_output",
                           pattern = "\\.parquet$",
                           full.names = TRUE)

# Read all chunks into a single data frame
results <- arrow::open_dataset(parquet_files, format = "parquet") |>
  dplyr::collect()
```

## Understanding metadata.json

Each Hugging Face output directory contains a `metadata.json` file that
records:

- `endpoint_url`: The API endpoint used
- `chunk_size`: Number of rows processed per chunk
- `n_texts`: Total number of texts processed
- `concurrent_requests`: Parallel request setting
- `timeout`: Request timeout in seconds
- `max_retries`: Maximum retry attempts
- `inference_parameters`: Model-specific parameters (e.g., truncate,
  max_length)
- `timestamp`: When the job was run
- `key_name`: Which API key was used

This metadata is useful for:

- Debugging failed runs
- Reproducing results with the same settings
- Tracking which endpoint/model was used
- Understanding performance characteristics

``` r
metadata <- jsonlite::read_json("embeddings_output/metadata.json")

# check which endpoint was used
metadata$endpoint_url
```

**Note:** Add output directories to `.gitignore` to avoid committing API
responses and metadata.

Read the [LLM Providers
Vignette](https://jpcompartir.github.io/EndpointR/articles/llm_providers.md),
and the [Structured Outputs
Vignette](https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.md)
for more information on common workflows with the OpenAI Chat
Completions API [¹](#fn1)

# API Key Security

- Read the [httr2
  vignette](https://httr2.r-lib.org/articles/wrapping-apis.html#basics)
  on managing your API keys securely and encrypting them.

- Read the [EndpointR API
  Keys](https://jpcompartir.github.io/EndpointR/articles/api_keys.md)
  vignette for information on which API keys you need for wach endpoint
  we support, and how to securely import those API keys into your
  .Renvironfile.

------------------------------------------------------------------------

1.  Content pending implementation for Anthroic Messages API, Gemini
    API, and OpenAI Responses API

# Package index

## Getting Started

Essential functions for setting up and managing API keys

- [`set_api_key()`](https://jpcompartir.github.io/EndpointR/reference/set_api_key.md)
  : Set your API keys so they can be accessed by EndpointR
- [`get_api_key()`](https://jpcompartir.github.io/EndpointR/reference/get_api_key.md)
  : Retrieve an API key which has been stored as an Environment
  Variable.

## Hugging Face - Text Embeddings

Functions for generating text embeddings using Hugging Face endpoints

- [`hf_embed_text()`](https://jpcompartir.github.io/EndpointR/reference/hf_embed_text.md)
  : Generate embeddings for a single text
- [`hf_embed_batch()`](https://jpcompartir.github.io/EndpointR/reference/hf_embed_batch.md)
  : Generate batches of embeddings for a list of texts
- [`hf_embed_chunks()`](https://jpcompartir.github.io/EndpointR/reference/hf_embed_chunks.md)
  : Embed text chunks through Hugging Face Inference Embedding Endpoints
- [`hf_embed_df()`](https://jpcompartir.github.io/EndpointR/reference/hf_embed_df.md)
  : Generate embeddings for texts in a data frame
- [`tidy_embedding_response()`](https://jpcompartir.github.io/EndpointR/reference/tidy_embedding_response.md)
  : Process embedding API response into a tidy format

## Hugging Face - Text Classification

Functions for classifying text using Hugging Face endpoints

- [`hf_classify_text()`](https://jpcompartir.github.io/EndpointR/reference/hf_classify_text.md)
  : Classify text using a Hugging Face Inference API endpoint
- [`hf_classify_batch()`](https://jpcompartir.github.io/EndpointR/reference/hf_classify_batch.md)
  : Classify multiple texts using Hugging Face Inference Endpoints
- [`hf_classify_chunks()`](https://jpcompartir.github.io/EndpointR/reference/hf_classify_chunks.md)
  : Efficiently classify vectors of text in chunks
- [`hf_classify_df()`](https://jpcompartir.github.io/EndpointR/reference/hf_classify_df.md)
  : Classify a data frame of texts using Hugging Face Inference
  Endpoints
- [`tidy_classification_response()`](https://jpcompartir.github.io/EndpointR/reference/tidy_classification_response.md)
  : Convert Hugging Face classification response to tidy format

## Hugging Face - Core Infrastructure

Low-level functions for building and performing Hugging Face requests

- [`hf_build_request()`](https://jpcompartir.github.io/EndpointR/reference/hf_build_request.md)
  : Prepare a single text embedding request
- [`hf_build_request_batch()`](https://jpcompartir.github.io/EndpointR/reference/hf_build_request_batch.md)
  : Prepare a batch request for multiple texts
- [`hf_build_request_df()`](https://jpcompartir.github.io/EndpointR/reference/hf_build_request_df.md)
  : Prepare embedding requests for texts in a data frame
- [`hf_perform_request()`](https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.md)
  : Execute a single embedding request and process the response

## Hugging Face Endpoint Utilities

Small functions for checking things with Hugging FACE APIs

- [`hf_get_endpoint_info()`](https://jpcompartir.github.io/EndpointR/reference/hf_get_endpoint_info.md)
  : Retrieve information about an endpoint
- [`hf_get_model_max_length()`](https://jpcompartir.github.io/EndpointR/reference/hf_get_model_max_length.md)
  : Check the max number of tokens allowed for your inputs

## Anthropic Messages

functions for working with Anthropic’s Messages API

- [`ant_build_messages_request()`](https://jpcompartir.github.io/EndpointR/reference/ant_build_messages_request.md)
  : Build an Anthropic Messages API request
- [`ant_complete_text()`](https://jpcompartir.github.io/EndpointR/reference/ant_complete_text.md)
  : Generate a completion for a single text using Anthropic's Messages
  API
- [`ant_complete_chunks()`](https://jpcompartir.github.io/EndpointR/reference/ant_complete_chunks.md)
  : Process text chunks through Anthropic's Messages API with batch file
  output
- [`ant_complete_df()`](https://jpcompartir.github.io/EndpointR/reference/ant_complete_df.md)
  : Process a data frame through Anthropic's Messages API

## OpenAI Completions

Functions for working with OpenAI’s APIs including structured outputs

- [`oai_build_completions_request()`](https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request.md)
  : Build an OpenAI API Chat Completions request
- [`oai_build_completions_request_list()`](https://jpcompartir.github.io/EndpointR/reference/oai_build_completions_request_list.md)
  : Build OpenAI requests for batch processing
- [`oai_complete_text()`](https://jpcompartir.github.io/EndpointR/reference/oai_complete_text.md)
  : Generate a completion for a single text using OpenAI's Chat
  Completions API
- [`oai_complete_chunks()`](https://jpcompartir.github.io/EndpointR/reference/oai_complete_chunks.md)
  : Process text chunks through OpenAI's Chat Completions API with batch
  file output
- [`oai_complete_df()`](https://jpcompartir.github.io/EndpointR/reference/oai_complete_df.md)
  : Process a data frame through OpenAI's Chat Completions API with
  chunked processing

## OpenAI Embeddings

Functions for extracting embeddings with OpenAI’s text embedding models

- [`oai_build_embedding_request()`](https://jpcompartir.github.io/EndpointR/reference/oai_build_embedding_request.md)
  : Build OpenAI embedding API request
- [`oai_embed_text()`](https://jpcompartir.github.io/EndpointR/reference/oai_embed_text.md)
  : Generate embeddings for a single text using OpenAI
- [`oai_embed_batch()`](https://jpcompartir.github.io/EndpointR/reference/oai_embed_batch.md)
  : Generate embeddings for multiple texts using OpenAI
- [`oai_embed_chunks()`](https://jpcompartir.github.io/EndpointR/reference/oai_embed_chunks.md)
  : Embed text chunks through OpenAI's Embeddings API
- [`oai_embed_df()`](https://jpcompartir.github.io/EndpointR/reference/oai_embed_df.md)
  : Generate embeddings for texts in a data frame using OpenAI
- [`tidy_oai_embedding()`](https://jpcompartir.github.io/EndpointR/reference/tidy_oai_embedding.md)
  : Process OpenAI embedding API response into a tidy format

## JSON Schema for Structured Outputs

Type-safe schema creation and validation for structured LLM outputs

- [`create_json_schema()`](https://jpcompartir.github.io/EndpointR/reference/create_json_schema.md)
  : Create a JSON Schema object
- [`json_schema()`](https://jpcompartir.github.io/EndpointR/reference/json_schema.md)
  : Create JSON Schema S7 class for structured outputs
- [`json_dump`](https://jpcompartir.github.io/EndpointR/reference/json_dump.md)
  : Convert json_schema to API format
- [`validate_response`](https://jpcompartir.github.io/EndpointR/reference/validate_response.md)
  : Validate response data against schema

## Schema Builders

Helper functions for creating different types of JSON schema properties

- [`schema_object()`](https://jpcompartir.github.io/EndpointR/reference/schema_object.md)
  : Create JSON Schema object definitions
- [`schema_string()`](https://jpcompartir.github.io/EndpointR/reference/schema_string.md)
  : Create string property schema
- [`schema_number()`](https://jpcompartir.github.io/EndpointR/reference/schema_number.md)
  : Create numeric property schema
- [`schema_integer()`](https://jpcompartir.github.io/EndpointR/reference/schema_integer.md)
  : Create integer property schema
- [`schema_boolean()`](https://jpcompartir.github.io/EndpointR/reference/schema_boolean.md)
  : Create boolean property schema
- [`schema_enum()`](https://jpcompartir.github.io/EndpointR/reference/schema_enum.md)
  : Create enumerated property schema
- [`schema_array()`](https://jpcompartir.github.io/EndpointR/reference/schema_array.md)
  : Create array property schema

## Core Utilities

Internal utility functions and data processing helpers

- [`safely_perform_request()`](https://jpcompartir.github.io/EndpointR/reference/safely_perform_request.md)
  : Safely perform an embedding request with error handling
- [`chunk_dataframe()`](https://jpcompartir.github.io/EndpointR/reference/chunk_dataframe.md)
  : Split a data frame into chunks for batch processing
- [`perform_requests_with_strategy()`](https://jpcompartir.github.io/EndpointR/reference/perform_requests_with_strategy.md)
  : Perform multiple requests with configurable concurrency strategy
- [`process_response()`](https://jpcompartir.github.io/EndpointR/reference/process_response.md)
  : Process API response with error handling
- [`hf_perform_request()`](https://jpcompartir.github.io/EndpointR/reference/hf_perform_request.md)
  : Execute a single embedding request and process the response
- [`validate_hf_endpoint()`](https://jpcompartir.github.io/EndpointR/reference/validate_hf_endpoint.md)
  : Validate that a Hugging Face Inference Endpoint is available
- [`base_request()`](https://jpcompartir.github.io/EndpointR/reference/base_request.md)
  : Create a base HTTP POST request for API endpoints
- [`safely_from_json()`](https://jpcompartir.github.io/EndpointR/reference/safely_from_json.md)
  : Safely extract JSON

## Sample Data

Datasets included with the package for examples and testing

- [`batch_concurrent_benchmark`](https://jpcompartir.github.io/EndpointR/reference/batch_concurrent_benchmark.md)
  : Batch concurrent benchmark results
- [`sentiment_classification_example`](https://jpcompartir.github.io/EndpointR/reference/sentiment_classification_example.md)
  : Single sentiment classification result example
- [`df_sentiment_classification_example`](https://jpcompartir.github.io/EndpointR/reference/df_sentiment_classification_example.md)
  : Example sentiment classification results from Hugging Face API
- [`single_embedding_hf`](https://jpcompartir.github.io/EndpointR/reference/single_embedding_hf.md)
  : Single embedding result example from Hugging Face API
- [`df_embeddings_hf`](https://jpcompartir.github.io/EndpointR/reference/df_embeddings_hf.md)
  : Example embedding results from Hugging Face API

# Articles

### All vignettes

- [Managing API Keys
  Securely](https://jpcompartir.github.io/EndpointR/articles/api_keys.md):
- [Embeddings
  Providers](https://jpcompartir.github.io/EndpointR/articles/embeddings_providers.md):
- [Using Hugging Face Inference
  Endpoints](https://jpcompartir.github.io/EndpointR/articles/hugging_face_inference.md):
- [Improving
  Performance](https://jpcompartir.github.io/EndpointR/articles/improving_performance.md):
- [Connecting to Major Model
  Providers](https://jpcompartir.github.io/EndpointR/articles/llm_providers.md):
- [Structured Outputs - JSON
  Schema](https://jpcompartir.github.io/EndpointR/articles/structured_outputs_json_schema.md):
