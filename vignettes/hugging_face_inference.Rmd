---
title: "Using Hugging Face Inference Endpoints"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using Hugging Face Inference Endpoints}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

```{r setup}
library(EndpointR)
library(dplyr)
library(tibble)
library(httr2)
library(purrr)
```

# Who is this vignette for?

This vignette is for users needing to embed text using pre-configured company endpoints via the {EndpointR} package, assuming you have received your API key and the endpoint URL/details.

# What is Hugging Face?

For the past few years [Hugging Face](https://huggingface.co/) has been the de-facto location for open-source AI. It is a place to save and share datasets & Machine Learning models alike. Importantly, we can train our own models and then upload them to the Hugging Face Hub, so that other people can access them.

# What are Inference Endpoints?

Any model that is saved on Hugging Face can be connected to a Hugging Face Inference Endpoint allowing us to get predictions from that model using Hugging Face's [^1] hardware. Given that modern-day Machine Learning models tend to require a large amount of compute, and not everybody has powerful laptops to use the models locally, this is a democratising force and a useful tool to have at our disposal.

[^1]: or another inference provider's

## Common Use-cases

-   Text embeddings via Sentence Transformers
-   Sentiment analysis
-   Text classification
-   Image recognition
-   Question answering
-   Text generation

This vignette focuses on using Hugging Face endpoints for text embeddings.

# API Key Management

Assuming the endpoint has already been set up, you'll need to retrieve your API key from Hugging Face (or the team responsible for providing API keys) and store it securely in an environment variable.

`set_api_key` will ask you to input the value for your API key using askpass, this reduces [^2] the likelihood of your API key ending up in your code somewhere

[^2]: but never reduces to zero, so always be vigilant!

```{r}
set_api_key("HF_TEST_API_KEY")
```

# High-level API

The high-level API provides convenient functions for common embedding tasks without needing to understand the underlying request structure.

## Embedding a Single Text

> For this example, I have previously set up a package-level key for encrypting sensitive information, `ENDPOINTR_KEY`, this can be used with {httr2}'s `secret_encrypt` and `secret_decrypt` function to encrypt any information.

1.  Make sure your api key is the correct one for your endpoint
2.  Get your endpoint's url
3.  Get a single piece of text you want to embed
4.  Call `hf_embed_text()`

If this is the first time we're hitting the endpoint, it will not be running. The endpoint initialises when we send a request with a valid API key. Behind the scenes the model's weights are loaded onto a server. This process tends to take \~20 seconds, so our first requests will always be delayed. `max_retries = 5` tells EndpointR to retry 5 times, with an increasing time increment between requests; this gives the endpoint enough time to initialise.

```{r}
endpoint_url <- httr2::secret_decrypt("kcZCsc92Ty7PuAk7_FdCcdOU_dlDpvWdDyfpwCWg-wW80eJxJPdQ68nz4V_0922SzSwM5_dYfpTOqsZ-GodUpLN4PQbwE73wlZkBCWIaIXc15g", "ENDPOINTR_KEY")

embeddings <- hf_embed_text(
  text = "This is a sample text to embed",
  endpoint_url = endpoint_url,
  key_name = "HF_TEST_API_KEY",
  max_retries = 5
)
```

The output is tidied into a tibble with 1 row and 384 columns, because the endpoint is using the 'all-minilm-l6-v2' model under the hood:

```{r}
embeddings
```

## Embedding a Data Frame of Texts

When working with multiple texts, it would be possible to create a `hf_embed_text` function call for each text, and save each result to its own variable:

```{r}
text_1_embeddings <- hf_embed_text(text = "This is a sample text to embed", endpoint_url = endpoint_url, key_name = "HF_TEST_API_KEY", max_retries = 5)
text_2_embeddings <- hf_embed_text(text = "This is a sample text nunber 2 to embed", endpoint_url = endpoint_url, key_name = "HF_TEST_API_KEY", max_retries = 5)

```

And so on, for as many texts as we have. But this would very quickly get old. Instead we want to process multiple texts in one function call. As data professionals, we are often working with data frames, and given this is where our texts and their IDs will be stored, it makes sense to roll our data up into a data frame, and embed directly from this data frame, i.e. we want to put a data frame into the function and get a data frame back out. Whilst this is convenient, we run into some difficulties when working with large data frames. We'll look at how to work around these difficulties below when we talk about batching and sending parallel requests.

For now, let's create a data fame and embed some texts[^3].

[^3]: Claude was used to generate these sentences, instructed to write 10 sentences about embeddings, APIs, being British, and to add humour where it saw fit. The results are 'mixed' at best.

```{r}
embedding_ids <- 1:10
embedding_sentences <- c(
  "Text embedding models compress your rambling prose into compact vectors, much like how British commuters squeeze themselves into tube carriages during rush hour.",
  "Setting up an inference endpoint without proper documentation is akin to attempting to navigate London using a map of Birmingham.",
  "When your embedding model starts hallucinating, it's rather like watching Parliament during Question Time—entertaining but utterly unpredictable.",
  "Optimising your inference endpoint is essential unless you fancy your users growing old whilst waiting for a response, perhaps even ageing enough to collect their pension.",
  "The distance between word embeddings reveals semantic relationships, though sadly not the distance between what your client requested and what they actually wanted.",
  "Creating multilingual embeddings is a bit like attempting to order tea properly across Europe—technically possible but fraught with cultural misunderstandings.",
  "Batch processing through inference endpoints saves computing resources, much like how the British save conversation topics by discussing the weather exclusively.",
  "Token limits on embedding APIs are the digital equivalent of a queue at a British post office—inevitably, you'll reach the front just as they close for lunch.",
  "Fine-tuning embedding models on domain-specific corpora is rather like training a British child to apologise—it requires repetition, patience, and considerable examples.",
  "When your inference endpoint crashes under load, it maintains that quintessentially British trait of breaking down precisely when most inconvenient."
)

embedding_df <-
  tibble(
    id = embedding_ids,
    sentence = embedding_sentences
  )
```

Now we can the use the `hf_embed_df` to embed each of the 10 rows in our data frame. Due to the lower number of sentences, this will happen almost instantaneously, so long as the endpoint is already running. If the endpoint has shut down - if it's been a while between your requests, give it \~30 seconds and it will start back up.

When embedding our data frame, we must provide an input to:

-   `text_var =` this should be the column in your data frame with the text data you need to embed.
-   `id_var =` this should be the column in your data frame which identifies each piece of text - this makes sure we can link our embeddings to the correct text.
-   `key_name` we've seen this before
-   `endpoint_url` we've seen this before

There are some other optional arguments:

-   `parallel` lets EndpointR know whether to send multiple requests or not. For now we'll set this to `FALSE`
-   `progress` lets EndpointR know whether to provide a progress bar, this is only really we're worth it if we're embedding a lot of data. For now we'll set it to `FALSE`
-   `include_errors` should the rows where the API request failed by kept in the data, or not? We'll set this to `TRUE` so that we get a data frame with the same number of rows as the one we put in.
-   `max_retries` tells EndpointR how many times it should try to repeat the request before moving on to the next item.

```{r}
result_df <- hf_embed_df(
  df = embedding_df,
  text_var = sentence,
  id_var = id,
  key_name = "HF_TEST_API_KEY",
  endpoint_url = endpoint_url,
  parallel = FALSE,
  progress = FALSE,
  include_errors = TRUE,
  max_retries = 5
)
```

We now have a data frame with our original ID column, the text column, and some new columns:

-   `.error` indicates whether there was an error or not
-   `.error_message` displays the content of the error if there was one

And then a number of columns with a 'V' followed by some digits. Each digit represents an 'embedding dimension', you should have as many 'VXX' columns as there are embedding dimensions for the model you are using.

You can check whether you had any erorrs a number of ways, I like to use `dplyr::count()` or `dplyr::filter()`, here's the count way:

```{r}
count(embedding_df, .error)
```

If you need to select only the columns that contain embeddings - for example if you want to feed these embeddings into a clustering, or dimensionality reduction model, you can select a range of columns using `dplyr::select()` - here I'll select V1:V384, which gives me all of the embeddings.

```{r}
embedding_df |> select(V1:V384)
```

If I had 768 embedding dimensions, I would want to input `V1:V768`, and similarly for 1024 dimensions, `V1:V1024`, you get the idea.

```{r}
embedding_sentences <- rep(embedding_sentences, 10)
embedding_ids <- 1:10
```

## Scaling Performance

So far we've looked at how to send a single piece of text, and how to iterate through a data frame full of texts, sending one request at a time. Whilst these are key workflows, once you're more comfortable with sending requests to APIs and receiving responses, you'll probably need to send a bunch of requests quickly.

Before we get into how, let's go a bit further into why.

For each row of our data frame, we need to create a request which we will then perform. We then need to perform the request by sending our information over the internet to the endpoint. The endpoint accepts our request, performs any computation it needs to make and sends a response. Each of these steps has a cost in time associated with it, and the cost for each step is not equal - some steps are more costly than others.

Creating each request is almost instant, but if we have a larger data frame - e.g. with 100,000 rows: creating 100,000 requests simultaneously will take $\approx$ 30s-90s. If we process them all at once, we'll have to wait the full amount of time before we send any requests and receive any responses. Clearly this is wasteful.

Sending each request to the endpoint and waiting for it to process the data then send its response, will take a lot longer than preparing the request. Therefore, if we can find ways to reduce the time we spend waiting here, we should be able to reduce overall time spent waiting significantly.

```{r}
result_df <- hf_embed_df(
  df = embedding_df,
  text_var = sentence,
  id_var= id,
  endpoint_url = endpoint_url,
  key_name = "HF_TEST_API_KEY",
  include_errors = TRUE,
  batch_size = 10,
  # max_active = 3,
  # parallel = TRUE,
  # max_active = 2,
  # parallel = TRUE,
  progress = TRUE
)

```

```{r}
result_df <- hf_embed_df(
  df = texts_df,
  text_var = text,
  endpoint_url = endpoint_url,
  key_name = "HF_TEST_API_KEY",
  id_var= id,
  include_errors = TRUE,
  batch_size = 2,
  max_active = 5,
  parallel = TRUE,
  progress = TRUE
)
```

```{r}
glimpse(result_df)
```

## Working with Larger Datasets

For larger datasets, batching and parallel processing help manage memory and time constraints. EndpointR will take care of batching your data, performing the requests, and tidying the response. Hugging Face's inference endpoint will take care of how batches are processed on the client's side.

```{r}
# process in batches with parallel requests
result_df <- hf_embed_df(
  df = texts_df,
  text_var = text,
  id_var = id, 
  key_name = "HF_TEST_API_KEY",
  endpoint_url = endpoint_url,
  batch_size = 100,       # process 20 texts at a time
  parallel = TRUE,       # execute requests in parallel
  max_active = 5,        # maximum 5 concurrent requests
  progress = TRUE        # show progress bar
)
```

# Low-level API

The low-level API offers more control over the embedding process. This is useful for custom workflows, debugging, or when you need fine-grained control over requests.

## Request Preparation

Prepare requests without executing them, useful for inspection or custom processing:

```{r}
is_endpoint_active <- validate_hf_endpoint(endpoint_url, "HF_TEST_API_KEY")

req <- hf_embed_request_single(
  text = "Text for custom request",
  endpoint_url = endpoint_url,
  max_retries = 5,       # retry failed requests
  timeout = 30,          # longer timeout for large texts
  validate = FALSE,       # skip validation if already done
  key_name = "HF_TEST_API_KEY"
)

req # check it before you run it
```

## Manual Request Execution

If you've checked the request and everything looks ok, go ahead and execute it. We'll set tidy = FALSE first so that we can see what the raw request comes back like, then we'll tidy it up:

```{r}
response_raw <- hf_perform_single(req, tidy = FALSE)

resp_status <- httr2::resp_status(response_raw)
resp_headers <- httr2::resp_headers(response_raw)
```

We should now end up with our tidied data frame where each column is an embedding dimension.

```{r}
(embeddings <- tidy_embedding_response(response_raw))
```

## Batch Processing with Data Frames

Work directly with the component functions for custom batch workflows.

First we create a data frame of requests - this takes a data frame as an input, and returns a data frame but with some additional columns added.

```{r}
requests_df <- hf_build_request_df(
  df = texts_df,
  text_var = text,
  id_var = id,
  endpoint_url = endpoint_url,
  key_name = "HF_TEST_API_KEY"
)

str(requests_df$.request[[1]]) # check the structure of the first request
requests_df$.request[[1]]$body # check the data that's been added to the request
```

Then we batch our data frames, returning a list of `nrow(df)/batch_size` where each element in the list is its own data frame with `nrow(df) == batch_size`.

Still very much a WIP / TODO as to how this should work, too manual atm, probably.

```{r}
batch_dfs <- requests_df |>  batch_dataframe(5)
length(batch_dfs)
```

```{r}
responses_batch_dfs <- purrr::map(batch_dfs, ~ hf_perform_sequential(.x), .progress = TRUE)

purrr::map(responses_batch_dfs, ~ process_batch_responses(.x)) |> 
  bind_rows()
```

## Error Handling

The low-level API allows more detailed error inspection and recovery. We just wrap `req_perform` in `purrr::safely`. This changes the return object slightly, we now have a list with `$result` and `$error`

```{r}
result <- safely_perform_request(req)

if (!is.null(result$error)) {
  print(result$error)
} else {
  embeddings <- tidy_embedding_response(result$result)
}
```
