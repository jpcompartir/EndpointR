---
title: "Using Hugging Face Inference Endpoints"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: flatly
    number_sections: true
    fig_caption: true
    df_print: paged
    highlight: tango
    code_folding: show
    anchor_sections: true
vignette: >
  %\VignetteIndexEntry{Using Hugging Face Inference Endpoints}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE,
  message = FALSE,
  warning = FALSE
)
```

This vignette shows how to embed and classify text with EndpointR using two Hugging Face services:

-   **Inference API**: Free, shared service for testing
-   **Dedicated Inference Endpoints**: Private, production-ready service

<details>

<summary>Do I need the Inference API or a Dedicated Inference Endpoint?</summary>

| Feature | Inference API | Dedicated Inference Endpoints |
|----|----|----|
| **Accessibility** | Public, shared service | Private, dedicated hardware |
| **Cost** | Free (with paid tiers) | Paid service - rent specific hardware |
| **Hardware** | Shared computing resources | Dedicated hardware allocation |
| **Wait Times** | Variable, unknowable in advance | Predictable, minimal queuing, \~30s for first request |
| **Production Ready** | Not recommended for production | Recommended for production use |
| **Use Case** | Casual usage, testing, prototyping | Production applications, consistent performance |
| **Scalability** | Limited by shared resources | Scales with dedicated allocation |
| **Availability** | Subject to shared infrastructure limits | Guaranteed availability during rental period |
| **Model Coverage** | Commonly-used models, models selected by Hugging Face | Virtually all models on the Hub are available |

</details>

# Getting Started

> **TIP**: EndpointR's Hugging Face functions work with the Inference API and Dedicated Inference Endpoints - you just need to change the URL in the `endpoint_url =` argument!

```{r setup, eval = TRUE}
library(EndpointR)
library(dplyr)
library(tibble)
library(httr2)
library(purrr)
library(tidyr)
```

Follow Hugging Face's [docs](https://huggingface.co/docs/hub/security-tokens) to generate a Hugging Face token, and then register it with EndpointR:

```{r, keys_and_urls}
set_api_key("HF_TEST_API_KEY") # I'm using a test API key for this vignette

# Option 1: Inference API (free, for testing)
inference_api_url <-"https://router.huggingface.co/hf-inference/models/sentence-transformers/all-mpnet-base-v2/pipeline/feature-extraction"

# Option 2: Dedicated Endpoint (production)
dedicated_url <- "https://your-endpoint-name.endpoints.huggingface.cloud"
```

# Getting Started

Go to [Hugging Face's models hub](https://huggingface.co/models) and fetch the Inference API's URL for the model you want to embed your data with. Not all models are available via the Hugging Face Inference API, if you need to use a model that is not available you may need to deploy a [Dedicated Inference Endpoint](https://huggingface.co/inference-endpoints/dedicated).

```{r, my_data}
my_data <- tibble(
  id = 1:3,
  text = c(
    "Machine learning is fascinating",
    "I love working with embeddings", 
    "Natural language processing is powerful"
  ),
  category = c("ML", "embeddings", "NLP")
)
```

## Core Functions

The two core functions for inference via Hugging Face services are used for embedding and classifying a data frame of texts.

To embed:

```{r}
embedding_result <- hf_embed_df(
  df = my_data,
  text_var = text,      # column containing text
  id_var = id,          # column with unique IDs
  key_name = "HF_TEST_API_KEY",
  endpoint_url = inference_api_url, # or dedicated_url
  progress = FALSE,     
  max_retries = 5
)
```

```{r, echo = FALSE, eval = TRUE}
data("df_embeddings_hf")
df_embeddings_hf
```

> **note**: The number of embedding columns (V1:V384) depends on your model. Check the model's documentation on Hugging Face for its embedding size

To classify:

```{r}
classification_url = "https://router.huggingface.co/hf-inference/models/distilbert/distilbert-base-uncased-finetuned-sst-2-english"

# Classify a data frame of texts
classification_result <- hf_classify_df(
  df = my_data,
  text_var = text,
  id_var = id,
  endpoint_url = classification_url,  # works with dedicated url also
  key_name = "HF_TEST_API_KEY"
)
```

```{r, echo = FALSE, eval = TRUE}
data("sentiment_classification_example")
sentiment_classification_example
```

# Additional Features

We can also classify and embed single texts, and lists (or batches) of text as we'll see below:

If the endpoint is not already active, it will take some time to initialise - the model's weights will be loaded onto a server. This process tends to take $\approx$ 20-30 seconds. Setting `max_retries = 5` will usually give the endpoint enough time to initialise, receive our request and return its response.

```{r}
embeddings <- hf_embed_text(
  text = "This is a sample text to embed",
  endpoint_url = inference_api_url,
  key_name = "HF_TEST_API_KEY",
  max_retries = 5
)
```

The output is tidied into a tibble with 1 row and 384 columns. Each column is an embedding dimension, and the 'all-minilm-l6-v2' model used by this endpoint has 384 dimensions.

```{r}
embeddings
```

## Embedding a List of Texts {#embedding-a-list-of-texts}

The `hf_embed_batch()` function handles a list of texts, and turns them into batches, with each batch according to `batch_size =`. The default value is 8, we'll change it to 3 and send all of our texts in 1 request.

```{r}
hf_embed_batch(
  my_data$text,
  inference_api_url,
  "HF_TEST_API_KEY",
  batch_size = 5,
)
```

The `hf_embed_batch` function will return our texts in a single data frame, with the text itself, two columns related to errors, and columns for each embedding dimension.

The `.error` and `.error_message` are introduced because we need to know whether errors occur at the individual post level. For example, if a batch of 10 requests has 2 errors, we want to see which individual post had errors, and keep the embeddings for the rest.

Batching is generally a good idea, and we'll see in the [Improving Performance Vignette](articles/improving_performance.html) that holding everything else equal, it does lead to a speed up over non-batched. However, if you try to set the batch_size too high, you are more likely to hit rate limits, or have your request rejected due to the payload being too large.

> **TIP**: Experiment to find the correct batch size for your data and the endpoint. Start small and work upwards in small incremeents.

## Embedding a Data Frame of Texts

As data professionals, we are more likely to work with data frames than any other data structure. We will usually have the texts we want to embed, their unique identifiers, and a selection of other columns to work with. EndpointR has a function - `hf_embed_df()` - built just for this use case.

The function requires an input to:

-   `text_var =` this should be the column in your data frame with the text data you need to embed.
-   `id_var =` this should be the column in your data frame which identifies each piece of text - this makes sure we can link our embeddings to the correct text.
-   `key_name` we've seen this before
-   `endpoint_url` we've seen this before

And then we have some optional arguments - these are optional because there is a default value for when we don't set them. We'll look at some of the other optional arguments in a later section. The key optional arguments for `hf_embed_df()` are:

-   `progress` lets EndpointR know whether to provide a progress bar, this is only really we're worth it if we're embedding a lot of data. For now we'll set it to `FALSE`
-   `max_retries` tells EndpointR how many times it should try to repeat the request before moving on to the next item.

> **NOTE**: For performance-related arguments, check out the [Improving Performance Vignette](articles/improving_performance.html)

```{r}
(
  result_df <- hf_embed_df(
  df = my_data,
  text_var = text,
  id_var = id,
  key_name = "HF_TEST_API_KEY",
  endpoint_url = inference_api_url,
  progress = FALSE,
  max_retries = 5)
)
```

We now have a data frame with our original ID column, the text column, the same '`.error` columns as in the batching case, and a number of columns with a 'V' followed by some digits. Each digit represents an 'embedding dimension', you should have as many 'VXX' columns as there are embedding dimensions for the model you are using.

You can check whether you had any errors a number of ways, I like to use `dplyr::count()` or `dplyr::filter()`, here's the count way:

```{r}
count(result_df, .error)
```

If you need to select only the columns that contain embeddings - for example if you want to feed these embeddings into a clustering, or dimensionality reduction model, you can select a range of columns using `dplyr::select()` - here I'll select V1:V384, which gives me all of the embeddings.

```{r}
result_df |> select(V1:V384)
```

If I had 768 embedding dimensions, I would input `V1:V768`, and similarly for 1024 dimensions, `V1:V1024`, you get the idea.

# Improving Performance

EndpointR's functions come with knobs and dials and you can turn to improve throughput and performance. Visit the [Improving Performance](articles/improving_performance.html) vignette for more information.
