---
title: "Using Hugging Face Inference Endpoints"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: flatly
    number_sections: true
    fig_caption: true
    df_print: paged
    highlight: tango
    code_folding: show
    anchor_sections: true
vignette: >
  %\VignetteIndexEntry{Using Hugging Face Inference Endpoints}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE,
  message = FALSE,
  warning = FALSE
)
```

# Introduction

```{r setup, eval = TRUE}
library(EndpointR)
library(dplyr)
library(tibble)
library(httr2)
library(purrr)
library(tidyr)
library(ggplot2)
```

## Who is this vignette for?

This vignette is for people who want to embed or classify data using pre-configured, dedicated Hugging Face Endpoints, assuming you have received your API key and the endpoint URL/details.

Experienced users looking for fine-grained control over their API requests will be better served by heading directly to the {[httr2](https://httr2.r-lib.org/index.html)} package.

## What is Hugging Face?

For the past few years [Hugging Face](https://huggingface.co/) has been the de-facto location for open-source AI. It is a place to save and share datasets & Machine Learning models alike. Importantly, we can train our own models and then upload them to the Hugging Face Hub for others to use.

## What are Inference Endpoints?

Any model that is saved on the Hugging Face Hub can be connected to a Hugging Face Inference Endpoint allowing us to get predictions from that model using Hugging Face's [^1] hardware. Given that modern-day Machine Learning models tend to require a large amount of compute, and not everybody has powerful laptops to use the models locally, this is a democratising force and a useful tool to have at our disposal.

[^1]: or another inference provider's

### Common Use-cases

-   Text embeddings via Sentence Transformers
-   Sentiment analysis
-   Text classification
-   Image recognition
-   Question answering
-   Text generation

This vignette focuses on using Hugging Face endpoints for text embeddings.

# Getting Started

## API Key Management

First read the [EndpointR API Keys](vignettes/api_keys.Rmd) vignette.

Assuming the endpoint has been set up, you'll need to retrieve your API key from Hugging Face (or from the team/department responsible for providing API keys) and store it securely in an environment variable.

`set_api_key` will ask you to input the value for your API key using {askpass}, this reduces [^2] the likelihood of your API key ending up in your code somewhere.

[^2]: but never reduces to zero, so always be vigilant!

```{r}
set_api_key("HF_TEST_API_KEY")
```

EndpointR functions that need API keys will have a `key_name` argument, you put the same name as you entered into `set_api_key` and EndpointR will retrieve it, without printing the contents to the console.

# Quick Start Guide

EndpointR provides convenient functions for common embedding tasks without exposing the underlying request structure. EndpointR will handle retries, errors, concurrent requests, and batching of requests through configurable options.

## Embedding a Single Text

> For this example, I have previously set up a package-level key for encrypting sensitive information, `ENDPOINTR_KEY`, this can be used with {httr2}'s `secret_encrypt` and `secret_decrypt` function to encrypt any information. For example, you can use these functions to encrypt your API keys themselves.

1.  Get your endpoint's URL
2.  Make sure the key_name points to the correct API key for your endpoint
3.  Get a single piece of text you want to embed
4.  Call `hf_embed_text()`

If the endpoint is not already active, it will take some time to initialise - the model's weights will be loaded onto a server. This process tends to take $\approx$ 20-30 seconds. Setting `max_retries = 5` will usually give the endpoint enough time to initialise, receive our request and return its response.

```{r}
endpoint_url <- httr2::secret_decrypt("kcZCsc92Ty7PuAk7_FdCcdOU_dlDpvWdDyfpwCWg-wW80eJxJPdQ68nz4V_0922SzSwM5_dYfpTOqsZ-GodUpLN4PQbwE73wlZkBCWIaIXc15g", "ENDPOINTR_KEY")

embeddings <- hf_embed_text(
  text = "This is a sample text to embed",
  endpoint_url = endpoint_url,
  key_name = "HF_TEST_API_KEY",
  max_retries = 5
)
```

The output is tidied into a tibble with 1 row and 384 columns. Each column is an embedding dimension, and the 'all-minilm-l6-v2' model used by this endpoint has 384 dimensions.

```{r}
embeddings
```

## Working with Multiple Texts

When working with multiple texts, it would be possible to create a `hf_embed_text` function call for each text, and save each result to its own variable:

```{r}
text_1_embeddings <- hf_embed_text(text = "This is a sample text to embed", endpoint_url = endpoint_url, key_name = "HF_TEST_API_KEY", max_retries = 5)
text_2_embeddings <- hf_embed_text(text = "This is a sample text nunber 2 to embed", endpoint_url = endpoint_url, key_name = "HF_TEST_API_KEY", max_retries = 5)

```

And so on, for as many texts as we have. But this would get very boring, very quickly. Instead, it would be nice if we could embed multiple texts at the same time.

## Embedding a List of Texts

Rather than send 10 requests and get 10 responses, we could batch our texts together so that each request has multiple texts. In the following example we'll send a list of 10 texts in 2 separate requests.

> **DISCLAIMER**: Claude was used to generate these sentences, instructed to write 10 sentences about embeddings, APIs, being British, and to add humour where it saw fit. The humour is 'mixed' at best.

```{r}
embedding_sentences <- c(
  "Text embedding models compress your rambling prose into compact vectors, much like how British commuters squeeze themselves into tube carriages during rush hour.",
  "Setting up an inference endpoint without proper documentation is akin to attempting to navigate London using a map of Birmingham.",
  "When your embedding model starts hallucinating, it's rather like watching Parliament during Question Time—entertaining but utterly unpredictable.",
  "Optimising your inference endpoint is essential unless you fancy your users growing old whilst waiting for a response, perhaps even ageing enough to collect their pension.",
  "The distance between word embeddings reveals semantic relationships, though sadly not the distance between what your client requested and what they actually wanted.",
  "Creating multilingual embeddings is a bit like attempting to order tea properly across Europe—technically possible but fraught with cultural misunderstandings.",
  "Batch processing through inference endpoints saves computing resources, much like how the British save conversation topics by discussing the weather exclusively.",
  "Token limits on embedding APIs are the digital equivalent of a queue at a British post office—inevitably, you'll reach the front just as they close for lunch.",
  "Fine-tuning embedding models on domain-specific corpora is rather like training a British child to apologise—it requires repetition, patience, and considerable examples.",
  "When your inference endpoint crashes under load, it maintains that quintessentially British trait of breaking down precisely when most inconvenient."
)
```

the `hf_embed_batch()` function handles a list of texts, and turns them into batches, sized according to what we enter into `batch_size =`. The default value is 8, we'll change it to 5 and send all of our texts in 2 requests.

```{r}
hf_embed_batch(
  embedding_sentences,
  endpoint_url,
  "HF_TEST_API_KEY",
  batch_size = 5,
)
```

The `hf_embed_batch` function will return our texts in a single data frame, with the text itself, two columns related to errors, and columns for each embedding dimension.

The `.error` and `.error_message` are introduced because we need to know whether errors occur at the individual post level. For example, if a batch of 10 requests has 2 errors, we want to see which individual post had errors, and keep the embeddings for the rest.

Batching is generally a good idea, and we'll see later that holding everything else equal, it does lead to a speed up over non-batched. However, if you try to set the batch_size too high, you are more likely to hit rate limits, or have your request rejected due to the payload being too large.

> **TIP**: Experiment to find the correct batch size for your data and the endpoint. Start small and work upwards in small incremeents.

## Embedding a Data Frame of Texts

As data professionals, we are more likely to work with data frames than any other data structure. We will usually have the texts we want to embed, their unique identifiers, and a selection of other columns to work with. EndpointR has a function - `hf_embed_df()` - built just for this use case.

For now, let's create some IDs and build a data frame from the sentences we defined earlier:

```{r}
embedding_ids <- 1:10

embedding_df <-
  tibble(
    id = embedding_ids,
    sentence = embedding_sentences
  )
```

The function requires an input to:

-   `text_var =` this should be the column in your data frame with the text data you need to embed.
-   `id_var =` this should be the column in your data frame which identifies each piece of text - this makes sure we can link our embeddings to the correct text.
-   `key_name` we've seen this before
-   `endpoint_url` we've seen this before

And then we have some optional arguments - these are optional because there is a default value for when we don't set them. We'll look at some of the other optional arguments in a later section. The key optional arguments for `hf_embed_df()` are:

-   `progress` lets EndpointR know whether to provide a progress bar, this is only really we're worth it if we're embedding a lot of data. For now we'll set it to `FALSE`
-   `max_retries` tells EndpointR how many times it should try to repeat the request before moving on to the next item.

> **NOTE**: We'll look at some additional arguments, such as batch_size and concurrent_requests later in [scaling performance](improving-performance)

```{r}
(
  result_df <- hf_embed_df(
  df = embedding_df,
  text_var = sentence,
  id_var = id,
  key_name = "HF_TEST_API_KEY",
  endpoint_url = endpoint_url,
  progress = FALSE,
  max_retries = 5)
)
```

We now have a data frame with our original ID column, the text column, the same '`.error` columns as in the batching case, and a number of columns with a 'V' followed by some digits. Each digit represents an 'embedding dimension', you should have as many 'VXX' columns as there are embedding dimensions for the model you are using.

You can check whether you had any errors a number of ways, I like to use `dplyr::count()` or `dplyr::filter()`, here's the count way:

```{r}
count(result_df, .error)
```

If you need to select only the columns that contain embeddings - for example if you want to feed these embeddings into a clustering, or dimensionality reduction model, you can select a range of columns using `dplyr::select()` - here I'll select V1:V384, which gives me all of the embeddings.

```{r}
result_df |> select(V1:V384)
```

If I had 768 embedding dimensions, I would input `V1:V768`, and similarly for 1024 dimensions, `V1:V1024`, you get the idea.

# Improving Performance

> This section will focus mainly on the `hf_embed_df()` function and the options we have for improving throughput of requests.

So far we've looked at how to send a single piece of text, a list or batch of texts, and how to iterate through a data frame full of texts, sending one request at a time. Whilst these are key workflows, once you're more comfortable with sending requests to APIs and receiving responses, you'll probably need to send a bunch of requests quickly, or in a more memory-efficient manner.

Before we get into how, let's go a bit further into why.

For each row of our data frame, we need to create a request which we will then perform. We then need to perform the request by sending our information over the internet to the endpoint. The endpoint accepts our request, performs any computation, and sends a successful response or an error. Each of these steps has an associated cost in time, and the cost for each step is not equal - some steps are more costly than others.

Creating each request is almost instant, but if we have a larger data frame - e.g. with 100,000 rows: creating 100,000 requests simultaneously will take $\approx$ 30s-90s. If we process them all at once, we'll have to wait the full amount of time before we send any requests and receive any responses. Clearly this is wasteful - if we had created the first request and sent it, we wouldn't be waiting 90s just to start sending requests.

Sending each request to the endpoint and waiting for it to process will usually take a lot longer than preparing the request. Therefore, if we can find ways to reduce the time we spend waiting here, we should be able to reduce the overall time it takes to prepare and send our requests significantly.

For the `hf_embed_df` function, our main options are to increase the number of requests we send at at once, and the number of texts we send within each request.

-   `concurrent_requests` lets EndpointR know how many requests to send at the same time. This should be a number between 1 and \~20-50 (in extreme cases you may be ok with 100, but most endpoints will not accept this)
-   `batch_size` lets EndpointR know whether to send requests with individual texts, or with a number of texts 'batched up' together.

## Concurrent Requests

Hugging Face' Inference Endpoints can handle multiple request arriving at the same time - and if configured they will 'autoscale' to provide higher throughput when there is a backlog of requests. As a rule, start with \~5 concurrent requests, and work up to \~20. If you start hitting rate limits go back down to \~10 and find the sweetspot. If endpoint is handling 20 requests then you could continue increasing gradually but it's not recommended unless you are sure the endpoint can handle it.

Here we send 5 concurrent requests - this will iterate through the data frame 5 rows at a time, and send new requests when responses are returned, until we run out of data to embed.

```{r}
hf_embed_df(
  df = embedding_df,
  text_var = sentence,
  endpoint_url = endpoint_url,
  key_name = "HF_TEST_API_KEY",
  id_var= id,
  concurrent_requests = 5,
  progress = TRUE
)
```

Let's benchmark performance (see appendix for code) - there is some overhead associated with generating parallel requests, so we'll need a bigger data frame to understand what type of speed up we can get.

```{r}
id_1000 <- 1:1000
sentences_1000 <- rep(embedding_df$sentence, 100)
embedding_df_1000 <- tibble(id = id_1000, sentence = sentences_1000)
```

Recording the results: we can see a \~40% reduction in processing time when going from 5-\> 10 concurrent requests and a \~15% reduction when going from 10 -\> 20 concurrent requests.

| concurrent_requests | processing_time_secs | success |
|--------------------:|---------------------:|:--------|
|                   5 |                 19.8 | TRUE    |
|                  10 |                 11.9 | TRUE    |
|                  20 |                 10.2 | TRUE    |

\*Exact times will fluctuate, so take these as approximates.

## Batch Requests

If our endpoint does not allow for multiple concurrent requests, or it's being overloaded by other users, we can still gain some efficiency by sending batches of requests.

```{r}
hf_embed_df(
  df = embedding_df_1000,
  text_var = sentence,
  endpoint_url = endpoint_url,
  key_name = "HF_TEST_API_KEY",
  id_var= id,
  concurrent_requests = 1,
  batch_size = 16,
  progress = TRUE
)
```

## Batching Concurrent Requests

For maximum speed up, we can also send batches of multiple concurrent requests. If our endpoint is able to handle them, 10 concurrent requests of `batch_size = 10` will be marginally faster than 10 concurrent requests of `batch_size = 5`.

### Benchmarking Batching & Concurrent Requests

In a separate session we did some benchmarking to understand the relationship between batch size, concurrent requests, and throughput, where throughput is the number of rows processed per second. We looked at combinations of:

-   **batch_size = c(1, 4, 8, 16, 32)**
-   **concurrent_requests = c(1, 5, 10, 15, 20)**

```{r, eval = TRUE}
batch_concurrent_benchmark <- read.csv("../dev_docs/batch_concurrent_benchmark.csv") |>
  tibble()

knitr::kable(batch_concurrent_benchmark |> mutate_if(is.numeric, ~ round(.x, 2)))
```

The results effectively speak for themselves. To embed $\approx$ 2,000 documents sending them 1 text and 1 request at a time, we get a throughput of $approx$ 2.16 texts per second, and it takes over 15 minutes! At the other end, 20 concurrent requests of batch size 8, and 20 concurrent requests of batch size 32 have a throughout of 195, which is close to 200x quicker. And we get our results back in 10 seconds!

> **NOTE**: If we needed to, we could re-run the benchmarking code multiple times but the general trend is very clear.

```{r, eval = TRUE}
batch_concurrent_benchmark |> 
  mutate(batch_size = factor(batch_size), concurrent_requests= factor(concurrent_requests)) |>
  ggplot(aes(x= batch_size, y = throughput, group = concurrent_requests)) +
  geom_point(aes(colour = concurrent_requests)) +
  geom_line(aes(colour = concurrent_requests)) +
  labs(y = "Throughput", x = "Batch Size", title = "Increasing `batch_size` and `concurrent_requests` increases throughput") +
  scale_colour_viridis_d() +
  theme_minimal() +
  theme(legend.position = "bottom") 
```

Which parameter is more important for speed, `batch_size =` or `concurrent_requests =`? By eye it looks like concurrent_requests has the bigger effect, but it's clear that both parameters have a positive effect, and within the boundaries of our parameters, there is an approximately linear relationship between them and throughput.

Putting together a quick linear model confirms that everything else equal, increasing `concurrent_requests` increases throughput more than `batch_size`

```{r, eval = TRUE}
model <- lm(throughput ~ batch_size + concurrent_requests, data = batch_concurrent_benchmark)

broom::tidy(model) |>  
  mutate(across(c(estimate, std.error, statistic), ~round(.x, 2)))
```

> **QUESTION**: What might happen if we keep increasing `batch_size` and `concurrent_requests`? Will the relationship hold or not?

# Advanced Usage

The low-level API offers more control over the embedding process. This is useful for custom workflows, debugging, or when you need fine-grained control over requests.

## Request Preparation

TODO: fix following all the re-factoring Prepare requests without executing them, useful for inspection or custom processing:

```{r}
is_endpoint_active <- validate_hf_endpoint(endpoint_url, "HF_TEST_API_KEY")

req <- hf_embed_request_single(
  text = "Text for custom request",
  endpoint_url = endpoint_url,
  max_retries = 5,       # retry failed requests
  timeout = 30,          
  validate = FALSE,     # skip validation if endpoint is up (we just checked so it will be)
  key_name = "HF_TEST_API_KEY"
)

req # check it before you run it
```

## Manual Request Execution

If you've checked the request and everything looks ok, go ahead and execute it. We'll set tidy = FALSE first so that we can see what the raw request comes back like, then we'll tidy it up:

```{r}
response_raw <- hf_perform_single(req, tidy = FALSE)

resp_status <- httr2::resp_status(response_raw)
resp_headers <- httr2::resp_headers(response_raw)
```

We should now end up with our tidied data frame where each column is an embedding dimension.

```{r}
(embeddings <- tidy_embedding_response(response_raw))
```

## Batch Processing with Data Frames

Work directly with the component functions for custom batch workflows.

### Batch Requests

Sending concurrent requests is one way to be more efficient - mainly through increasing throughput. Another way is to send more data inside each request, thus reducing total number of requests, and time spent sending requests to the endpoint.

To be concrete, this means sending multiple rows of data inside each request. If we have 1,000 rows of data, and we create batches of size 50, we'll have $\frac{1000}{50}= 20$ total requests to send, rather than the 1,000 requests we have without batching. If our endpoint is powered by GPU(s) or other hardware accelerators, it will take care of processing the data using parallel computation, increasing speed up.

> **Warning**: experiment with batch sizes to find the sweetspot - usually starting around 8-16, and capping out at \~64. You'll know when you've gone too high because you'll start seeing the retry bar, and/or your responses will contain errors.

TODO: come back to this when batching is fully fleshed out.

Going back to our list of sentences which we aved in the list - `embedding_sentences` above, we can create a single request with all 10 sentences batched in using `hf_build_request_batch()`

```{r}
batch_request <- hf_build_request_batch(
  embedding_sentences, endpoint_url, key_name = "HF_TEST_API_KEY")
```

Now if you inspect the body of the request, and look at data -\> inputs, you'll see the list of 10 sentences.

```{r}
batch_request$body$data
```

You can use `hf_perform_request()`to send the request. To the endpoint it will look like a single request, and it will return a single response. Each individual item in our requests `body$data$inputs` will be embedded separately.

```{r}
batch_result <- hf_perform_request(batch_request)
```

### Testing Batching

TODO Fix following re-factoring

grab a random id, re-embed it, chekc they're identical. this should go in tests somewhwre.

```{r}
batched_embeddings <- hf_embed_batch(sentences_1000, endpoint_url, "HF_TEST_API_KEY", concurrent_requests = 5)

test_index <- batched_embeddings |>  slice(157)
test_embed <- test_index |>  select(contains("V"))
test_repro_embed <- hf_embed_text(test_index$text, endpoint_url = endpoint_url, "HF_TEST_API_KEY")

all(test_embed[, 1:384] |> unlist()-test_repro_embed[, 1:384] |>  unlist() == 0) 
```

### Chunking Data Frames

When we have a lot of data - 100,000s or 1,000,000s of rows - we may want (or need!) to break the task down into manageable chunks.

### Writing to Files

Embeddings are returned to us as data frames of floating point numbers, where each column is an embedding dimension. The number of dimensions will usually fall in the range of \$\approx\$300-3000 (and some LLMs have internal represenations in the 10,000s of dimensions range!). That's a lot of data, and our computers only have so much RAM available. If we try to embed 100,000s - 1,000,000 rows at a time, there's a heightened chance of our R session crashing, meaning we lose the results. This wastes time, API credits, energy, and water.

It would be better to chunk our data up, create batches of requests, and write each batched response to a file. The file will persist if our R session crashes.

> At this stage it would normally be advisable to build up your own request directly with httr2 - you'll have full control over batching, concurrent requests, exponential back-off, time-outs and retries, error handling.

However, because it seems quite likely people will run into trouble, and there's a lot at stake if we're sending 100,000s-1,000,000s of requests, we've created a high-level function for chunking a data frame, batching requests, sending concurrent batches, and appending the batch to a file.

## Error Handling

The low-level API allows more detailed error inspection and recovery. We just wrap `req_perform` in `purrr::safely`. This changes the return object slightly, we now have a list with `$result` and `$error`

```{r}
result <- safely_perform_request(req)

if (!is.null(result$error)) {
  print(result$error)
} else {
  embeddings <- tidy_embedding_response(result$result)
}
```

## Testing Responses

If you are testing an API request with `hf_embed_text()`, you can pass additional arguments into `httr2::req_perform()`. For example, you can pass in a value for 'verbosity' to get more information on the request.

How much information to print?

From {httr}'s docs: \> This is a wrapper around req_verbose() that uses an integer to control verbosity: - 0: no output - 1: show headers - 2: show headers and bodies - 3: show headers, bodies, and curl status messages

You can also pass in a value for 'path', which will save the response to a file, we'll look more at how to manage this later.

# Appendix

## Benchmarking Concurrent Requests

```{r}
run_benchmark <- function(num_concurrent, data, endpoint, key) {
  start_time <- Sys.time()
  res_df <- try(hf_embed_df(
                  df = data,
                  text_var = sentence, 
                  id_var= id,         
                  endpoint_url = endpoint,
                  key_name = key,
                  include_errors = FALSE, 
                  concurrent_requests = num_concurrent,
                  progress = FALSE 
                ), silent = TRUE) 
  processing_time <- Sys.time() - start_time
  success <- !inherits(res_df, "try-error") && nrow(res_df) == nrow(data)
  return(data.frame(
    concurrent_requests = num_concurrent,
    processing_time_secs = as.numeric(processing_time, units = "secs"),
    success = success
  ))
}

num_requests_vec <- c(5, 10, 20)
results_list <- lapply(num_requests_vec, function(n) {
  run_benchmark(
    num_concurrent = n,
    data = embedding_df_1000,
    endpoint = endpoint_url,
    key = "HF_TEST_API_KEY" 
  )
})

(summary_df <- do.call(rbind, results_list))
```
