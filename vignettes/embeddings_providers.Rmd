---
title: "Embeddings Providers"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: flatly
    number_sections: true
    fig_caption: true
    df_print: paged
    highlight: tango
    code_folding: show
    anchor_sections: true
vignette: >
  %\VignetteIndexEntry{Embeddings Providers}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE,
  message = FALSE,
  warning = FALSE
)
```

This vignette shows how to generate text embeddings using EndpointR with both Hugging Face and OpenAI providers.

# What are Text Embeddings?

Text embeddings are numerical representations of text that capture semantic meaning. Think of them as coordinates in a high-dimensional space where similar texts are closer together. They're the foundation for:

-   Semantic search
-   Clustering similar documents\
-   Finding duplicates
-   Building recommendation systems
-   Powering RAG (Retrieval-Augmented Generation) applications

EndpointR makes it easy to generate embeddings from your text data using either Hugging Face or OpenAI APIs.

# Setup

```{r setup, eval=TRUE}
library(EndpointR)
library(dplyr)
library(tibble)

sample_texts <- tibble(
  id = 1:3,
  text = c(
    "Machine learning is transforming how we process information",
    "I love building applications with embeddings", 
    "Natural language processing enables computers to understand text"
  ),
  category = c("ML", "embeddings", "NLP")
)
```

# Provider Comparison

Before diving into code, let's understand the key differences:

| Feature | Hugging Face | OpenAI |
|----|----|----|
| **Models** | Many open-source models | text-embedding-3-small/large, ada-002 |
| **Dimensions** | Model-dependent (often 384, 768) | Configurable (512-3072) |
| **Pricing** | Free tier available, pay for dedicated | Pay per token |
| **Rate Limits** | Varies by tier | Generous for most use cases |
| **Max Input** | Model-dependent | 8,192 tokens per request |
| **Batching** | Supported | Supported (multiple texts per request) |

# Hugging Face Embeddings

## Setting Up

First, get your API key from [Hugging Face](https://huggingface.co/settings/tokens) and set it:

```{r}
set_api_key("HF_TEST_API_KEY")

embed_url <-  "https://router.huggingface.co/hf-inference/models/sentence-transformers/all-mpnet-base-v2/pipeline/feature-extraction"

```

## Single Text

The simplest case - embed one piece of text:

```{r}
embedding <- hf_embed_text(
  text = "I want to understand the meaning of this sentence",
  endpoint_url = embed_url,
  key_name = "HF_TEST_API_KEY"
)

dim(embedding) # result: a tibble with 768 columns (V1 to V768)

embedding
```

## Batch Processing

For multiple texts, use `hf_embed_batch()` which handles batching automatically. We feed in a vector of inputs and a `batch_size`, and the function takes care of batching our vector into as many batches as necessary.

> **TIP**: We can send many batches with `hf_embed_batch()`!s

```{r}
texts_to_embed <- c(
  "First document about machine learning",
  "Second document about deep learning",
  "Third document about neural networks",
  "Fourth document about data science"
)

batch_embeddings <- hf_embed_batch(
  texts = texts_to_embed,
  endpoint_url = embed_url,
  key_name = "HF_TEST_API_KEY",
  batch_size = 2,  # process 2 texts per API call
  concurrent_requests = 2  # run 2 requests in parallel
)

# Check results
glimpse(batch_embeddings[1,1:10 ]) # truncated for ease
```

The result includes: - `text`: your original text - `.error` and `.error_message`: error tracking - `V1` to `V768`: the embedding dimensions

## Data Frame Integration

Most commonly, you'll want to embed a column from a data frame:

```{r}
embedded_df <- hf_embed_df(
  df = sample_texts,
  text_var = text,      # column containing text
  id_var = id,          # unique identifier column
  endpoint_url = embed_url,
  key_name = "HF_TEST_API_KEY",
  batch_size = 3,
  concurrent_requests = 1
)

# Original data + embeddings
names(embedded_df)[1:10]  # shows: id, text, category, .error, .error_message, V1, V2...

embedded_df
```

# OpenAI Embeddings

## Setting Up

Get your API key from the [OpenAI](https://platform.openai.com/api-keys) website and set it:

```{r}
set_api_key("OPENAI_API_KEY")
```

## Single Text

OpenAI offers configurable embedding dimensions:

```{r}
# Default dimensions (1536 for text-embedding-3-small)
embedding <- oai_embed_text(
  text = "I want to understand the meaning of this sentence"
)

# Custom dimensions for smaller embeddings
small_embedding <- oai_embed_text(
  text = "I want to understand the meaning of this sentence",
  model = "text-embedding-3-small",
  dimensions = 512  # reduce size by ~67%
)

dim(small_embedding)  # 1 row, 512 columns
```

## Batch Processing

OpenAI allows multiple texts in a single API call, which `oai_embed_batch()` leverages.

```{r}
texts_to_embed <- c(
  "First document about machine learning",
  "Second document about deep learning",
  "Third document about neural networks",
  "Fourth document about data science"
)

batch_embeddings <- oai_embed_batch(
  texts = texts_to_embed,
  model = "text-embedding-3-small",
  dimensions = 1536,  # default for this model
  batch_size = 10,    # texts per API request
  concurrent_requests = 3  # parallel requests
)


batch_embeddings |>
  reframe(
    total = n(),
    succeeded = sum(!.error),
    failed = sum(.error)
  )
```

## Data Frame Integration

```{r}
embedded_df <- oai_embed_df(
  df = sample_texts,
  text_var = text,
  id_var = id,
  model = "text-embedding-3-large",  # higher quality embeddings
  dimensions = 3072,  # maximum dimensions for this model
  batch_size = 20,
  concurrent_requests = 5
)

# Extract just the embeddings for downstream use
embedded_df |> 
  select(starts_with("V")) 
```
```
