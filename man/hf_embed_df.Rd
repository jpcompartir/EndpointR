% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hf_embed.R
\name{hf_embed_df}
\alias{hf_embed_df}
\title{Generate embeddings for texts in a data frame}
\usage{
hf_embed_df(
  df,
  text_var,
  id_var,
  endpoint_url,
  key_name,
  max_length = 8192L,
  output_dir = "auto",
  chunk_size = 5000L,
  concurrent_requests = 1L,
  max_retries = 5L,
  timeout = 15L,
  progress = TRUE
)
}
\arguments{
\item{df}{A data frame containing texts to embed}

\item{text_var}{Name of the column containing text to embed}

\item{id_var}{Name of the column to use as ID}

\item{endpoint_url}{The URL of the Hugging Face Inference API endpoint}

\item{key_name}{Name of the environment variable containing the API key}

\item{max_length}{The maximum number of tokens in the text variable. Beyond this cut-off everything is truncated.}

\item{output_dir}{Path to directory for the .parquet chunks}

\item{chunk_size}{The size of each chunk that will be processed and then written to a file.}

\item{concurrent_requests}{Number of requests to send at once. Some APIs do not allow for multiple requests.}

\item{max_retries}{Maximum number of retry attempts for failed requests.}

\item{timeout}{Request timeout in seconds}

\item{progress}{Whether to display a progress bar}
}
\value{
A data frame with the original data plus embedding columns
}
\description{
High-level function to generate embeddings for texts in a data frame.
This function handles the entire process from request creation to
response processing, with options for batching & parallel execution.
Setting the number of retries

Avoid risk of data loss by setting a low-ish chunk_size (e.g. 5,000, 10,000). Each chunk is written to a \code{.parquet} file in the \verb{output_dir=} directory, which also contains a \code{metadata.json} file which tracks important information such as the endpoint URL used. Be sure to check any output directories into .gitignore!
}
\examples{
\dontrun{
  # Generate embeddings for a data frame
  df <- data.frame(
    id = 1:3,
    text = c("First example", "Second example", "Third example")
  )

  # Use batching without parallel processing
  embeddings_df <- hf_embed_df(
    df = df,
    text_var = text,
    endpoint_url = "https://my-endpoint.huggingface.cloud",
    id_var = id
  )

  # Use both chunking and parallel processing
  embeddings_df <- hf_embed_df(
    df = df,
    text_var = text,
    endpoint_url = "https://my-endpoint.huggingface.cloud",
    id_var = id,
    chunk_size = 10000,
    concurrent_requests = 50
  )
}
}
