% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hf_embed.R
\name{hf_embed_chunks}
\alias{hf_embed_chunks}
\title{Embed text chunks through Hugging Face Inference Embedding Endpoints}
\usage{
hf_embed_chunks(
  texts,
  ids,
  endpoint_url,
  output_dir = "auto",
  chunk_size = 5000L,
  concurrent_requests = 5L,
  max_retries = 5L,
  timeout = 10L,
  key_name = "HF_API_KEY"
)
}
\arguments{
\item{texts}{Character vector of texts to process}

\item{ids}{Vector of unique identifiers corresponding to each text (same length as texts)}

\item{endpoint_url}{Hugging Face Embedding Endpoint}

\item{output_dir}{Path to directory for the .parquet chunks}

\item{chunk_size}{Number of texts to process in each chunk before writing to disk (default: 5000)}

\item{concurrent_requests}{Number of concurrent requests (default: 5)}

\item{max_retries}{Maximum retry attempts per failed request (default: 5)}

\item{timeout}{Request timeout in seconds (default: 10)}

\item{key_name}{Name of environment variable containing the API key (default: "HF_API_KEY")}
}
\value{
A tibble with columns:
\itemize{
\item \code{id}: Original identifier from input
\item \code{.error}: Logical indicating if request failed
\item \code{.error_msg}: Error message if failed, NA otherwise
\item \code{.chunk}: Chunk number for tracking
\item Embedding columns (V1, V2, etc.)
}
}
\description{
This function is capable of processing large volumes of text through Hugging Face's Inference Embedding Endpoints. Results are written in chunks to a file, to avoid out of memory issues.
}
\details{
This function processes texts in chunks, creating individual requests for each text
within a chunk. The chunk size determines how many texts are processed before writing results
to disk. Within each chunk, requests are sent with the specified level of concurrency.
}
