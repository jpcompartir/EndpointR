% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/openai_completions.R
\name{oai_complete_df}
\alias{oai_complete_df}
\title{Process a data frame through OpenAI's Chat Completions API with chunked processing}
\usage{
oai_complete_df(
  df,
  text_var,
  id_var,
  model = "gpt-4.1-nano",
  output_dir = "auto",
  system_prompt = NULL,
  schema = NULL,
  chunk_size = 1000,
  concurrent_requests = 1L,
  max_retries = 5L,
  timeout = 30,
  temperature = 0,
  max_tokens = 500L,
  key_name = "OPENAI_API_KEY",
  endpoint_url = "https://api.openai.com/v1/chat/completions"
)
}
\arguments{
\item{df}{Data frame containing text to process}

\item{text_var}{Column name (unquoted) containing text inputs}

\item{id_var}{Column name (unquoted) for unique row identifiers}

\item{model}{OpenAI model to use (default: "gpt-4.1-nano")}

\item{output_dir}{Path to directory for the .parquet chunks. "auto" generates a timestamped directory name. If NULL, uses a temporary directory.}

\item{system_prompt}{Optional system prompt applied to all requests}

\item{schema}{Optional JSON schema for structured output (json_schema object or list)}

\item{chunk_size}{Number of texts to process in each batch (default: 5000)}

\item{concurrent_requests}{Integer; number of concurrent requests (default: 5)}

\item{max_retries}{Maximum retry attempts per failed request (default: 5)}

\item{timeout}{Request timeout in seconds (default: 30)}

\item{temperature}{Sampling temperature (0-2), lower = more deterministic (default: 0)}

\item{max_tokens}{Maximum tokens per response (default: 500)}

\item{key_name}{Name of environment variable containing the API key (default: OPENAI_API_KEY)}

\item{endpoint_url}{OpenAI API endpoint URL}
}
\value{
A tibble with the original id column and additional columns:
\itemize{
\item \code{content}: API response content (text or JSON string if schema used)
\item \code{.error}: Logical indicating if request failed
\item \code{.error_msg}: Error message if failed, NA otherwise
\item \code{.chunk}: Chunk number for tracking
}
}
\description{
This function takes a data frame with text inputs and processes each row through
OpenAI's Chat Completions API using efficient chunked processing. It handles
concurrent requests, automatic retries, and structured output validation while
writing results progressively to disk.
}
\details{
This function provides a data frame interface to the chunked processing
capabilities of \code{oai_complete_chunks()}. It extracts the specified text column,
processes texts in configurable chunks with concurrent API requests, and returns
results matched to the original data through the \code{id_var} parameter.

The chunking approach enables processing of large data frames without memory
constraints. Results are written progressively as parquet files (either to a specified
directory or auto-generated) and then read back as the return value.

When using structured outputs with a \code{schema}, responses are validated against
the JSON schema and stored as JSON strings. Post-processing may be needed to
unnest these into separate columns.

Failed requests are marked with \code{.error = TRUE} and include error messages,
allowing for easy filtering and retry logic on failures.

Avoid risk of data loss by setting a low-ish chunk_size (e.g. 5,000, 10,000). Each chunk is written to a \code{.parquet} file in the \verb{output_dir=} directory, which also contains a \code{metadata.json} file. Be sure to add output directories to .gitignore!
}
\examples{
\dontrun{
  df <- data.frame(
    id = 1:100,
    text = paste("Analyse this text:", 1:100)
  )

  results <- oai_complete_df(
    df = df,
    text_var = text,
    id_var = id,
    model = "gpt-4.1-nano"
  )
}
}
