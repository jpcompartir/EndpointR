% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hf_classify.R
\name{hf_classify_chunks}
\alias{hf_classify_chunks}
\title{Efficiently classify vectors of text in chunks}
\usage{
hf_classify_chunks(
  texts,
  ids,
  endpoint_url,
  max_length = 512L,
  tidy_func = tidy_classification_response,
  output_dir = "auto",
  chunk_size = 5000L,
  concurrent_requests = 5L,
  max_retries = 5L,
  timeout = 30L,
  key_name = "HF_API_KEY",
  id_col_name = "id",
  text_col_name = "text"
)
}
\arguments{
\item{texts}{Character vector of texts to classify}

\item{ids}{Vector of unique identifiers corresponding to each text (same length as texts)}

\item{endpoint_url}{Hugging Face Classification Endpoint}

\item{max_length}{The maximum number of tokens in the text variable. Beyond this cut-off everything is truncated.}

\item{tidy_func}{Function to process API responses, defaults to
\code{tidy_classification_response}}

\item{output_dir}{Path to directory for the .parquet chunks}

\item{chunk_size}{Number of texts to process in each chunk before writing to disk (default: 5000)}

\item{concurrent_requests}{Integer; number of concurrent requests (default: 5)}

\item{max_retries}{Integer; maximum retry attempts (default: 5)}

\item{timeout}{Numeric; request timeout in seconds (default: 30)}

\item{key_name}{Name of environment variable containing the API key}

\item{id_col_name}{Name for the ID column in output (default: "id"). When called from hf_classify_df(), this preserves the original column name.}

\item{text_col_name}{Name for the text column in output (default: "text"). When called from hf_classify_df(), this preserves the original column name.}
}
\value{
A data frame of classified documents with successes and failures
}
\description{
Classifies large batches of text using a Hugging Face classification endpoint.
Processes texts in chunks with concurrent requests, writes intermediate results
to disk as Parquet files, and returns a combined data frame of all classifications.
}
\details{
The function creates a metadata JSON file in \code{output_dir} containing processing
parameters and timestamps. Each chunk is saved as a separate Parquet file before
being combined into the final result. Use \code{output_dir = "auto"} to generate a
timestamped directory automatically.

For single text classification, use \code{hf_classify_text()} instead.
}
\examples{
\dontrun{
# basic usage with vectors
texts <- c("I love this", "I hate this", "This is ok")
ids <- c("review_1", "review_2", "review_3")

results <- hf_classify_chunks(
  texts = texts,
  ids = ids,
  endpoint_url = "https://your-endpoint.huggingface.cloud",
  key_name = "HF_API_KEY"
)
}
}
