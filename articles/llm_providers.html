<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Connecting to Major Model Providers • EndpointR</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Connecting to Major Model Providers">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">EndpointR</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Unreleased version">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><h6 class="dropdown-header" data-toc-skip>Getting Started</h6></li>
    <li><a class="dropdown-item" href="../articles/api_keys.html">API Key Management</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Hugging Face</h6></li>
    <li><a class="dropdown-item" href="../articles/hugging_face_inference.html">Hugging Face Inference Endpoints</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>LLM Providers (OpenAI, Anthropic)</h6></li>
    <li><a class="dropdown-item" href="../articles/llm_providers.html">Working with Major LLM Providers</a></li>
    <li><a class="dropdown-item" href="../articles/structured_outputs_json_schema.html">Structured Outputs with JSON Schema</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Embeddings</h6></li>
    <li><a class="dropdown-item" href="../articles/embeddings_providers.html">Embeddings Providers</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Advanced Topics</h6></li>
    <li><a class="dropdown-item" href="../articles/improving_performance.html">Improving Performance</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-news" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">News</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-news">
<li><h6 class="dropdown-header" data-toc-skip>Releases</h6></li>
    <li><a class="dropdown-item" href="../news/index.html">Version 0.1.0</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><a class="dropdown-item" href="../news/index.html">Changelog</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/jpcompartir/EndpointR" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Connecting to Major Model Providers</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/jpcompartir/EndpointR/tree/main/vignettes/llm_providers.Rmd" class="external-link"><code>vignettes/llm_providers.Rmd</code></a></small>
      <div class="d-none name"><code>llm_providers.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://jpcompartir.github.io/EndpointR/">EndpointR</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://httr2.r-lib.org" class="external-link">httr2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://purrr.tidyverse.org/" class="external-link">purrr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org" class="external-link">dplyr</a></span><span class="op">)</span></span></code></pre></div>
<p>This vignette details how we can use EndpointR to interact with
services hosted by major model providers, such as Anthropic, Google, and
OpenAI. The focus is on sending texts for a specific, one-off purpose -
similar to how traditional Machine Learning endpoints work. Though, we
will see that it’s not always straightforward to get LLMs to do what we
want them to do!</p>
<div class="section level2">
<h2 id="openai---quick-start">OpenAI - Quick Start<a class="anchor" aria-label="anchor" href="#openai---quick-start"></a>
</h2>
<div class="section level3">
<h3 id="openai---quick-start---chat-completions-api">OpenAI - Quick Start - Chat Completions API<a class="anchor" aria-label="anchor" href="#openai---quick-start---chat-completions-api"></a>
</h3>
</div>
<div class="section level3">
<h3 id="openai---quick-start---responses-api">OpenAI - Quick Start - Responses API<a class="anchor" aria-label="anchor" href="#openai---quick-start---responses-api"></a>
</h3>
</div>
</div>
<div class="section level2">
<h2 id="openai---the-basics">OpenAI - The Basics<a class="anchor" aria-label="anchor" href="#openai---the-basics"></a>
</h2>
<p>Before we get started, we need to make sure we have a couple of
things in place:</p>
<ul>
<li>First, get your API key and store it as “OPENAI_API_KEY” with
<code><a href="../reference/set_api_key.html">set_api_key()</a></code>
</li>
<li>Second, figure out if you need the <a href="https://platform.openai.com/docs/api-reference/responses" class="external-link">Responses
API</a> or the <a href="https://platform.openai.com/docs/api-reference/chat" class="external-link">Chat
Completions API</a>
</li>
<li>Third, choose your model - EndpointR is configured to select the
smaller, cheaper model but you are free to choose specific models</li>
</ul>
<blockquote>
<p><strong>Information:</strong> if you are unsure whether you should
prefer the Completions API or the Responses API, checkout <a href="https://platform.openai.com/docs/guides/responses-vs-chat-completions" class="external-link">this
webpage</a></p>
</blockquote>
<div class="section level3">
<h3 id="completions-api">Completions API<a class="anchor" aria-label="anchor" href="#completions-api"></a>
</h3>
<p>“The Chat Completions API endpoint will generate a model response
from a list of messages comprising a conversation.”</p>
<p>For most use-cases EndpointR covers, the conversation will be a
single interaction between the user and the model. Usually the goal is
to achieve some narrow task repeatedly.</p>
<blockquote>
<p><strong>TIP:</strong> If you are looking for persistent, open-ended
chats with LLMs, it’s best to head to Claude, ChatGPT, Gemini - or your
provider of choice.</p>
</blockquote>
<p>So what type of tasks might we use the Completions API for?</p>
<ul>
<li>Document classification, e.g. Sentiment Analysis</li>
<li>Translation</li>
<li>Structured data extraction</li>
</ul>
<div class="section level4">
<h4 id="sentiment-analysis">Sentiment Analysis<a class="anchor" aria-label="anchor" href="#sentiment-analysis"></a>
</h4>
<p>Whilst we would generally not recommend using OpenAI’s models for
sentiment analysis, <a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;using such powerful models is inelegant - much smaller,
less costly (time, energy, $$) models can do the job&lt;/p&gt;"><sup>1</sup></a> it is a task most people are familiar
with.</p>
<p>We set up a very basic <a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;Generally prompts should be more detailed than this,
we’ll see why&lt;/p&gt;"><sup>2</sup></a> system prompt to tell the LLM what we want
it to do with the text we send it:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">system_prompt</span> <span class="op">&lt;-</span> <span class="st">"Analyse the text's sentiment: "</span></span>
<span></span>
<span><span class="va">text</span> <span class="op">&lt;-</span> <span class="st">"Oh man, I'm getting to the end of my limit with this thing. WHY DOESN'T IT JUST WORK?!?"</span></span></code></pre></div>
<p>Then we can create our request and inspect it with {httr2}’s
<code><a href="https://httr2.r-lib.org/reference/req_dry_run.html" class="external-link">req_dry_run()</a></code> function - it’s good hygiene to check your
request is formatted as you expect it to be, particularly when using it
for the first time. For example, if you intend to have a system prompt,
is there a {“role”: “system”} key:value pair inside the messages list?
Is the value of “model”: a valid model ID? Etc.</p>
<blockquote>
<p><strong>TIP</strong>:You may need to familiarise yourself with the <a href="https://platform.openai.com/docs/api-reference/introduction" class="external-link">OpenAI
API Documentation</a> before proceeding.</p>
</blockquote>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentiment_request</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/oai_build_completions_request.html">oai_build_completions_request</a></span><span class="op">(</span></span>
<span>  <span class="va">text</span>,</span>
<span>  system_prompt <span class="op">=</span> <span class="va">system_prompt</span></span>
<span><span class="op">)</span> </span>
<span></span>
<span><span class="va">sentiment_request</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://httr2.r-lib.org/reference/req_dry_run.html" class="external-link">req_dry_run</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>Your generated HTTP request should look something like this:</p>
<pre class="http"><code>POST /v1/chat/completions HTTP/1.1
accept: */*
accept-encoding: deflate, gzip
authorization: 
content-length: 247
content-type: application/json
host: api.openai.com
user-agent: EndpointR

{
  "model": "gpt-4.1-nano",
  "messages": [
    {
      "role": "system",
      "content": "Analyse the text's sentiment: "
    },
    {
      "role": "user",
      "content": "Oh man, I'm getting to the end of my limit with this thing. WHY DOESN'T IT JUST WORK?!?"
    }
  ],
  "temperature": 0,
  "max_tokens": 500
}</code></pre>
<p>EndpointR handles the HTTP mechanics: authentication, request
headers, and endpoint configuration. It then constructs the JSON payload
with your specified model, system prompt, and user message, whilst
setting sensible defaults for temperature <a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;lower values = less random output, max value is 2&lt;/p&gt;"><sup>3</sup></a> and max_tokens <a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;Maximum tokens of the &lt;em&gt;output&lt;/em&gt; request&lt;/p&gt;"><sup>4</sup></a>.</p>
<p>For demonstrative purposes, we ran the same prompt with the same data
twice (see below)</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentiment_response</span> <span class="op">&lt;-</span> <span class="va">sentiment_request</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">perform_request_or_return_error</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentiment_response</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://httr2.r-lib.org/reference/resp_status.html" class="external-link">resp_check_status</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://httr2.r-lib.org/reference/resp_body_raw.html" class="external-link">resp_body_json</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://purrr.tidyverse.org/reference/pluck.html" class="external-link">pluck</a></span><span class="op">(</span><span class="st">"choices"</span>, <span class="fl">1</span>, <span class="st">"message"</span>, <span class="st">"content"</span><span class="op">)</span></span></code></pre></div>
<p>First run:</p>
<blockquote>
<p>“The sentiment of the text is frustrated and exasperated.”</p>
</blockquote>
<p>Second run:</p>
<blockquote>
<p>“The sentiment of the text is quite negative. The speaker appears
frustrated and exasperated, expressing dissatisfaction with the
situation.”</p>
</blockquote>
<p>Third run:</p>
<blockquote>
<p>““The sentiment of the text is quite negative. The speaker appears
frustrated and exasperated, expressing dissatisfaction and impatience
with the situation.”</p>
</blockquote>
<p>Whilst the responses are directionally/approximately accurate, the
inconsistent output can be very unpleasant to work with. When scaled to
hundreds or thousands of requests, this inconsistency can be extremely
draining, and productivity reducing.</p>
<p>If we wanted the response to conform to the usual sentiment
categories, we would need to write a custom parser to extract
‘negative’, ‘neutral’, or ‘positive’ from each output. Looking at the
first output, it’s clear the parser would need to be reasonably
sophisticated. Or we could send another request, asking our model to
please output only ‘positive’, ‘negative’, or ‘neutral’ <a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;or go to jail&lt;/p&gt;"><sup>5</sup></a>.</p>
<p>Clearly this is more work than we should be willing to do. We’ll look
at techniques for how to deal with this systematically, and achieve
predictable outputs in the <a href="#openai-structured-outputs">Structured Outputs</a> section.</p>
<p>What does the model do when we hand it a text which is difficult for
traditional, three-category <a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;positive, negative, neutral&lt;/p&gt;"><sup>6</sup></a>, document-level sentiment analysis?</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ambiguous_text</span> <span class="op">&lt;-</span> <span class="st">"The interface is brilliant but the performance is absolutely dreadful"</span></span>
<span></span>
<span><span class="va">ambiguous_sentiment</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/oai_build_completions_request.html">oai_build_completions_request</a></span><span class="op">(</span></span>
<span>  <span class="va">ambiguous_text</span>,</span>
<span>  system_prompt <span class="op">=</span> <span class="va">system_prompt</span></span>
<span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">perform_request_or_return_error</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://httr2.r-lib.org/reference/resp_body_raw.html" class="external-link">resp_body_json</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://purrr.tidyverse.org/reference/pluck.html" class="external-link">pluck</a></span><span class="op">(</span><span class="st">"choices"</span>, <span class="fl">1</span>, <span class="st">"message"</span>, <span class="st">"content"</span><span class="op">)</span></span></code></pre></div>
<p>[1] “The sentiment of the text is mixed, with positive feelings
expressed about the interface (”brilliant”) and negative feelings about
the performance (“absolutely dreadful”).”</p>
<p>The model is smart enough to recognise that the sentiment does not
fit neatly into ‘positive’, ‘negative’, ‘neutral’ but once again the
output is not formatted in a nice way for downstream use.</p>
</div>
<div class="section level4">
<h4 id="multiple-texts">Multiple Texts<a class="anchor" aria-label="anchor" href="#multiple-texts"></a>
</h4>
<p>Individual requests will be useful for some applications but
EndpointR was built to handle more, whilst taking care of implementation
details like concurrent requests, retries, and failing gracefully.</p>
<p>Let’s experiment with a better system prompt and a list of texts:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">updated_sentiment_prompt</span> <span class="op">&lt;-</span> <span class="st">"Classify the text into sentiment categories.</span></span>
<span><span class="st">The accepted categories are 'positive', 'negative', 'neutral', and 'mixed'.</span></span>
<span><span class="st">A 'mixed' text contains elements of positive and negative.</span></span>
<span><span class="st">"</span></span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classical_texts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span> <span class="st">"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife."</span>,</span>
<span> <span class="st">"All happy families are alike; each unhappy family is unhappy in its own way."</span>,</span>
<span> <span class="st">"It was the best of times, it was the worst of times."</span>,</span>
<span> <span class="st">"Call me Ishmael."</span>,</span>
<span> <span class="st">"The sun shone, having no alternative, on the nothing new."</span>,</span>
<span> <span class="st">"All animals are equal, but some animals are more equal than others."</span>,</span>
<span> <span class="st">"So we beat on, boats against the current, borne back ceaselessly into the past."</span>,</span>
<span> <span class="st">"The heart was made to be broken."</span>,</span>
<span> <span class="st">"Tomorrow, and tomorrow, and tomorrow, creeps in this petty pace from day to day."</span>,</span>
<span> <span class="st">"I have always depended on the kindness of strangers."</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classical_requests</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/oai_build_completions_request_list.html">oai_build_completions_request_list</a></span><span class="op">(</span></span>
<span>  <span class="va">classical_texts</span>,</span>
<span>  system_prompt <span class="op">=</span> <span class="va">updated_sentiment_prompt</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>When running this in testing, it took 8.5 seconds for the 10 requests
if we send one at a time.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">start_seq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.time.html" class="external-link">Sys.time</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">classical_responses</span>  <span class="op">&lt;-</span> <span class="va">classical_requests</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="../reference/perform_requests_with_strategy.html">perform_requests_with_strategy</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">end_seq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.time.html" class="external-link">Sys.time</a></span><span class="op">(</span><span class="op">)</span> <span class="op">-</span> <span class="va">start_seq</span></span></code></pre></div>
<p>And it took 1.8 seconds we send 10 requests in parallel - a ~4.7x
speed up, showing there is some overhead cost in sending requests in
parallel - i.e. we did not see a 10x speed increase for 10x
requests.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">start_par</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.time.html" class="external-link">Sys.time</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">classical_responses</span>  <span class="op">&lt;-</span> <span class="va">classical_requests</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="../reference/perform_requests_with_strategy.html">perform_requests_with_strategy</a></span><span class="op">(</span>concurrent_requests <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">end_par</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.time.html" class="external-link">Sys.time</a></span><span class="op">(</span><span class="op">)</span> <span class="op">-</span> <span class="va">start_par</span></span></code></pre></div>
<p>Now when we extract the content of the response, we can see <a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;at least when I ran this in testing&lt;/p&gt;"><sup>7</sup></a> that each
response is a classification belonging to our classes - making our
prompt slightly better helped us get to a better final result. However,
if we were to repeat this over many texts - hundreds/thousands, it’s
unlikely every single response would conform to our categories. Without
further instruction, the models have a tendency to slightly change the
output at unpredictable intervals.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">classical_responses</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span><span class="op">~</span> <span class="fu"><a href="https://httr2.r-lib.org/reference/resp_body_raw.html" class="external-link">resp_body_json</a></span><span class="op">(</span><span class="va">.x</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>        <span class="fu"><a href="https://purrr.tidyverse.org/reference/pluck.html" class="external-link">pluck</a></span><span class="op">(</span><span class="st">"choices"</span>, <span class="fl">1</span>, <span class="st">"message"</span>, <span class="st">"content"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>        <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html" class="external-link">as_tibble</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://purrr.tidyverse.org/reference/list_c.html" class="external-link">list_rbind</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>text <span class="op">=</span> <span class="va">classical_texts</span>, .before <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="data-frame-of-texts">Data Frame of Texts<a class="anchor" aria-label="anchor" href="#data-frame-of-texts"></a>
</h4>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df_classical_texts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html" class="external-link">tibble</a></span><span class="op">(</span></span>
<span>  id <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>,</span>
<span>  text <span class="op">=</span> <span class="va">classical_texts</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>We have a function <code><a href="../reference/oai_complete_df.html">oai_complete_df()</a></code> which takes a data
frame, an id variable, and a text variable as input, and returns a data
fraem with the <code>id</code>, <code>text</code>, <code>status</code>,
<code>content</code>, <code>.error_msg</code>, <code>.error</code>.</p>
<p>The function attempts to handle errors, and this aspect will improve
over time, as more of the common failure modes become apparent). You can
speed up processing by submitting multiple requests in parallel, using
the <code>concurrent_requests</code> parameter, but be careful not to
hit rate limits!</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/oai_complete_df.html">oai_complete_df</a></span><span class="op">(</span><span class="va">df_classical_texts</span>, </span>
<span>                text_var <span class="op">=</span> <span class="va">text</span>, </span>
<span>                id_var <span class="op">=</span> <span class="va">id</span>,</span>
<span>                concurrent_requests <span class="op">=</span> <span class="fl">5</span></span>
<span>                <span class="op">)</span></span></code></pre></div>
<p><code><a href="../reference/oai_complete_df.html">oai_complete_df()</a></code>reports on progress as it happens, and
provides a summary of failures and successes directly in your
console.</p>
</div>
</div>
<div class="section level3">
<h3 id="openai-structured-outputs">OpenAI Structured Outputs<a class="anchor" aria-label="anchor" href="#openai-structured-outputs"></a>
</h3>
<p>The textual responses we get from LLM providers are difficult to deal
with programmatically, because they make no guarantees on the form of
the response. For example, if we ask the LLM to classify documents,
sometimes it will give just the classification - ‘positive’, sometimes
it will add some pre-amble ‘the document is positive’, and sometimes it
will do something else entirely.</p>
<p>For detailed information on creating JSON schemas for structured
outputs, see
<code><a href="../articles/structured_outputs_json_schema.html">vignette("structured_outputs_json_schema")</a></code>.</p>
</div>
</div>
<div class="section level2">
<h2 id="anthropic">Anthropic<a class="anchor" aria-label="anchor" href="#anthropic"></a>
</h2>
<p>TBC</p>
</div>
<div class="section level2">
<h2 id="google">Google<a class="anchor" aria-label="anchor" href="#google"></a>
</h2>
<p>TBC</p>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://github.com/jpcompartir" class="external-link">Jack Penzer</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
